<?xml version="1.0" encoding="UTF-8"?>
    <rss 
        xmlns:dc="http://purl.org/dc/elements/1.1/" 
        xmlns:content="http://purl.org/rss/1.0/modules/content/" 
        xmlns:atom="http://www.w3.org/2005/Atom" 
        version="2.0" 
        xmlns:media="http://search.yahoo.com/mrss/"
    >
        <channel>
            <title><![CDATA[ Who is Nnamdi? ]]></title>
            <description>
                <![CDATA[ Thoughts on technology, venture capital, and the economics of both ]]>
            </description>
            <link>https://whoisnnamdi.com</link>
            <lastBuildDate>Tue, 23 Dec 2025, 18:23:31 GMT</lastBuildDate>
            <atom:link href="https://whoisnnamdi.com/rss" rel="self" type="application/rss+xml"/>
            <item>
                        <title>
                            <![CDATA[Seed Valuations Aren’t Valuations]]>
                        </title>
                        <description>
                            <![CDATA[It’s not obvious what drives them]]>
                        </description>
                        <link>https://whoisnnamdi.com/seed-valuations/</link>
                        <guid isPermaLink="false">seed-valuations</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Wed, 2 Oct 2024, 17:51:14 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512305055-header.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
Seed valuations don't behave like valuations:

-   They are too stable over time for such a speculative asset
-   They are impervious to shifting interest rates
-   They don’t follow public tech valuations

In short, seed valuations are a bit of an enigma — it’s not at all obvious what drives them. However investors arrive at these numbers, they aren’t doing so based on typical Finance 101 factors like discount rates or comparable company analysis.

Seed companies don’t seem to be priced as businesses with intrinsic value derived from future cash flows. Rather than _venture capital_, they seem to be a proxy for the **human capital** of the founders and early team.

Seed valuations aren’t valuations.

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div>

## Non-standard deviations

**First: seed valuations aren’t volatile enough.**

This really stood out when constructing my [Venture Activity Index](https://whoisnnamdi.com/venture-activity-index/) – the volatility of the late stage is much higher than the early stage, which is true even if you exclude the pandemic era:

![vol](https://nnamdi.net/content/images/2024/10/vol.png)

This is strange. If seed stage investments are as speculative as they’re purported to be, we'd expect wild valuation fluctuations over time. Late-stage valuations should be more stable, since they have an existing business model, revenues, and potentially even cash flows (hard to believe, I know).

Imagine if large, blue chip stocks were more volatile than small caps. That wouldn't make much sense. The stability of seed valuations relative to more mature startups makes no sense either.

As I’ve previously covered, [capital flows affect valuations](https://whoisnnamdi.com/shadow-price/), and this force is most extreme at the later stages where the so-called "crossover funds" (funds who've crossed over from public market to private market investing) are most active. Thus, capital flows drive excess volatility at the later stages. However, I don't think that can explain everything going on here.

And yes, there are many more seed deals getting done each quarter than later-stage rounds, so due to some [fundamental laws of statistics](https://en.wikipedia.org/wiki/Central_limit_theorem) we’d expect less variability with a larger sample. Even so, seed valuations seem too stable on a relative basis.

## Discount rates don’t matter if there’s nothing to discount

**Second: interest rates don’t affect seed valuations.**

In the standard corporate finance logic, early stage companies should be the most sensitive to interest rates because they have the highest _duration_. Think of duration as a measure of distance, measured in years, to the average dollar of cash flow for a business. For early stage companies, all cash flow the business will ever produce is far into the future, so they definitionally have high duration. In a discounted cash flow model, seed valuations would be extremely sensitive to movements in the discount rate, similar to a long-dated bond:

> Duration measures the sensitivity of the value of a bond to a change in interest rates, which is tied to the lifetime of the bond. Bonds with longer tenure or back-loaded cash flows are more sensitive to changes in interest rates
> 
> Due to the [multiplicative nature](https://whoisnnamdi.com/you-dont-understand-compound-growth/) of discounting, the present value of far-away payments is more sensitive to a change in interest rates than the value of soon-to-come payments – [High Retention = High Volatility](https://whoisnnamdi.com/high-retention-high-volatility/)

**That's not what we see.** [Some time ago](https://whoisnnamdi.com/discount-rates/) I investigated the “average” impact of interest rate surprises on venture valuations across all stages. I've since revisited and refined that analysis, adding more granularity to the estimates. I've also stripped out the COVID era, where the impact of fund flows contaminates the estimates.

What I found was fascinating. Here's the impact of a surprise 1% increase in the one-year Treasury yield across seed, Series A / B, and Series C / D, along with a measure of uncertainty around these estimates, going twelve quarters out:

![sv-1](https://nnamdi.net/content/images/2024/10/sv-1.png)

-   **Interest rates have no effect on seed valuations.** Not only is the impact zero, it’s _precisely zero_. There isn't a ton of uncertainty around these estimates.
-   For Series A / B, we start to see some effect, maxing out at a 16% decline seven quarters after impact before recovering.
-   For proper growth stage rounds like Series C / D, the impact is even quicker and more severe, peaking at 19% six quarters out.

This is the most striking evidence — the clearest indication that investors value seed stage startups in a totally distinct way to the rest of the venture market. It’s hard to rationalize this evidence within the usual frameworks.

## They not like us

**Third: public tech valuations don’t influence seed valuations.**

When the big tech (“FANG”, the “Magnificent 7, etc) valuations move around, private valuations typically follow, with a lag. This makes intuitive sense given venture investors use comparable public company valuations to decide how much they’re willing to pay for private companies. In a rational market we’d expect some correlation between public and private valuations, even if imperfect. This is “comps analysis” in a nutshell.

And that’s what we see — _except_ for seed companies:

![nv-1](https://nnamdi.net/content/images/2024/10/nv-1.png)

When the Nasdaq rises 1%:

-   Series A through D valuations rise, matching the bump in the Nasdaq within about a year.
-   Seed valuations don’t move at all, marching to the beat of their own, relatively quiet drummer

While there’s a very clear pass-through effect of public prices on private valuations for most stages, seed startups are the exception, seeing no pass-through at all. It turns out, **investors don’t care about public comps when pricing seed stage companies.**

It’s clear that seed valuations are not really valuations in the traditional sense. They don't behave like valuations in either their volatility over time or their sensitivity to interest rates. Something else must be going on here.

## These prices are sticky too

It struck me that seed valuations were incredibly _sticky_ (an early working title for this essay was "Why are Seed Valuations so Sticky?”). Seed valuations neither rise nor fall dramatically far from trend, whereas other stages see much strong gyrations:

![sticky](https://nnamdi.net/content/images/2024/10/sticky.png)

This "stickiness" is unique to seed valuations and doesn't mirror the behavior of free-floating financial assets, which are typically much more volatile and difficult to forecast.

I stared at the seed valuation data for a long time as I contemplated this essay. As my eyes glazed over, I tried to come up with analogues, other phenomena that mimic the behavior of seed valuations.

Then it hit me – wages. [Wages are often said to be “sticky”](https://www.richmondfed.org/~/media/richmondfedorg/publications/research/econ_focus/2013/q1/pdf/jargon_alert.pdf), and seed stage valuations look eerily similar to wages over time, which also tend to be quite stable around a long-term, upward trend.

The simplest thing to do is plot compensation against seed stage valuations. I found a wage series called the “Employment Cost Index” (ECI) that measures compensation growth over time, and I pulled out a [version](https://fred.stlouisfed.org/series/CIU2015400000000I) of this that’s specific to “professional, scientific, and technical services,” which I take to be a good proxy for tech workers:

![cbs-1](https://nnamdi.net/content/images/2024/10/cbs-1.png)

-   The first thing that stands out is the obvious difference in growth rates. Seed stage valuations have risen much faster than wages for the typical tech worker. **For every 1% increase in tech worker wages, seed stage valuations grow 4-5%.**
-   We can predict seed stage valuations from the wage data. I regress seed valuations on the ECI using pre-2020 data. The fit is tight pre-2020 (which is frankly easy since they’re both roughly straight lines).
-   Then I evaluate its forecasts on post-2020 data, “out of sample”. The model returns to a reasonably close fit once valuations settle down after the 2021-2022 bonanza.
-   Notably, **seed valuations bottomed out at exactly the level you would have predicted using the employment cost index.**

This feels like more than coincidence. Regressions do have a high risk of being spurious, which is to say, total nonsense. I was skeptical the first time I ran these numbers. But after multiple sanity checks, this seems to be the real deal. There is a tight connection between seed valuations and wages for tech workers; the two follow the same trend, one an accelerated version of the other. Thus we have a [scaling law](https://arxiv.org/abs/2001.08361) between tech wages and seed valuations:

$$\text{ Seed Valuations} \propto \text{ Tech Wages}^{4.5}$$

The inverse of [stock-based compensation](https://corporatefinanceinstitute.com/resources/accounting/share-stock-based-compensation/), seed companies appear to be **compensation-based stocks**, at least in how they’re valued.

## Google is my [BATNA](https://www.pon.harvard.edu/daily/batna/translate-your-batna-to-the-current-deal/)

This analysis doesn't _prove_ anything, but it does suggest an interesting link between tech wages and seed valuations.

Why would seed valuations be linked to the cost of tech labor? And why would those valuations grow so much faster than wages? I’m not even 100% convinced that it’s a direct, causal connection — perhaps there’s some third variable that drives both. Totally plausible. I plan to explore this in a future piece.

Regardless, it’s quite clear to me after crunching the numbers that seed valuations don’t behave anything like valuations, at least not valuations of _companies_ or speculative assets. Their stability suggests investors have a precise sense of their worth, despite their riskiness. The value investors place on these companies does not fluctuate wildly over time.

This is odd only if your mental model values these fledging enterprises as… enterprises. What if seed stage valuations instead represent the value of the labor and human capital of founders and early employees? The behavior of seed valuations would make a lot more sense if we saw them as proxies for the opportunity cost of talented tech workers.

I’ll explore this hypothesis in a future essay.

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div> ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[AI Benchmarking Is Broken]]>
                        </title>
                        <description>
                            <![CDATA[We should take AI models seriously, which means taking their evaluation seriously]]>
                        </description>
                        <link>https://whoisnnamdi.com/ai-benchmarking-broken/</link>
                        <guid isPermaLink="false">ai-benchmarking-broken</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Developers ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Tue, 28 May 2024, 13:29:17 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512303112-header-ai-benchmarking-broken.jpg" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
_To celebrate [Patronus AI’s $17M Series A](https://www.patronus.ai/blog/announcing-our-17-million-series-a), I wanted to share some thoughts on the current state of AI benchmarking._

Imagine a standardized test that works like this:

-   The questions and answers are freely available on the public internet
-   Cheating is not regulated, it’s a 100% honor system
-   The exam never changes, it’s the same questions every time
-   Scores on the exam seem to be getting better every year

In case you’re wondering, the correct reaction is “this is not a very good exam.” Cheating is rampant. The answers can all be memorized. The test isn’t really much of a “test.” The scores are meaningless.

This is the current state of AI benchmarking. Ironically, we hold AI benchmarks to a very _low_ standard. We allow practices that would never fly in other serious domains. It’s happening in plain sight, with a nod and a wink every time an AI lab releases a new model that tops the leaderboards.

We must do better.

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div>

## Something’s in the water

![training-data-contamination](https://nnamdi.net/content/images/2024/05/training-data-contamination.jpeg)

The first dirty secret of AI benchmarks is quite literally, dirty: **training data contamination**.

A common refrain among machine learning researchers and engineers — “don’t train on the test set.” In other words, don’t train a model on the same dataset that you will evaluate it on. If you do, whether on purpose or not, that particular test is useless for evaluating model performance.

Most open benchmarks are available on the public internet. In other words, the entire set of questions and answers exists on the open web, likely in multiple places. It is extremely easy for this data to inadvertently make its way into LLM training pipelines. The big AI labs are constantly hoovering up data from the internet via web crawlers, scraping their way across the web. While diligent researchers are careful about what ends up in the training data, applying all sorts of sophisticated filters to the raw data before feeding it to their models, invariably these filters are imperfect, and data from the benchmarks leaks in. It can happen during pre-training, it can happen during fine-tuning, either way, the bad data gets in. This renders invalid the downstream model performance on the benchmark.

The test data from a particular benchmark can appear in many places. It could be in plaintext on a webpage somewhere. It could be a file in a GitHub repo, alongside code which a model is being trained with. It could be in the _comments_ of a file or script inside of a repo. It could be on Reddit or X. It could really be anywhere.

> An increasingly important issue with evals is test set contamination. After the creation of a good eval, examples of the eval tend to get propagated into various places in the internet, like arxiv papers, ChatGPT examples, or reddit — [Jason Wei, OpenAI](https://www.jasonwei.net/blog/evals)

It’s been shown that a single training example can meaningfully impact the behavior of a model, so it only takes one mishap to contaminate a model. Model builders routinely train on trillions of tokens, so there’s no shortage of ways for accidents to occur.

You might wonder, if the entire benchmark makes its way into the training data, then why don’t the models score perfectly on the benchmarks? Remember, these models are complicated mathematical functions of the data — they are optimizing for all sorts of criteria that we don’t fully understand ([though we’re making progress](https://www.anthropic.com/research/mapping-mind-language-model)) and have been trained on vast sums of data, so any particular benchmark represents a very small share of the overall training corpus. A model trained on a particular dataset will fail to reproduce that data at times. This complex and non-deterministic mapping of inputs to outputs is arguably the distinguishing feature of LLMs relative to traditional machine learning models. So we shouldn’t necessarily expect to see perfect benchmark performance even in the most blatantly contaminated scenarios.

## It’s not what you think

The next time you hear about how a new model crushes some well-established benchmark, take a moment to actually go to the primary source and review the tasks in the benchmark. Pick any of them, it doesn’t really matter which. I would bet that, even as a non-specialist, you will very quickly begin to question whether the benchmark is really testing what it claims to test.

Sometimes this is subtle; sometimes it’s not. Many AI coding benchmarks have fairly trivial solutions that don’t require meaningful code understanding or “intelligence.” Many “reasoning” benchmarks don’t require any reasoning.

Again, don’t take my word for it — I encourage you to, right now, read through a few examples from the many published benchmarks, and see what you think about the concepts they are purportedly testing. **You will be less than impressed**, and you will leave the exercise with a much reduced opinion of and trust in the performance of the latest and greatest models.

This also relates to my point earlier about contamination, which is especially pernicious in the software development domain. Many “brain twister” coding tasks are well-represented in the training data for these models, in part because software developers spend so much time preparing for such questions as part of job interviews. Coding competitions abound, which can serve as a signal to employers of an engineer’s talent and skills. Their competition code often ends up on the public internet, where it gets picked up by training data crawlers.

At test time, the model has already seen the question, perhaps in slightly modified form, and has memorized the answer. It’s not right to say a model regurgitating some code from some repo is “reasoning” about software development. It’s much more akin to an engineer getting stuck and looking up some code on Stack Overflow, pasting it into their editor, and moving on.

There’s nothing wrong with a “Stack Overflow bot”! I would and do pay for such things. I just think we need to be very careful and specific about what we call it.

## Teaching to the test

Critics of standardized tests often complain they encourage teachers to “teach to the test”, focusing exclusively on material that will improve their students’ performance on the test but that perhaps misses other information valuable to their overall edification as human beings and citizens. I think this critique has bite to it in the context of AI models.

Here’s why. [Kaggle](https://www.kaggle.com/) is the most famous competition platform for machine learning practitioners. They hold frequent competitions where the goal is to build a machine learning model that can generate accurate predictions in a particular context based on provided (or otherwise procured) inputs. A key feature of Kaggle competitions is the [public leaderboard](https://www.kaggle.com/docs/competitions#leaderboard) — the real-time ranking of submitted models on a “validation” dataset that only Kaggle has access to. Thus, they avoid the problem of data [leaking](https://www.kaggle.com/docs/competitions#leakage) into the training set.

However, as is well-known among Kagglers, this “one weird trick” doesn’t solve all problems. After some time, the competition closes, at which point every model is re-evaluated on a previously closed “test set”, the scores for which make up the “private leaderboard”. It is extremely common for the “best” model from the public leaderboard to fall a few places in the rankings on the private leaderboard and for another model to rise to the top. In fact, this happens so often that being in 1st place going into the final evaluation can often be a source of stress rather than relief, since you know there’s some risk you’ve “[overfit to the public leaderboard.](https://www.kaggle.com/docs/competitions#leaderboard)”

> Many users watch the public leaderboard closely, as breakthroughs in the competition are announced by score gains in the leaderboard. These jumps in turn motivate other teams working on the competition in search of those advancements. But it’s important to keep the public leaderboard in perspective. It’s very easy to overfit a model, creating something that performs very well on the public leaderboard, but very badly on the private one. This is called overfitting.

Even though your model never saw the validation data directly, you did see its _score_ on the data. That score is a noisy proxy for the data itself, so anything you do to improve the score implicitly leverages features of validation data, even if you never saw data directly. Since that score influenced the techniques and tricks you employed during model training, there’s a sense in which the validation data did “leak” into training.

You’ve effectively “taught to the _practice_ test”. And as anyone who’s experimented with the materials of various test prep providers, _practice_ tests can differ meaningfully from the _actual_ test.

The same risk applies to most AI benchmarks, which are the “public leaderboard” of AI evaluation. At any point during the training process, researchers can check their models against the benchmark to see how it’s doing and ensure that performance is moving in the right direction. Benchmark performance isn’t a “surprise” at the end of a training run. It’s the same with Kaggle contests — contestants can always check their performance via the public leaderboard. This creates perverse incentives to only do things that drive direct improvement on the benchmarks, leading to overfitting, the curse of all machine learning practitioners.

Again, if you want some intuition, think about the education context. Imagine if teachers knew in real-time how their students would perform on the test — a score every day of class rather than one a year. Whatever concerns you have about teaching to the test, you should be only more concerned if teachers had a real-time view of their students’ test performance. The dopamine rush of seeing those scores bob up and down every day would drive an even more myopic focus on test performance. Anything a teacher could do to drive up those scores, even if it meant rote memorization of facts, would be extremely tempting. This is the world we live in when it comes to AI benchmarking.

There’s a balance we must strike. To the degree the test actually tests what you think and hope it does, it makes sense to “teach to the test”, assuming it’s actual learning and not just coerced memorization. However, if the test isn’t perfectly correlated with the true objective, these efforts can drive perverse behaviors in both the teachers (trainers) and students (models).

## Closed source benchmarks

![closed-source-benchmarks](https://nnamdi.net/content/images/2024/05/closed-source-benchmarks.jpeg)

[I’m a big fan of open source](https://whoisnnamdi.com/portfolio/) when it comes to software. I’m a big fan of closed source when it comes to standardized testing.

Open benchmarks have an important role to play, and **I don’t think we think we should dispose of them.** A common basis of comparison that is open access, that we can all inspect and contribute to, is certainly valuable. But we also need to round out the ecosystem with **closed, private benchmarks**, a critical complement.

Just like in the world of standardized testing, private benchmarks would need to establish themselves as standard-bearers for quality, which requires building up trust with the broad community. In a competitive market where end users of these models really do care about quality and performance, benchmark providers will sprout up and compete for their business, jostling for position as the ultimate arbiter of model performance. In the limit, _the benchmarks will be benchmarked_, in the same way that colleges are ranked for quality in explicit and implicit ways. Trusted “brands” will emerge.

Ideally, these private evaluators would not train and market state-of-the-art foundation models themselves, as there would always be an incentive to either leak the benchmark data to the training pipeline or craft a benchmark that the researchers know their model will perform well on.

In a perfect world, you would have **blind benchmarking**, which is to say benchmarking that is obfuscated from the folks who trained the model, so that there would be no way for the training pipeline to be contaminated. Model trainers would not know in advance what the benchmark is or what the questions are. They would train models to the best of their abilities, and only know how it performs on the benchmark after training is done. This is analogous to pre-registration of scientific studies, where researchers pre-commit to a certain methodology before conducting a trial or analysis.

Even better, you would want **one-time benchmarks**: benchmarks crafted \*once\* and then only used \*once\* to test a model or a set of models. At that point, the benchmark would be “used up” and no longer valid. That’s how Kaggle works, and it’s exactly how most standardized tests work — each exam is only given once, on a certain day, and then it’s thrown out or farmed out to test prep providers and tutors who use it as part of their materials. This is fine — as long as the score students report to colleges is from a clean, never-before-seen exam.

Again, a college would never accept a student’s scores from an SAT practice exam as equally valid as the real thing. All I’m asking for here is a similar level of rigor.

## Super serious

My point here is not — “this AI stuff is a big conspiracy, these models are just stochastic parrots with no real intellect at all”. I think we’re far enough along and the results of these models are way too compelling to short change them like that. We should take these models very seriously; they really are getting better and better all the time.

My point is exactly that — **we should take these models seriously, which means taking their evaluation seriously.** We aren’t right now, at least not in the public discourse around model performance.

No exam is perfect — as we know, standardized tests have their flaws. But we’re much better off having them than not. Right now, the broader AI community has no form of standardized testing for AI models. This is untenable.

It’s why I was so excited to lead [Patronus AI’s](https://www.patronus.ai/) [seed round](https://lsvp.com/stories/building-with-patronus-ai-automated-ai-evaluation/) last year, and it’s why I’m even more pumped now that they’ve raised a [$17M Series A](https://www.patronus.ai/blog/announcing-our-17-million-series-a).

![patronus-series-a](https://nnamdi.net/content/images/2024/05/patronus-series-a.jpeg)

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div> ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[The Venture Activity Index – Q4 2023]]>
                        </title>
                        <description>
                            <![CDATA[Capital deployment remains depressed, sitting about 50% below trend]]>
                        </description>
                        <link>https://whoisnnamdi.com/vai-q4-2023/</link>
                        <guid isPermaLink="false">vai-q4-2023</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Thu, 7 Mar 2024, 18:18:09 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512300770-header.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
I’ve updated the Venture Activity Index for [Q4 2023](https://whoisnnamdi.com/vai/) data.

As a reminder, the VAI is an index measuring current venture investment across stages relative to the long-run trend. It’s a useful and simple barometer of activity and sentiment in the venture market. The methodology is [here](https://whoisnnamdi.com/venture-activity-index/).

Some small modifications since the last version of the index that I published in Q2 2023:

-   Updated data for Q4 2023
-   Removed seasonality from the data
-   Removed super later stage rounds post-Series D. This was causing excess volatility that wasn’t representative of the overall venture market

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div>

With that out of the way, here’s where we’re at:  
![vai_plot](https://nnamdi.net/content/images/2024/03/vai_plot.png)

-   Capital deployment remains depressed, sitting about 50% below trend
-   We’ve hit the trough but remained there rather than recovered to trend. It's a bit of a [dead cat bounce](https://en.wikipedia.org/wiki/Dead_cat_bounce)

Another interesting view of the data is to show the various components by stage:  
![vai_disagg](https://nnamdi.net/content/images/2024/03/vai_disagg.png)

-   The later stages spiked the most in 2021, the earliest stages the least. This then reversed, with late stage capital falling somewhat more than early stage capital
-   However, all stages are similarly depressed at this point relative to their respective trends

As I caveated in the first version of the index, if we continue to stay below trend I may need to update the definition of trend to recognize that we’re on a new trajectory. For now, I'll continue to use a long-term, constant growth rate to define the trend for each stage:  
![detrend](https://nnamdi.net/content/images/2024/03/detrend.png)

That's it! I think the data mostly speaks for itself here, so I'll keep my commentary light for this one. Will update this again next quarter.

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div> ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[The Series A Bust]]>
                        </title>
                        <description>
                            <![CDATA[Investors have lost faith in Series A as a sign of product-market fit. In other words, they expect more but believe less.]]>
                        </description>
                        <link>https://whoisnnamdi.com/series-a-bust/</link>
                        <guid isPermaLink="false">series-a-bust</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Wed, 3 Jan 2024, 15:58:05 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512299180-header.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
It's cold out there in Series A land:

![deals_a](https://nnamdi.net/content/images/2024/01/deals_a.png)

A common explanation for the Series A winter is raised expectations – investors are demanding to see better metrics and traction. Most companies don't meet this new, higher bar, hence fewer deals get done.

While certainly true, _this can't be the whole story._ Only if we look at Series As in a vacuum does this explanation seem comprehensive. Incorporating what's happening in the rest of the market, however, there's a much richer narrative to explore.

The bigger picture? In addition to raising their expectations for Series A, investors have lost faith in Series A as a sign of product-market fit.

**Series A investors expect more but believe less.**

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div>

## Relatively speaking

Valuations for both Seed and Series A stage companies rose dramatically over the last decade or so of venture:

![valuations](https://nnamdi.net/content/images/2024/01/valuations.png)

The appreciation in Series A valuations seems extreme relative to that of Seed deals. Of course, the scales here are different, so we'd do better to baseline these to the same starting point and track the growth over time:

![valuations_growth](https://nnamdi.net/content/images/2024/01/valuations_growth.png)

While the two grew roughly in line for the first few years, Series A valuations began to accelerate around 2017. That acceleration sustained itself until the recent fall off in venture valuations.

Notably, both Seed and Series A valuations are recovering. That's a small point in favor of the "raised expectations" story – investors are funding higher-quality businesses, deserving of stronger valuations.

Here's where the expectations story starts to lose its luster. Below I plot the valuation gap or "premium" for Series A relative to Seed, along with a flexible trend line to remove noise:

![premiums_a](https://nnamdi.net/content/images/2024/01/premiums_a.png)

-   From 2010 to 2014, the two grew roughly in line, with Series As being priced 200% higher or 3x Seed rounds
-   Then, Series A valuations accelerate relative to Seeds to a 325% premium or 4.25x multiple
-   In 2019, the Series A premium spikes again, peaking in late 2021 at 500% or 6x
-   Things collapse after that, ending in Q3 2023 at 2010 levels

If investor expectations told the full story, Series A valuations relative to Seed deals would be _higher_ today. Since Seed investments remain speculative and standards have risen for Series A investment, we'd expect the typical Series A to be even _more_ valuable relative to Seeds, reflecting higher quality. Instead, the gap has compressed.

It's been quite the rollercoaster ride. Notably, this rollercoaster is unique to Series A / Seed. We don't see remotely the same behavior in the premium between later rounds:

![premiums](https://nnamdi.net/content/images/2024/01/premiums.png)

The premium for Series B vs. A and the premium for Series C vs. B are both much more stable over time, with little trend once you remove the noise. The Series C / B premium saw a slight bump during the exuberant 2021 days, but that was short lived:

-   The Series B / A premium has been roughly 200% for the entire period
-   The Series C / B premium hovers around 125% with few exceptions

For whatever reason, the way investors value Series A companies relative to Seed stage companies has fluctuated substantially over time (slowly rising and then suddenly falling), while their views on relative value in later stages have held steady.

Why?

## Series A University

What is different about graduating from Seed to Series A than going from Series A to B, Series B to C, etc?

There are many potential explanations, but the one that jumps out to me is some notion of **product-market fit (PMF) and business de-risking.** Series A companies tend to have much better PMF than Seed companies, in a way that's distinct from Series B relative to Series A, and so on.

Think about it like higher education – a college degree is a signal of one's skills to the labor market. [Employers value them accordingly.](https://www.clevelandfed.org/publications/economic-commentary/2012/ec-201210-the-college-wage-premium) Further, a college degree means more relative to not having one than having a graduate degree means relative to having only an undergraduate education. Again, employers value them accordingly:  
![Pasted-image-20231227144822](https://nnamdi.net/content/images/2024/01/Pasted-image-20231227144822.png)

While the probability of a company achieving PMF is certainly not 100% at the time of Series A (many companies raise Series A before that), it's definitely much higher at the A round than at the Seed round, since it's effectively 0% at that point.

Startup risk reduces materially once a company graduates to Series A. In a way, that's exactly why a startup successfully raises its Series A – the founders have de-risked the business, so much so that an investor is willing to invest substantial capital, take a board seat, etc.

If we posit that the "price" or "value" of PMF is well-proxied by the Series A / Seed premium, we can think of this premium moving over time due to fluctuations in the supply (among startups) and demand (among investors) for PMF:

-   A rising price typically indicates either contracting supply – the relative supply of startups with PMF vs those without – or growing demand – the relative demand for post-PMF startups vs pre-PMF startups
-   Importantly, these are _relative_ concepts – the number of pre and post-PMF startups could be both growing over time, but if the number of pre-PMF startups is growing faster, that would imply a _relative decline_ in PMF supply

$$\text{Relative Supply of PMF} = \frac{\text{Supply of Post-PMF Startups}}{\text{Supply of Pre-PMF Startups}}$$

So what explains the pre-2022 run up in the Series A / Seed premium: declining (relative) _supply_ of PMF or rising (relative) _demand_ for PMF?

My argument: **rising demand for PMF.**

First, as I've argued elsewhere, the growth and development of the venture ecosystem has been a mostly demand-side story. That should strongly bias us toward the demand explanation:

> Investors are the primary driver of fluctuations in venture activity and equity prices around their long-run trend – [We Don't Have Nearly Enough Startups](https://whoisnnamdi.com/not-enough-startups/)

> The venture ecosystem is supply-constrained – there isn't nearly enough startup equity out there to satisfy investor demand.  
> Additional capital drives opportunistic company formation at the Seed stage. However, the additional capital doesn't improve survival to the later stages – it simply drives prices up for the remaining companies – [It's Valuations (Almost) All the Way Down](https://whoisnnamdi.com/its-valuations/)

Second, the ratio of Series A to Seed transactions has been reasonably stable since about 2014. It's declining somewhat (Seed rounds are growing slightly faster than Series As) but not nearly as rapidly or vigorously as relative prices have moved. In fact, it's remarkably consistent, even through the ups and downs of the last few years:

![premiums_deals](https://nnamdi.net/content/images/2024/01/premiums_deals.png)

-   Seed activity (the denominator) exploded through 2014, bringing down the ratio (A long time ago, Series As were more common than Seeds)
-   The Series A vs Seed ratio stabilized thereafter, only slightly falling over the years

My read: this is evidence of stable relative supply of pre vs. post PMF companies. More often than not, when quantities are steady (as in the chart) and prices are rising, it's due to constrained supply rather than constrained demand.

So, investor demand for de-risked companies rose substantially, driving higher relative prices for Series A companies. Meanwhile, the supply of such opportunities didn't grow to match that, so we didn't see a dramatic change in Series A relative to Seed activity.

## Busted

Ok, so we have a plausible story for everything that happened up until the 2021 market peak. But things obviously turned after that. For my theory to be credible, it has to offer some explanation for the recent downturn that fits the data.

Speaking of data – everything we've looked at thus far is from the perspective of the overall market. But individual companies and founders don't experience the aggregate, they live their own particular trajectory, raising subsequent rounds spaced apart rather than at the same time. A founder looking to raise Series A cares about how the market looks 18-24 months _after_ they raise their Seed:

![Pasted-image-20231024140149](https://nnamdi.net/content/images/2024/01/Pasted-image-20231024140149.png)

Let's take the founder perspective and compare Series A deal activity to Seed activity 21 months prior, a reasonable (median) estimate for the time it takes to raise that next round. Think about it as a rough proxy for the Seed to Series A "graduation" or "survival" rate. Not every Series A was preceded by a Seed financing, so this is more of an upper bound for the graduation rate:

![premiums_deals_a](https://nnamdi.net/content/images/2024/01/premiums_deals_a.png)

-   From 2014-2020 it's a very similar picture to what we looked at before, a stable "graduation rate"
-   Conditions improve substantially in 2020, i.e. more Seed companies survive to the next round
-   However, post 2021 the graduation rate collapses to the lowest levels ever seen, ~20%

So, while the ratio of Seed to Series A deals at any particular point in time has been stable, due to the timing discrepancy (founders raise their Series A some time after the Seed), the numerous startups that raised Seed funding during the boom are now facing a massive bust. Ironically, though 2021 was the "best" time to raise a Seed based purely on venture activity and valuations, **it was the "worst" time to raise Seed funding once you account for today's tough Series A environment.**

This simple analysis doesn't accurately identify the "level" of the graduation rate, but it proxies the "change" over time. It's safe to say **the graduation or survival rate from Seed to A has fallen by more than half.** Other sources like Charles Hudson, Jamesin Seidel, and [Carta](https://www.linkedin.com/posts/peterjameswalker_cartadata-seed-seriesa-activity-7128075191611523072-twip/) corroborate this:

> **Decline in Graduation Rate from Seed to Series A -** Historically, we've seen a strong pipeline of companies moving from seed to Series A. Recent numbers, however, indicate a significant decline in this graduation rate. Measured graduation rates will continue to fall for several quarters as companies go out for and fail to raise Series A rounds. Graduation rates from seed to Series A could drop to 25%, or one-third or one-half of what they were at the peak. – [The Big Reset in Seed to Series A Graduation Rates is Real and Permanent](https://chudson.substack.com/p/the-big-reset-in-seed-to-series-a)

> Back in 2020, approximately 23% of Seed-stage startups made it to Series-A within two years. Fast forward to 2022, and the market looks different. Over the past 20 months since the start of 2022, the graduation rates have decreased to 5%. – [A Deep Dive into Q3 2023's Funding Landscape](https://jamesin.substack.com/p/a-deep-dive-into-q3-2023s-funding)

![Pasted-image-20231030084014](https://nnamdi.net/content/images/2024/01/Pasted-image-20231030084014.png)

> The percentage of companies who make it from seed to Series A within two years fell by a lot for the 2021 seed cohort.
> 
> 27.5% of companies that raised a seed round in 2019 made it to Series A within 2 years.
> 
> Only 17.6% of companies who raised their seed in 2021 have "graduated" to the next round. – [Peter Walker on LinkedIn](https://www.linkedin.com/posts/peterjameswalker_cartadata-seed-seriesa-activity-7128075191611523072-twip/)

![Pasted-image-20231108101945](https://nnamdi.net/content/images/2024/01/Pasted-image-20231108101945.png)

How does my theory explain this data? Put simply, **venture investors re-assessed their beliefs about Series A companies.** They no longer see Series A as a substantial indicator of product-market fit, given so much risk still remains. That's why their relative price has fallen off.

This is different in subtle ways from the common "Series A investors have raised their expectations" narrative:

-   If it was only about loftier expectations, the Series A premium would have _risen_, since only the best companies would be getting funded, and those companies would fetch a high price.
-   That's _not_ what we see – relative prices have fallen, not risen.

Investors haven't only raised their expectations; they just don't see Series A as indicative of PMF to begin with. If they did, the relative price of Series A companies would still be elevated. Instead, **they no longer see the point in paying up for them.**

Again, return to the education analogy:

-   If the college wage premium plunged, we'd say employers no longer see the college-educated as having "_labor market fit_". In other words, they lost faith in the college degree as a signal of quality in the labor market
-   You would _not_ intuitively connect this to rising employer expectations (though that could certainly be true too)

So sure, investors raised their expectations – you need more revenue to raise your Series A, more users, etc. But the bigger factor is that investors' _beliefs_ about Series A changed.

**Investors today expect more but believe less.**

## If the product doesn't fit, you must acquit

I have abused the expression "post-PMF". The reality is most companies do not achieve product-market fit by their Series A, and that's always been true. But that's my entire point – investors have wizened up to the fact that Series A companies still have material risk.

PMF is more fleeting than we all had appreciated. Series A valuations corrected to reflect this.

Balance has returned to the Force. Maybe that's a good thing. But it's dreadfully volatile if you're living it in first-person as a founder. The Series A market got the wind knocked out of it just as the large 2021 Seed cohort came up for air.

While it's a tough time for founders, I think it's important to be clear-eyed about the reality of the situation. The bar has been raised, but further, investors are questioning the bar as a meaningful indicator of success in the first place.

So yes, your first priority is still to raise that next round, but there's still much work to be done thereafter. More so than ever, Series A does not mean you have product-market fit. Investors don't believe it and neither should you.

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div> ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[How Redpanda is Taking Data Streaming Mainstream]]>
                        </title>
                        <description>
                            <![CDATA[Monitoring that even (Franz) Kafka would approve of]]>
                        </description>
                        <link>https://whoisnnamdi.com/data-streaming-mainstream/</link>
                        <guid isPermaLink="false">data-streaming-mainstream</guid>
                        <category>
                                        <![CDATA[ Developers ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Thu, 10 Aug 2023, 02:25:00 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512297592-blog-lightspeed-hero.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
Real-time data systems tend not to have amazing developer experiences. There are two core reasons.

One, the developer experience for the engine itself is poor. Streaming engines tend to be difficult to use, difficult to spin up, and difficult to operate on an ongoing basis, as they have a ton of moving parts and complexity to manage. This is especially painful when trying to self-host, develop locally, or integrate into a modern [continuous integration/deployment (CI/CD) pipeline](https://redpanda.com/blog/test-driven-development-ci-testing-kafka). As a result, only the most sophisticated and experienced engineers can make proper use of these systems, leaving behind most software developers and narrowing the market for event-based architectures.

Second, most streaming data systems don’t integrate well into the rest of the developer workflow, and they lack many of the “creature comforts” developers are used to seeing in their other tools. The modern developer workflow extends much further out and is much broader in scope than the streaming engine itself. For streaming engines to cement their place in the hearts and minds of developers, they must offer solutions to the rest of the puzzle.

We’ve talked before about [why developers love](https://medium.com/lightspeed-venture-partners/why-developers-love-redpanda-30bf2f3b8231) Redpanda’s core developer experience, which is leaps and bounds above what’s come before in the streaming space. With [native Raft](https://thenewstack.io/raft-native-the-foundation-for-streaming-datas-best-future/), no JVM, and full Kafka compatibility, Redpanda enables developers to do their best work, reducing unnecessary complexity and toil while doing it all at the [highest throughput, lowest latency](https://redpanda.com/blog/redpanda-vs-kafka-performance-benchmark), and, importantly, [lowest cost](https://redpanda.com/platform-tco).

![Comparison of infrastructure and admin costs between Redpanda and Kafka](/https://nnamdi.net/content/images/2025/03/6687382f21aa7598bd65ede7_blog-lightspeed-img1-1.png)
*Comparison of infrastructure and admin costs between Redpanda and Kafka*

But it’s also important to address that second point — how Redpanda creates a holistic developer experience that services all aspects of the modern developer’s workflow, from deployment to monitoring and Day 2 operations to long-term storage. These remain unresolved, “open loops” that have prevented a broader audience of developers and organizations from taking advantage of real-time capabilities.

At Lightspeed, it’s our core belief that there’s a potentially massive market expansion opportunity for real-time, streaming data systems. Opening up this opportunity will require thinking much bigger about what a streaming engine can be and do.

That’s where Redpanda comes in.

## Spinning up without spinning your wheels

Too often, developers looking to deploy streaming data systems are left to their own devices when it comes to deployment. Deployment is often a mess, involving multiple separate binaries, each with its own peculiarities, requiring engineers to develop expertise in systems they couldn't care less about and in which they have no comparative advantage. Streaming systems can easily become one of the most painful pieces of infrastructure they’ll deploy.

Further, developers increasingly see the cloud as their default deployment platform. In the same way that SaaS relieves some of the operational burden of running software by offloading that to the vendor, developers would love to be able to consume cloud-based streaming services with as few wrinkles as possible. Developers want a cloud experience but their organizations don’t want to lose data sovereignty. They need a security model that works within the constraints of the typical enterprise while also getting the ease of use of the cloud — [the best of both worlds](https://redpanda.com/blog/deploy-redpanda-clusters-cloud-aws-gcp).

Lastly, developers on the bleeding edge don’t want to think about infrastructure at all, cloud or otherwise. They don’t want to think about servers, machines, nodes, availability zones, or regions. They want access to true streaming as a service rather than streaming as a server. In other words, they want serverless, and they want it now.

The modern developer experience requires a modern platform. Redpanda is that platform.

Here’s how Redpanda makes deployment a breeze.

## One binary to rule them all

The core of the Redpanda deployment model is the way its nodes are architected. Redpanda nodes are [fully-contained processes](https://redpanda.com/blog/single-binary-architecture) that ship with everything needed in a modern streaming system, including an HTTP proxy, a Raft-based consensus mechanism, and a schema registry. Every node runs the same, single binary, leading to significant operational simplification along with a more efficient overall operating model.

![Close up of a self-contained Redpanda node ](/https://nnamdi.net/content/images/2025/03/6687382d569120c8751b8575_blog-lightspeed-img2.png)
*Close up of a self-contained Redpanda node*

It’s hard to overstate how game-changing this is. Due to this architecture, running a single Redpanda cluster doesn’t require operating a massive fleet of varying services. This is a massive boon for developer productivity and also enables Redpanda to go where previous streaming systems have struggled, including [edge / IoT deployments](https://redpanda.com/blog/real-time-security-iot-customer-story) and CI/CD pipelines with tight performance requirements and resource constraints.

> “The latency and the throughput we got with almost no configuration was incredible. Redpanda is fast, reliable, friction-free and has very low operational overhead.” - [Alpaca](https://alpaca.markets/)

## Hold my cloud

Redpanda’s cloud turns the table on traditional cloud infrastructure. While the company does offer standard, dedicated, managed clusters in the cloud, they didn’t want to stop there. Redpanda went further, pioneering a deployment model they call [BYOC](https://redpanda.com/blog/deploy-redpanda-clusters-cloud-aws-gcp), or, “bring your own cloud.” BYOC lets customers deploy Redpanda clusters within their own cloud environment, while still being fully managed by Redpanda.

> “BYOC’s privacy-first architecture drives compliance for streaming data, and allows you to scale on your own infrastructure while maintaining data sovereignty requirements.” - [Bring Your Own Cloud (BYOC): best of both worlds](https://redpanda.com/blog/deploy-redpanda-clusters-cloud-aws-gcp)

Redpanda does this by cleanly splitting the control and the data plane. The data plane stays within the customer’s cloud account, while the control plane stays on Redpanda’s side. This creates an incredibly slick and elegant setup, with proper separation of concerns between Redpanda’s and the customer’s cloud environment. With BYOC, organizations get all the benefits of the cloud while retaining [data sovereignty](https://redpanda.com/blog/kafka-redpanda-future) and maintaining privacy.

![Diagram of how BYOC keeps the customer cloud separate for data sovereignty and privacy](/https://nnamdi.net/content/images/2025/03/6687382d907eec59adc13e52_blog-lightspeed-img3.png)
*Diagram of how BYOC keeps the customer cloud separate for data sovereignty and privacy*

Redpanda takes care of everything you’d expect in a modern, competent cloud service, like provisioning, monitoring, and maintenance, while sensitive data and credentials never leave the customer environment. Rolling upgrades that the customer can control ensure zero application downtime. For obvious reasons, BYOC has become an incredibly popular deployment paradigm among Redpanda users.

> “Redpanda BYOC gives us a fully managed Kafka service running on our own cloud servers, balancing our internal compliance requirements with ease of use, and without compromising performance and compatibility.” - [LiveRamp](https://liveramp.com/)

## Fewer servers, better service

Lastly, Redpanda is working on a number of exciting features around serverless, a relatively new paradigm within the context of streaming systems. This is their focus on developer experience taken to the logical extreme — what a streaming data system could look like from the perspective of a developer who doesn’t want to worry about systems or deployment at all.

Importantly, serverless isn’t some esoteric technology that is far ahead of where developers are today. Serverless is here now. According to Datadog’s [2022 State of Serverless](https://www.datadoghq.com/state-of-serverless/) report, over half of the organizations surveyed in each of the major public clouds have already adopted serverless in some fashion. For AWS in particular that number is over 70%.

And yet streaming data systems haven’t kept pace. Developers today who want to pair streaming data with a serverless approach are largely figuring it out on their own.

We believe that serverless, in addition to [Redpanda’s planned WebAssembly capabilities](https://medium.com/lightspeed-venture-partners/webassembly-ing-the-pieces-vectorizeds-data-policy-engine-5ceea983ed5d), is the last major step toward unlocking accessible streaming and event-based systems for the great majority of developers out there. Developers will be able to talk to a streaming data system in their native tongue and perform data transformations on the fly, almost like a universal Google Translate for streaming data.

With JavaScript and Python being the world’s most popular programming languages broadly and most used specifically within serverless functions, Redpanda’s upcoming serverless offerings will expand the relevant, addressable audience for streaming data by an order of magnitude or more.

## Monitoring that even Franz Kafka would approve of

Streaming systems can be daunting, intimidating systems that are difficult to manage for the average developer. These difficulties are made no easier by the fact that the command line (i.e. a dark screen with some white text scrawled over it) is often the default way to manage these systems. Ad hoc inspection and analysis is much harder than it should be.

This is especially frustrating during crisis situations, such as production infrastructure going down. In such situations, time is of the essence, and engineers don’t have time to wrestle information out of their tools, searching for obscure command line invocations that they didn’t even know existed. Once the information is located, it often appears as a torrential downpour of logs and metrics, with little to no organization or formatting. We’ve all been there.

This just won’t do. While many developers swear by their terminals, many would love to have a more visual and intuitive representation of their infrastructure. This is especially true of younger, more junior developers who didn’t grow up writing MS-DOS applications and who have a higher standard for developer experience.

[Redpanda Console](https://redpanda.com/redpanda-console-kafka-ui) is a single pane of glass for managing the entire Kafka ecosystem. All of your streaming infrastructure in one place, with all the admin capabilities organizations expect:

-   Observability over clusters, topics, brokers, and partitions
-   Ability to easily change consumer group offsets
-   Management of schema registry, connectors, and Kafka Connect clusters

![Preview of how Redpanda Console serves as a dev-friendly pane of glass.](/https://nnamdi.net/content/images/2025/03/6687382d251781c480c13fdc_blog-lightspeed-img4.gif)
*Preview of how Redpanda Console serves as a dev-friendly pane of glass.*

But the most impressive thing about Redpanda Console is that it’s not merely a “dashboard” with a slew of charts and numbers — it goes so much further than that.

Redpanda Console gives developers data observability superpowers they aren’t used to. This includes capabilities like push-down JavaScript filter, allowing users to filter their cluster’s data at any level of complexity by writing programmable data filters in JavaScript, with encoding support for JSON, Protobuf, Avro, and more.

The console also simplifies and strengthens access control. The console presents an easy interface for configuring access control lists (ACLs), setting up fine-grained role-based access control (RBAC), and reviewing comprehensive audit logs. It also integrates tightly with all the identity providers you’d expect, like GitHub, Okta, and Google, enabling Single Sign On (SSO) access.

![Screenshot of the Kafka Access Control interface in Redpanda Console.](/https://nnamdi.net/content/images/2025/03/6687382d569120c8751b8579_blog-lightspeed-img5.png)
*Screenshot of the Kafka Access Control interface in Redpanda Console.*

> “Redpanda Console delivers a tremendous improvement in the productivity, effectiveness and quality of life of developers and operators who work with Redpanda or Kafka” - [Redpanda Console: Putting the “fun” back into Kafka](https://redpanda.com/blog/kafka-ui-redpanda-console)

## Store more

Okay, so a developer is running Redpanda. They’ve spun it up within their own cloud account as enabled by the BYOC model, and they have the beautiful Redpanda Console setup giving them full observability and control over their cluster. Everything is hooked up and data is flowing through Redpanda.

But all that data has to go somewhere. It’s time to talk _storage_.

Storage is quite important in the context of event-based architectures:

-   Systems like Redpanda sit between data producers and consumers, allowing each to go about their business without having to worry about what the other side is doing.
-   However, due to severing this tie between producers and consumers, the responsibility of confirming successful writes falls on the streaming system.
-   Data can be produced well in advance of being consumed, so streaming systems must guarantee that no data gets lost in the shuffle.

This is why Redpanda went out of its way to [submit itself for testing](https://jepsen.io/analyses/redpanda-21.10.1) by trusted third parties like [Jepsen](https://jepsen.io/).

> “How did Redpanda fare in its Jepsen testing? Redpanda is a safe system without known consistency problems. The consensus layer is solid. The idempotency and transactional layers had issues that we have already fixed. The only consistency findings we haven’t addressed reflect unusual properties of the Apache Kafka protocol itself, rather than of a particular implementation.” - [Redpanda’s official Jepsen report](https://redpanda.com/blog/redpanda-official-jepsen-report-and-analysis)

But it’s not just about the [CAP theorem](https://en.wikipedia.org/wiki/CAP_theorem) and distributed systems. Putting on our investor “cap” for a second — data gravity is how nearly every mission-critical data system of the past generated significant value for its creators and its customers. For a data infrastructure company to build a large and valuable business, it must hold and retain customer data for extended periods of time. Ephemerality won’t cut it:

-   Such a data system cannot merely be a place where data passes through on its way to its final destination. Such architectures and usage patterns have rarely generated multi-billion dollar outcomes. Instead, the system in question must become a trusted place for developers and organizations to store data on an ongoing basis
-   Users must be able to trust that once ingested, the data will always be there (or at least as long as their data retention policies demand).
-   Users must also trust that, once the data is there, it will be easily queryable and accessible.

## A sane default: the cloud

Here too, Redpanda has it covered. Presciently, the team built Redpanda with the cloud as its default storage tier. This is a stronger statement than merely being “cloud-native”: Redpanda intelligently manages reads and writes behind the scenes to provide the lowest friction and highest performance, effectively becoming an infinite storage destination:

> “Making cloud the default storage tier unlocks new streaming data use cases that were once considered out of reach. The long-term data retention capability encourages businesses to treat Redpanda as the “single source of truth” for their historical records.” - [How Redpanda’s cloud-first storage model reduces TCO](https://redpanda.com/blog/cloud-native-streaming-data-lower-cost)

In addition to unlimited data retention, Redpanda’s cloud-first architecture enables:

-   [Remote read replicas](https://redpanda.com/blog/remote-read-replicas-for-distributing-work) — a “CDN” for your streaming data, enabling developers to spin up and hydrate a new Redpanda cluster with mirrored data from an existing cluster, even across zones and regions
-   [Inexpensive disaster recovery](https://redpanda.com/blog/high-availability-software-deployment-patterns-part-1) — smooth failovers, with no need to maintain a separate data replication mechanism
-   [Reduced data management, admin, and toil](https://redpanda.com/blog/engineering-continuous-data-balancing) — Continuous data balancing, self-healing clusters, and unified retention controls

Redpanda accomplishes this with its [Shadowing Indexing](https://redpanda.com/blog/tiered-storage-architecture-shadow-indexing-deep-dive) architecture, which was built from scratch to support cloud-native [Tiered Storage](https://docs.redpanda.com/docs/manage/tiered-storage/), enabling Redpanda to seamlessly move data between brokers and reliable, cheap cloud stores like Amazon S3 or Google Cloud Storage (GCS). Users can access their data using the same Redpanda/Kafka APIs they’re used to, while getting infinite data retention and scalability for free.

![Diagram of how Redpanda’s Tiered Storage works with Amazon S3.](/https://nnamdi.net/content/images/2025/03/6687382d1dd2b64747bcf80f_blog-lightspeed-img6.png)
*Diagram of how Redpanda’s Tiered Storage works with Amazon S3.*

The upshot of all this innovation is amazing ease of use and performance at an incredibly low cost.

## Data unchained

We’ve already talked a bit about [data sovereignty](https://redpanda.com/blog/kafka-redpanda-future) in the context of the cloud, where security and retaining control of enterprise data are key concerns. That’s a very _literal_ interpretation of sovereignty. But there’s another, potentially equally important notion of control that developers and organizations care about, but often give up as a side effect of adopting cloud data stores. Often enough, one doesn’t even realize something has been lost until you’re in too deep.

> “Data sovereignty is much harder to achieve than data privacy. Privacy can be achieved with policy: delete this, mask that, obfuscate here, index like so. Sovereignty can only be achieved if you, the user, control the hard drive lifecycle where data resides. There are no two ways about it. Data either lives inside the hard drives that you control or it does not.” - [Data sovereignty is the future of cloud](https://redpanda.com/blog/kafka-redpanda-future)

Redpanda believes developers should control the data they produce. This goes without saying in the world of on-prem. However, we cannot take this for granted in the era of the cloud:

-   That hard drive holding your data is no longer running in your data center — it’s running in someone else’s.
-   Further, once it ends up in that external store, many vendors erect all sorts of barriers to accessing and manipulating the data: esoteric and proprietary data formats, domain-specific query languages, egregious egress fees, and the like.

These aren’t (pleasant sounding) data lakes or even (somewhat less enticing) data warehouses, they’re _data jails_, and the vendor holds the only key. As far as Redpanda is concerned, that isn’t true data sovereignty.

Redpanda wants to fix this. Leveraging popular open data formats like [Apache Iceberg](https://redpanda.com/blog/apache-hudi-iceberg-delta-lake-differences) and the unique BYOC architecture we discussed earlier, Redpanda will enable developers to finally regain ownership over their data. Developers will be able to pick their preferred storage vendor while maintaining ownership, queryability, indexing, and portability no matter where the data ends up. Today it’s S3, tomorrow it’s Snowflake and Databricks, and in the future, it’ll be any vendor they’d like. Organizations will also be able to bring their own query engines to the data, no matter the underlying data format.

Developers will once again own their data. All enabled by Redpanda.

## Conclusion

Redpanda meets developers where they are and then enables them to go even further. That reach will push Redpanda to places no streaming system has gone before, driving a level of mission-criticality beyond most other developer infrastructure. The ease of use and incredible flexibility of Redpanda will generate developer love and appreciation for the product. And, with infinite retention and a strong commitment to data sovereignty, Redpanda’s “data gravity” will only grow.

![Diagram of the Redpanda compatibility ecosystem](/https://nnamdi.net/content/images/2025/03/66a3d55edc368c41e27704d1_66873620cc1c1391aec633b1_blog-gaming-img2.png)
*Diagram of the Redpanda compatibility ecosystem*

It’s why we were so excited to lead their recent [$100M Series C financing](https://medium.com/lightspeed-venture-partners/why-lightspeed-is-leading-redpandas-100-million-series-c-553ffe38d6e). The real-time revolution has only just begun, and Redpanda is leading the charge. ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[The Venture Activity Index]]>
                        </title>
                        <description>
                            <![CDATA[Measuring the state of the venture "business cycle"]]>
                        </description>
                        <link>https://whoisnnamdi.com/venture-activity-index/</link>
                        <guid isPermaLink="false">venture-activity-index</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Tue, 25 Jul 2023, 09:09:47 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512296049-header.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
It's no secret – unless you're building in AI, the venture market isn't very kind right now.

After a euphoric 2021, funding took a nosedive, falling to the lowest pace we've seen in some time. While economists and politicos debate whether we're in an economic recession, there's no debate in the venture economy – **it ain't pretty out there for founders trying to raise capital right now.**

But that sentiment is somewhat anecdotal, backed by the gut feeling of venture market participants. It'd be great to have a view of the venture cycle that was backed up by the data, some sort of indicator of the phase of the cycle we're currently living through.

So I came up with a methodology for measuring the state of the venture "business cycle" – how the venture market is performing relative to some underlying notion of "trend". Its construction is actually quite simple, and the result is something I think could serve as a useful barometer for the ecosystem as we chart a course from here.

It's called the Venture Activity Index, or "VAI". Let me walk you through how I got there.

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div>

## Quick stats

To measure the venture cycle, we need a notion of "trend" – the thing around which the venture market is fluctuating. If we plot funding growth on a logarithmic scale across each stage of venture investment, a clear linear trend emerges. (Note: linear trends in logarithms implies constant percentage growth over time):

![detrend](https://nnamdi.net/content/images/2023/07/detrend.png)

I could have come up with a fancier notion of trend, but I'll proceed with the linear one as it's nice and simple.

Venture activity, as defined by aggregate funding, fluctuates up and down around this trend. The slope varies by stage, but it's a fairly consistent story across stages – **on average, funding grows ~5% each quarter.**

One quick note: I could have focused instead on the number of investments rather than the scale of investment activity measured in dollars. However, working in dollar terms is nice because it implicitly accounts for valuations, which is critical for holistically characterizing the state of the venture market.

Next, let's remove that trend and focus on the gyrations around it:

![cycles](https://nnamdi.net/content/images/2023/07/cycles.png)

-   The cycles were only moderately correlated across stages before 2020. The average cross-stage correlation hovered around ~0.25 pre-2020.
-   However, since 2020, they've moved in lockstep, following one common cycle up and then down.

This is typical in economic data: in volatile times, different parts of the market often become much more tightly correlated. It's interesting to see the same phenomenon in the venture data. Further, it's important that the various stages of venture correlate with one another, as otherwise there'd be no sense in talking about a singular "cycle" for the whole ecosystem.

Overall, the correlations are around ~0.65 for the whole period, with the seed stage being the most dissimilar to the other stages of investment:

![correl](https://nnamdi.net/content/images/2023/07/correl.png)

The amplitudes of the cycles, however, differ across stages, both historically and more recently:

-   The seed stage is relatively steady, never more than 50% off trend, with a quarterly standard deviation across time of about 20 percentage points.
-   Series A is a bit more volatile but not significantly so.
-   Growth and later stages are the most volatile, ranging from a standard deviation of ~30 p.p. for Series B all the way to ~50 p.p. for Series D+.

In other words, booms and busts are substantially larger at the later stages:

![volatility](https://nnamdi.net/content/images/2023/07/volatility.png)

And in case you're wondering, this was true before 2020 as well. It's not just a recent phenomenon: the late-stage market has always required a bit of a strong stomach.

## One metric to rule them all

Now let's throw it all together. Again, in the spirit of simplicity and robustness, let's take the simple average of the individual trends to arrive at a blended view of the venture ecosystem.

I present to you… the Venture Activity Index (VAI):

![vai_plot](https://nnamdi.net/content/images/2023/07/vai_plot.png)

The last three years of the VAI are the obvious anomaly:

-   Capital deployment rose to 90% above trend, exploding over only a few quarters.
-   The correction was just as vigorous, bottoming out at ~60% below trend and stable for the last two quarters.

**We haven't seen anything like this in the prior ten years of activity.** Previous peaks and valleys were at most +/-25%. The volatility of the last three years is _unheard of_ in recent memory.

When knocked off trend, venture activity returns to steady state after ~1.5 years. Only in that sense was the exuberance of 2021 relatively normal – it lasted roughly as long as such booms tend to last. We'll see if that behavior continues.

Again, I want to emphasize this is a measure of the venture business _cycle_, not the trend itself, which we first removed. A rise or decline in the VAI implies funding grew faster or slower than trend, respectively, _not_ that funding rose or fell in absolute terms.

Due to its cyclic nature, the VAI forecasts future changes in venture activity. When it's high, future venture activity slows. When it's low, venture activity accelerates in the next few quarters. This is true across stages, though it's most accurate for the later stages:

![vai_forecast](https://nnamdi.net/content/images/2023/07/vai_forecast.png)

Overall, a 10% increase in the VAI forecasts anywhere from a 6.5% decline in seed stage venture funding over the next year to a 21.5% decrease at the Series D+ stage (and vice versa for a decrease in the VAI).

## Conclusion

The Venture Activity Index is a simple and informative indicator of the state of the venture "business cycle". In a single number, it quantifies the "gut feelings" we all have about the venture climate today, in the past, and in the future.

To wrap up, a few caveats:

-   The backward-looking trend updates every quarter, which can slightly change the historical numbers as new data comes in.
-   A linear trend may not make sense in the long-run. For example, if the rate of capital deployment never returns to its prior trend, at some point we'd have to accept that a new trend has been established, making the old one irrelevant. I may later change my calculation of trend to address this.
-   My simple average of the various stage-specific trends could be improved upon with some sort of factor analysis technique. Again, I may switch to this at a later date.

**I plan to update the VAI quarterly as new data comes in.** For the latest data, look [here](https://whoisnnamdi.com/vai).

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div> ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[The Shadow Price of Venture Capital]]>
                        </title>
                        <description>
                            <![CDATA[Valuations are 60% too high relative to the volume of venture funding]]>
                        </description>
                        <link>https://whoisnnamdi.com/shadow-price/</link>
                        <guid isPermaLink="false">shadow-price</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Wed, 14 Jun 2023, 17:45:20 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512294402-header.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
Venture valuations have fallen off a cliff, but they are still too high.

Valuations rise and fall with the volume of dollars invested in startups according to a stable ratio: for every one percent change in funding, valuations move two-thirds of a percent.

But since peaking in late 2021, valuations have only fallen 0.4% for every 1% drop in funding. This pricing "error" has accumulated: **today's valuations are 60% higher than you’d expect for the amount of capital invested.**

In other words, the "shadow price" of venture capital is a lot lower than what we're seeing in announced transactions these days. Never before have valuations and funding diverged so meaningfully from their long-run equilibrium, for so long.

What does it all mean?

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div>

## A 3-for-2 special

Valuations and capital invested in venture have both grown substantially since 2010:

![val_cap](https://nnamdi.net/content/images/2023/06/val_cap.png)

Interestingly, they trend together. As one rises, the other does too. Same in the other direction.

It's not a perfect relationship, but there's clearly something tying them together.

In fact, you can predict average valuations from the capital invested each quarter. The below chart plots actual valuations over time against what we'd predict based on capital deployment – a simple regression of valuations on funding:

![val_pred](https://nnamdi.net/content/images/2023/06/val_pred.png)

**Capital invested predicts the average valuation of deals done each quarter.**

Most of the time, the "errors" of this capital-based prediction are small. Further, these deviations typically correct themselves within a quarter or two.

In this sense, **venture valuations are a function of capital flows.** More capital drives valuations ever higher, in a fairly predictable fashion. There's an equilibrium, a balance between the two.

They don't grow at the same rate, however. For every 1% increase in funding, valuations rise about 0.66% or two-thirds of a percent. If you like clean whole numbers, the ratio is about 3:2.

Most of the increased funding goes to rising prices rather than more individual investments. This mirrors points I've made previously:

> "the valuation inflation we've seen "comes from" the incredible growth in demand and lack of supply of startup equity"  
> …  
> "Additional capital drives opportunistic company formation at the Seed stage. However, the additional capital doesn't improve _survival_ to the later stages – it simply drives prices up for the remaining companies" – [It's Valuations (Almost) All the Way Down](https://whoisnnamdi.com/its-valuations/)

## Price check, please?

Economists have a term I think is relevant – **shadow price** – or, the price of something that either isn't typically traded in the market or for which accurate pricing is hard to come by. For example:

-   The price of an illicit good (drugs, contraband of various sorts, etc)
-   In certain economies like Argentina, the shadow or "black market" price for exchanging the local currency for a foreign currency like the U.S. dollar, which differs from the "official" rate.

Why is this relevant to venture capital?

Well, because so many venture transactions _don't_ happen. Most startups fundraise only once in a while. Founders delay fundraising if they can't fetch an attractive price from investors. Down rounds are verboten. Those deals are _missing_ from the quarterly venture activity data:

-   Accounting for these phantom fundraises would lower the average venture valuation, since low prices are the whole reason those deals aren't happening. In other words, there's massive **selection bias**.
-   Companies also tend to be more public about their valuations the higher they are. Journalists love reporting on high valuations. As a result, the data we have on valuations is also likely skewed too high due to **reporting bias**.

The prices we see investments getting done at are misleading. Thus, the concept of a shadow price applies to venture – it's the price that would prevail if companies were forced to transact at current prices and we had perfect data on all fundraises.

I think of the dashed line in the chart above as something akin to the "shadow valuation" of venture capital investment.

## Wow, that's a high price!

The reliable relationship between prices and funding has broken down in recent quarters.

While valuations and funding have both declined since late 2021, funding declined by much more. Valuations have not fallen by nearly as much as the 3:2 ratio would imply. Instead, since the market peaked the ratio has been more like 5:2, or, said differently, valuations have only fallen 0.4% for every 1% drop in funding.

In other words, even after collapsing, valuations are still too high relative to their historical relationship with funding.

We can visualize this growing "error" – the percentage difference between actual and predicted valuations – shown below. Above the zero line means valuations are too high, below means valuations are too low:

![coint_res](https://nnamdi.net/content/images/2023/06/coint_res.png)

**Current valuations are ~60% too high relative to the volume of capital being deployed.** There just aren't enough dollars sloshing around to support these prices, at least based on the 10 years of venture history prior to the go-go days of 2021.

Since 2010 valuations have rarely been "off" by more than 20%. 60% is _unprecedented_.

Again, notice how in the past any deviation from equilibrium quickly reverts, nearly always within a quarter or two. Something has changed – "error correction" is _totally absent_ in recent quarters.

**This is worrying.** Never in the last decade-plus of venture activity have valuations and funding flows diverged so meaningfully from their long-run equilibrium, for so long. Could this mean there's a lot more pain ahead?

## These deals won't last

Now, it's totally possible we have it all backwards.

Till now I've taken for granted the idea that capital flows drive valuations.

Perhaps valuations have risen for separate, independent reasons and funding volume grew to meet these new prices. Thus, it could be funding that corrects itself, rather than valuations. Perhaps the current gap signals funding is _too low_ rather than warning valuations are _too high_.

Let's check what happened in the past. That is, **how have valuations and funding historically reacted to past deviations from their long-run ratio?** That would be a strong clue as to which is the driver and which is the passenger here.

That's straightforward enough – just run a regression of valuation and funding growth on deviations. If valuations react more, then we know valuations correct for past deviations from equilibrium. If invested capital reacts more, then its funding that corrects.

Turns out, valuations correct past deviations (left chart), funding does not (right chart):

![err_corr](https://nnamdi.net/content/images/2023/06/err_corr.png)

When valuations and funding volume drift apart, it's valuations that come running back:

-   If valuations are high relative to funding, valuations tend to fall the following quarter
-   If valuations are too low relative to funding, valuations rise next quarter

Funding doesn't respond at all.

Deviations reflect unusual valuations rather than abnormal funding. **Valuations are out of whack, not funding volume.**

Pricing "errors" forecast future valuations, since valuations reliably and quickly respond to those deviations in later periods. The implication? **Valuations should fall dramatically from here.** That they haven't yet can be chalked up to a combination of market inefficiency and reluctance.

Just like in a Keynesian model of the economy, where sticky, slow to adjust prices exacerbate economic downturns, "sticky valuations" lengthen and worsen venture recessions. Deals that could get done at more reasonable valuations don't happen, as founders and existing investors don't want to take the hit:

> Private valuations lag public valuations, often by a substantial amount and for a long time – [Old Valuations Die Hard](https://whoisnnamdi.com/old-valuations/)

We should prefer swift, definitive corrections over slow moving train wrecks. Instead, we drag things out. Everyone suffers as a result.

## Tourist prices

That venture prices are tied so strongly to capital flows is striking:

-   This contradicts standard corporate finance which values companies based on fundamentals. The amount of capital in the market shouldn't impact valuations if the intrinsic value hasn't changed.
-   However, that's not what we see – there doesn't seem to be anything "fundamental" anchoring the price of startups. Startups are priced based on the amount of capital in the private markets at the time.

**More capital, higher prices.** Again, it's not one-for-one, but it's clear most of the capital goes toward higher prices rather than more deals getting done.

In my last essay, I was struck by how sensitive private valuations were to interest rates:

> … **seven quarters out valuations are up ~25%**, falling back to their original level after three years – [Don't Discount Interest Rates](https://whoisnnamdi.com/discount-rates/)

As readers pointed out, a pure discounted cash flow (DCF) analysis would never suggest such a large impact. This is a fair point, and one I too pondered.

However, these magnitudes make much more sense in the context of capital flows. Low interest rates attract capital to risky assets like venture capital as investors [reach for yield](https://www.richmondfed.org/~/media/richmondfedorg/publications/research/econ_focus/2013/q3/pdf/federal_reserve.pdf) unavailable elsewhere. If capital flowing into startups raises prices, then we really have two simultaneous effects:

-   The first is traditional, static, corporate finance, which says interest rates (or rather, the discount rate) influence valuations.
-   The second is the dynamic effect of interest rates on venture capital funding, which then impacts valuations via the 3:2 relationship I studied above.

This double whammy is how you end up with such severe valuation swings.

Ironically, it's the tourists who "price" venture investments on the margin – those much-derided investors who, like birds, cyclically flock to and flee from venture investing with the changing economic winds. Us "locals" have no choice but to live with it.

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div> ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Don't Discount Interest Rates]]>
                        </title>
                        <description>
                            <![CDATA[It's Jay Powell's world. We're just living in it.]]>
                        </description>
                        <link>https://whoisnnamdi.com/discount-rates/</link>
                        <guid isPermaLink="false">discount-rates</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Wed, 10 May 2023, 16:59:24 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512292592-header.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
Interest rates are the Federal Reserve's go-to treatment for an ailing, lethargic economy.

But the Fed's meds have a curious side effect – **they're a shot of adrenaline for the venture market.**

Low interest rates jolt the heart rate of venture capital, driving a manic frenzy of transactions and funding for startups.

On the other hand, high interest rates kill the vibe, causing deals to dwindle and prices to plummet.

You don't need a PhD to understand that. What's less obvious is exactly how large of an effect we're talking about here.

Turns out – it's huge. For a 25 basis point or 0.25% change in the one-year Treasury yield:

-   Deal activity adjusts ~10%
-   Valuations move ~25%
-   Capital invested shifts by ~30%

These effects are persistent, meaning we're always dealing with the aftermath of past interest rate shocks. It's the (highly oxygenated) air we breathe.

It's Jay Powell's world. We're just living in it.

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div>

## Interest rates are interesting

The Federal Reserve's zero interest rate policy (ZIRP) has come to an end:

![int_rate](https://nnamdi.net/content/images/2023/05/int_rate.png)

It's no secret interest rates affect tech valuations:

> During the last few years low interest rates and money printing [led to a funding bubble in private technology](https://blog.eladgil.com/p/changing-times-or-why-is-every-layoff) – Elad Gil, [Startup Decoupling & Reckoning](https://blog.eladgil.com/p/startup-decoupling-and-reckoning)

So the corporate finance logic goes, companies are worth the [present value](https://www.investopedia.com/terms/n/npv.asp) of their cash flows, and the "discount rate" one applies to those cash flows is the key input – lower rates mean higher valuations, and vice versa.

As much as investors like to ignore it, there's an obvious connection between valuations and interest rates:

![Pasted-image-20230321221512](https://nnamdi.net/content/images/2023/05/Pasted-image-20230321221512.png)  
Source: [Redpoint Ventures](https://docs.google.com/presentation/d/1Hyn4FWHSNRrWJeddi0BMEQIMlmXy2SNj50T8jJcrKbw/edit#slide=id.g11953bc14ff_2_116)

![Pasted-image-20230413194408](https://nnamdi.net/content/images/2023/05/Pasted-image-20230413194408.png)  
Source: [Bessemer Venture Partners](https://www.bvp.com/atlas/state-of-the-cloud-2023)

These charts reference publicly traded companies. However, I've never seen anyone quantify the sensitivity of _private_ tech valuations. In other words, "if interest rates fall by X percentage points, venture valuations rise by Y%," and vice versa:

-   Merely knowing Y is a positive number is only marginally helpful; **we should really want to know exactly how large Y is!**
-   Also, to what extent are tech valuations driven by interest rates and to what extent by other factors?
-   Oh, and while we're at it – is the effect instantaneous? Or does the effect take time to percolate given the inefficiencies of the venture market?

Meanwhile, interest rates could affect more than just valuations. Recall [a few essays ago](https://whoisnnamdi.com/its-valuations/) I introduced the following framework for thinking about the individual "components" of a dollar of venture capital funding – deals, valuation, and dilution:

![decomposition_excalidraw](https://nnamdi.net/content/images/2023/05/decomposition_excalidraw.png)

Interest rates could affect all these variables, and we should want to know how:

-   **Deals:** Low interest rates drive speculation, encouraging "betting" on startups. High rates send the gamblers home.
-   **Dilution:** Low rates create a more founder-friendly environment, reducing the ownership founders give up. Vice versa for high rates.
-   **Funding:** To the degree investors "search for yield," startups attract capital when other investment opportunities are scarce.

So I set out to find some answers.

A quick note before we proceed. It turns out, the Federal Reserve is mostly predictable, so most of its moves are already "priced in" by the market. I instead focus on _unexpected_ and unanticipated shifts in interest rates that market actors haven't yet reacted to. To account for these expectations, I control for other macro variables like U.S. GDP, inflation, and the Nasdaq index.

For example, if the one-year Treasury rate declines by 0.5%, the market may have only expected a 0.25% decline based on current economic conditions, leaving 0.25% unexpected:

![int_rate_excalidraw](https://nnamdi.net/content/images/2023/05/int_rate_excalidraw.png)

OK – that's the most complicated concept you need to understand. With that out of the way, let's jump to the results.

## Long and variable lags

The following charts trace the effect of a 0.25 percentage point or 25 basis point (bps) cut in the [one year U.S. Treasury yield](https://fred.stlouisfed.org/series/DGS1) on various measures of venture activity, up to twelve quarters / three years out:

-   25 bps is the typical increment the Federal Reserve uses.
-   I focus solely on the "surprise" component. The "total" change in rates would have been even larger.

The effects are quite strong, especially on deal flow and valuations:

![lp_1](https://nnamdi.net/content/images/2023/05/lp_1.png)

-   Deal activity rises for six quarters before receding back down to zero after ten quarters. Peak impact is substantial – **about 10% more deals are getting done six quarters out**
-   Valuations take longer to peak but the size of the effect is more extreme; **seven quarters out valuations are up ~25%**, falling back to their original level after three years
-   **Dilution is more muted**, remaining largely flat initially but then dipping ~5% after a few quarters without ever recovering with the three year window

Critically, interest rates affect deal flow, not just valuations:

-   It's not simply the same set of companies raising money at higher prices. **More companies get funded in the first place as well.**
-   As I found in [Old Valuations Die Hard](https://whoisnnamdi.com/old-valuations/), deal flow reacts quickly, while valuations take longer to adjust but see a larger impact overall.

This has pros and cons:

-   More deals means more entrepreneurs get to take a swing.
-   Conversely, however, when interest rates rise, founders not only accept lower valuations – _some founders don't get funded at all_.

Let's drive that point home by visualizing the same analysis but for an increase in interest rates rather than a decrease:

![lp_1n](https://nnamdi.net/content/images/2023/05/lp_1n.png)

Rising rates squeeze the life out of venture capital. Per the logic I outlined in my [last essay](https://whoisnnamdi.com/not-enough-startups/), we know this reflects reduced investor demand, since quantities and prices drift together:

> Demand can shift… which causes prices and quantities to move in the **same** direction (up when demand increases, down when demand decreases – [We Don't Have Nearly Enough Startups](https://whoisnnamdi.com/not-enough-startups/)

The (multiplicative) aggregation of these individual sub-effects yields the overall effect on venture funding:

![lp_2](https://nnamdi.net/content/images/2023/05/lp_2.png)

-   Funding builds up for almost two years, peaks at nearly 30% above baseline, then falls back to zero by about 11 quarters out.

With rising rates, funding falls just over 20%:

![lp_2n](https://nnamdi.net/content/images/2023/05/lp_2n.png)

Perhaps it's obvious, but these are extremely large effects!

Thankfully, interest rates don't move up or down enough to regularly generate these sorts of reactions. A 10 bps / 0.1% surprise is much more common than a 25 bps / 0.25% one.

Notably, the effects aren't permanent; As interest rates reset, so does the venture market.

## A brief history of time value

The current "era" of venture capital has been deeply influenced by this strange interest rate regime, having evolved entirely within it.

With our previous estimates, we can run a backwards-looking "attribution analysis", explaining the ups and downs of venture in terms of interest rates. In other words, we can break down the recent history of venture activity into the portions influenced by interest rates vs. other factors.

First up: deal activity. In red I plot an index of the overall growth in venture deal activity since early 2014, averaged across funding stages, and in blue I plot the portion attributable to unexpected interest rate shocks:

![hd_deals](https://nnamdi.net/content/images/2023/05/hd_deals.png)

-   **Interest rates explain nearly the entire explosion of venture deal activity over the last few years.**

Of all the charts, this is the one I probably spent the most time staring at and double-checking the numbers, as it's just so striking.

Let's do the same for valuations (notice the bigger scale here):

![hd_valuation](https://nnamdi.net/content/images/2023/05/hd_valuation.png)

-   The higher rates of the 2018 era pushed down valuations, but the effect reversed as rates fell heading into 2020. **Interest rates on their own doubled valuations.**
-   However, the valuation inflation was so extreme that interest rates can't explain it all, implying more had to be going on.

**This is important:** that interest rates explain nearly all deal flow but only part of the rise in valuations implies that demand for startups has outstripped supply of startups. With nowhere else to go, that excess demand spills over into prices. You can't invest in startups that don't yet exist, so you compete for the few that do, bidding up prices. This is the same conclusion I reached in a prior essay:

> … the valuation inflation we've seen "comes from" the incredible growth in demand and lack of supply of startup equity – [It's Valuations (Almost) All the Way Down](https://whoisnnamdi.com/its-valuations/)

Here's dilution:

![hd_dilution](https://nnamdi.net/content/images/2023/05/hd_dilution.png)

-   Dilution is noisier, complicating interpretation. The way to read this chart is that interest rates increased dilution through 2019 but afterward exerted downward pressure on dilution.
-   From 2019 through 2022, interest rates drove dilution down 20%, which is exactly how much they declined in total, therefore **accounting for nearly all of the fall.**

Putting it all together, here's what our little attribution methodology has to say about the effect of interest rates on overall VC funding:

![hd_capital](https://nnamdi.net/content/images/2023/05/hd_capital.png)

-   From 2014 through the top of the market in 2021, **interest rates accounted for a 200% (!!!) expansion in venture funding.**
-   As with valuations, the run up in venture funding was too extreme to be completely explained by interest rates.

## Don't discount interest rates

> Monetary actions affect economic conditions only after a lag that is both long and variable – Milton Friedman

Before concluding, I should mention some caveats to this analysis:

-   First, **there aren't many examples of big interest rate moves over this period.** I may have overfit to the few meaningful changes in rates since the 2008 Financial Crisis.
-   Second, **it's possible I haven't controlled for all the relevant variables.** For example, a lot happened during the pandemic that isn't fully captured by my choice of controls.
-   Third, **I use the U.S. Treasury yield, i.e. the risk free rate, as my measure of interest rates**, which doesn't fit a high-risk asset class like venture capital. Unfortunately, no venture-specific analog exists.
-   Fourth, and I'm repeating myself because it's important, **these are the effects of an unexpected shift in rates**, whereas most interest rate movement is expected by the market. If interest rates change by 50 basis points tomorrow, no more than half of that was in fact a "surprise."

Caveats aside, Friedman's proclamation appears to hold for venture capital – it takes multiple years for interest rate effects to fully play out.

**Even in the private markets, we're all Fed watchers now:** these interest rate effects are too large to ignore. In fact, interest rates are so impactful that they explain most of the mass hysteria of the last few years, both on the upside and the down.

Importantly, **interest rates cannot forever move in one direction.** Over the years, they mean revert, making their impact temporary at best. Don't get too accustomed to any particular regime – the new normal will always eventually look like the "old normal."

Don't discount interest rates.

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div> ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[We Don't Have Nearly Enough Startups]]>
                        </title>
                        <description>
                            <![CDATA[Where did the explosive growth in venture activity come from?]]>
                        </description>
                        <link>https://whoisnnamdi.com/not-enough-startups/</link>
                        <guid isPermaLink="false">not-enough-startups</guid>
                        <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Wed, 11 Jan 2023, 19:38:41 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512290728-header.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
Where did the explosive growth in venture activity come from?

Two possible drivers – investor exuberance and founder fervor:

-   Did investors become substantially more favorable toward private startups, and founders merely reacted to that increased interest?
-   Or, did we all become much more entrepreneurial, and investors simply provided the capital in response?

In other words, was it demand, supply, or some combination of the two?

Here's a hint: we don't have nearly enough startups.

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div>

## Narrative violation

Measured in either financing activity or equity valuations, the venture-backed startup ecosystem has grown _a lot_:

![deals_valuation](https://nnamdi.net/content/images/2023/01/deals_valuation.png)

There's a common narrative out there that the bonanza in venture-backed startups over the last decade reflects growing entrepreneurialism, the ease of starting a company, etc.

I question this story.

Only Seed rounds grew significantly faster than real GDP over the last decade. Series A's also outpaced GDP but not by much. The rest are on or even below trend, especially after the recent slowdown:  
![deals_gdp](https://nnamdi.net/content/images/2023/01/deals_gdp.png)  
_Note: Q4 2022 US GDP not yet available as of publication_

The long-run trend of real GDP reflects supply-side factors, i.e. the fundamental productive capacity of the economy. The similar trends at the later stages imply the "fundamental productive capacity" of the venture ecosystem, i.e. its ability to generate real, meaningful businesses, is growing no faster than the rest of the economy.

The U.S. economy isn't growing quickly by the way – 2% real growth year-over-year is a good benchmark. So if the trends line up, the venture world can't be growing much faster than 2% in real terms, that is, excluding the impact of [valuation inflation](https://whoisnnamdi.com/its-valuations/) that I discussed a few essays ago.

> At the later-stage, valuation inflation explains nearly _all_ growth in venture funding over the last eleven years – [It's Valuations (Almost) All the Way Down](https://whoisnnamdi.com/its-valuations/)

An equally if not more plausible story is that demand-side factors drove the explosion in venture activity over the last decade. Growing investor interest and appetite for private startups could explain much of the growth.

However, merely staring at trend lines doesn't yield much insight. It's like a tough marketing attribution exercise – the growth attributable to either demand or supply is ambiguous.

We should resolve this ambiguity:

-   As participants in this ecosystem, we should want to see it grow in real terms rather than merely nominal ones
-   In other words, we'd be much better off if venture activity reflects growing capacity and ability of entrepreneurs to start successful new businesses
-   If in fact the growth in venture activity is merely a reflection of investors needing increasingly obscure and speculative places to park their money, we have a big problem on our hands. The shell game will implode:

> Valuations can't rise forever, so over the long-run venture capital can't grow much faster than the number of ventures themselves – [It's Valuations (Almost) All the Way Down](https://whoisnnamdi.com/its-valuations/)

## Economics 101: Supply and Demand

First, let's quickly review Economics 101.

Here's the classic supply and demand chart. Supply and demand together determine equilibrium prices (Y-axis) and quantities (X-axis). Importantly, demand is downward sloping (you want more stuff the cheaper it is) and supply is upward sloping (you produce more stuff when you can sell it for more):  
![supply_demand](https://nnamdi.net/content/images/2023/01/supply_demand.png)

In this model, two forces can move markets. Demand can shift (for reasons that don't have to do with prices themselves), which causes prices and quantities to move in the **same** direction (up when demand increases, down when demand decreases):  
![supply_demand2](https://nnamdi.net/content/images/2023/01/supply_demand2.png)

Or, supply can change (again, for reasons other than price), in which case prices and quantities move in **opposite** directions (price move _against_ supply, quantities move _with_ supply):  
![supply_demand3](https://nnamdi.net/content/images/2023/01/supply_demand3.png)

In simple terms, when everyone wants the same thing and wants it _really badly_, the price tends to go up and more of that item gets bought and sold. When everyone wants to produce and sell the same thing and wants to do so _really badly_, they compete against one another, driving prices down to accommodate the increased activity.

In venture terms, when investor demand for startups rises, valuations increase and more deals get inked. When entrepreneurial supply expands, valuations decline and more deals get done.

## VC is trendy

Venture deals and valuations have both grown over time, so it's all demand-driven right?  
![deals_valuation2](https://nnamdi.net/content/images/2023/01/deals_valuation2.png)

Not so fast.

While it's tempting to end the analysis here, for reasons I'll skip over, you can't simply compare two trending metrics and assume they're correlated. The relationship could easily be [spurious](https://www.tylervigen.com/spurious-correlations), and we don't want that. By the magical transitive property, anything we concluded based on that relationship would be spurious as well. Cool math.

The key is to "de-trend" the data first. We need to remove the long-run trend and then compare _deviations_ from that trend.

Ignoring the details, here's what that looks like (data normalized and smoothed slightly to remove noise):  
![deals_valuation3](https://nnamdi.net/content/images/2023/01/deals_valuation3.png)

_Lo' and behold:_ deal flow and valuations fluctuate together around their respective trends.

OK, maybe that's not _always_ true, but it's nearly always true. Early stage is the exception to the rule.

Seed financings were negatively correlated with valuations before roughly 2017:

-   When deal flow increased, valuations plummeted, and vice versa
-   **This smells like supply to me** – there was only so much investor demand, so when more startups came to market, they competed, and prices tended to fall, benefitting investors. When the supply of startups contracted, prices rose as investor battled over the few remaining deals

Series A rounds used to be positively linked to valuations, but they've de-correlated over the last few years:

-   Pre-2019, Series A deal flow and valuations tended to move in the same direction
-   **Sounds like demand to me** – startup supply was constrained, so investor sentiment drove the market, moving prices up as they grew more eager and down as they soured on the venture ecosystem

Across growth and later stages, deal flow and valuations are unambiguously positively related, moving almost in perfect unison for the last decade:

-   **Demand clearly wins it** – late stage supply is badly constrained, so investor demand is the prime mover. Their manic and depressive episodes move the market accordingly

Some of these relationships have shifted over time, so let's visualize that with rolling three-year correlations for each stage:  
![deals_valuation4](https://nnamdi.net/content/images/2023/01/deals_valuation4.png)

This is my qualitative narrative in quantitative terms:

-   The Seed stage flipped being negatively correlated to positively (supply → demand)
-   Series A de-correlated to effectively zero (demand → ❓)
-   Series B and later deal flow and valuations have always been strongly positively related (demand all day baby)

## Channel attribution

This is the best evidence I've seen to date for the demand-side hypothesis.

The scorecard so far suggests demand reigns:

-   Investors are the primary driver of fluctuations in venture activity and equity prices around their long-run trend
-   Yes, more deals are getting done (so by definition more startups are getting funded), but that appears to be a function of increasingly desperate investors rather than increasingly bold and enabled founders

In other words, the supply of startup equity is badly constrained:

> The venture ecosystem is supply-constrained – there isn't nearly enough startup equity out there to satisfy investor demand.
> 
> Additional capital drives opportunistic company formation at the Seed stage. However, the additional capital doesn't improve survival to the later stages – it simply drives prices up for the remaining companies – [It's Valuations (Almost) All the Way Down](https://whoisnnamdi.com/its-valuations/)

As a reminder, our evidence for this is the positive link between de-trended deal activity and valuations. That's a nice trick, but it only tells us at each point in time whether movements in demand or supply dominated. I want to explain the _entire last decade or so_ of venture history in terms of supply and demand channels.

Yes, yes, history is not bi-causal, hammer looking for a nail, etc, but how about one more magic trick to close things out?

_**Warning: armchair econometrics ahead!**_

Let's stretch our simple supply and demand model to the absolute extreme:

-   Remember, a positive relationship between deal flow and valuations suggests a change in demand, while a negative or inverse relationship suggests shifting supply
-   So, we could simply attribute each quarter of venture activity to either demand or supply demand based on whether deal flow and valuations move in similar or opposing directions during the quarter
-   We can then cumulate the respective contributions of demand and supply to the growth of the venture ecosystem over time

Here's what that looks like for venture deal flow since early 2010. Demand is red, supply is blue, sugar is sweet, and so are you:  
![deals_supply](https://nnamdi.net/content/images/2023/01/deals_supply.png)

That's a lot of red out there:

-   The demand channel drove most of the growth in venture activity in nearly every stage other than Seed, where its relative contribution is closer to 50/50
-   Early stage supply contributed positively to deal flow from 2010 to about 2015 but then stagnated
-   Supply has never been a meaningful contributor at the growth and late stage and even seems to have _contracted_ in certain cases

Et tu, valuations?

![deals_supply2](https://nnamdi.net/content/images/2023/01/deals_supply2.png)

The story isn't much different for valuations, except perhaps with some signs flipped:

-   Here again, investor demand was the main driver, pushing prices higher in every stage
-   The increase in early stage supply in the early 2010s relieved some price pressure, but this eventually receded
-   The effect of late stage supply on valuations is somewhat noisy, but by the end of the sample those supply constraints appear to have driven prices higher, on balance

In case I haven't sufficiently caveated already: _this is extremely unscientific._ No Nobels will be awarded for this work (your subscription is enough reward for me, awwww), but it does serve as coarse, suggestive evidence that demand is, or at least has been, king in venture over the last decade.

## There's only so many startups to go around

So, what have we learned?

To the degree founders are starting more venture-backable companies, it's largely driven by the cold, rational calculus that investors are much more eager to buy up equity in private companies today than they used to be.

This explains much of the growth in the venture ecosystem over the last decade: more deals get done and those deals are more expensive because of surging investor demand. This has been great for founders.

However, this will be painful for investors, since [companies rarely grow into their valuations](https://whoisnnamdi.com/grow-valuation/):

> Most often, companies don't grow fast enough to compensate for rising valuation multiples. Instead, high valuations today imply slower value appreciation in the future, i.e. lower returns.  
> …  
> Investors like to think companies will grow into their valuations, but more often than not, the stock simply underperforms – [Companies Rarely Grow Into Their Valuations](https://whoisnnamdi.com/grow-valuation/)

Meanwhile, the supply of startup equity remains constrained. Rather than potential founders becoming more eager to start companies for "fundamental" reasons, entrepreneurs are reacting to investor sentiment. While there's been some growth in supply at the earliest stages, the fundamentals haven't necessarily improved much, which is why late stage deal flow hasn't grown any faster than U.S. GDP.

I'll repeat what I said earlier – **we'll all be much better off if more people start companies for good, wholesome reasons that don't have anything to do with valuations.**

There was a time where the notion of handing millions of dollars to an extremely young company sounded crazy, and anyone willing to do so extracted a significant ownership stake for taking on that risk. Founders got diluted, badly.

We're well past that now. [Dilution has fallen in every stage since 2010:](https://whoisnnamdi.com/its-valuations/)  
![dilution](https://nnamdi.net/content/images/2023/01/dilution.png)

We have more startups, but:

-   It's largely been a reaction to attractive valuations and reduced dilution that founders must endure to raise capital
-   Past the seed stage, it's not at all obvious that founders are fundamentally better equipped to build successful companies today vs. a decade ago

The demand side got a bit ahead of itself. It's time for the supply side to catch up.

We don't have _nearly_ enough startups.

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div> ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Old Valuations Die Hard]]>
                        </title>
                        <description>
                            <![CDATA[Private valuations substantially lag public tech valuations]]>
                        </description>
                        <link>https://whoisnnamdi.com/old-valuations/</link>
                        <guid isPermaLink="false">old-valuations</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Tue, 13 Dec 2022, 17:47:55 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512288499-header-6.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
Public technology valuations have crashed.

A worsening economic outlook and tight monetary policy sent tech stocks through the floor.

However, private valuations are slower to adjust.

Private valuations lag public valuations, often by a substantial amount and for a long time.

As we all lick our wounds during this venture downturn, many are asking: when will it all end?

The answer? **About three years.**

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div>

## Public-private partnership

Private valuations rose dramatically over the last decade. But so did the Nasdaq:

![nasdaq_valuation](https://nnamdi.net/content/images/2022/12/nasdaq_valuation.png)

It's remarkable how well the two track one another.

In a sense, the growth of public tech valuations seems to pin down the rise of private valuations, defining a trend to which private valuations always tend to return. It's clear that venture investors use public valuations to justify private ones.

One feature immediately stands out – private valuations are much more volatile than public valuations, particularly at the later stages:

-   This is to be expected – it's not the same companies that are fundraising each quarter, so we'd naturally expect the prices to jump around a bit
-   Meanwhile, the composition of the Nasdaq doesn't change much quarter to quarter, so as an aggregate it's much more stable

If we throw each one on its own axis, the relationship is even more obvious:

![nasdaq_valuation2](https://nnamdi.net/content/images/2022/12/nasdaq_valuation2.png)

Across stages, private valuations move one-for-one with public valuations.

-   Seed valuations have grown somewhat slower than the Nasdaq, moving 0.5% for every 1% move in the tech index
-   Series A, B, and C valuations have moved one-for-one
-   Series D+ valuations have grown 1.3% each point of growth in the Nasdaq

We can replicate the same analysis for private venture activity. The relationship is not as tight:

![nasdaq_deals](https://nnamdi.net/content/images/2022/12/nasdaq_deals.png)

Deal activity is more or less related to public tech valuations, depending on stage. Here too we see much more volatility in the private realm than the public, again, for understandable reasons.

![nasdaq_deals2](https://nnamdi.net/content/images/2022/12/nasdaq_deals2.png)

In general the slopes here are flatter than for valuations:

-   Seed deals have grown at roughly the same pace as public valuations
-   For other stages, growth has lagged public tech valuations – growing at about half the pace

## Level with me

Though intuitive, these comparisons are not rigorous. It's too easy to find spurious relationships among variables that are all trending in the same direction. There's less risk of that in our case since public and private technology markets are closely linked, but we should be wary regardless.

We can do better by focusing less on the relationship between _level_ of the Nasdaq and some other variable and more on the correlation between _changes_ in the respective metrics.

**The key question:** how does a change (up or down) in public technology valuations affect the prices of private financings, and how quickly?

Some good ol' regression analysis can answer this.

Glossing over a bunch of detail:

-   We can forecast future changes in private valuations and financings based on current changes in public tech valuations.
-   We'll run one regression for each forecast horizon: concurrent (time zero) impact, one quarter out, two quarters out, etc.
-   The coefficients of these regressions trace out the impact of the original movement in the Nasdaq on private prices and volume in later periods.

## The path to recovery

Hope you're still with me. Let's run those regressions and see what we get.

We'll focus on downward movements in the Nasdaq. This is how a 1% decline in the Nasdaq affects private valuations and deal flow over future quarters:

![valuations_deals-1](https://nnamdi.net/content/images/2022/12/valuations_deals-1.png)

Private valuations gradually decline after a drop in the Nasdaq:

-   **Prices drop continuously for four quarters**
-   At the trough, private valuations fall ~2.25% lower for every 1% loss in the Nasdaq
-   **Venture valuations take 10 quarters to recover**

Movements in the Nasdaq reliably forecast venture deal activity too, which drops and rebounds faster than prices:

-   Deal activity bottoms in the third quarter after impact
-   Overall deal count rises back to its original level by the 8th quarter

## Down for a down round?

This masks a lot of underlying variance between the different segments of the venture ecosystem. Starting with valuations, let's explore how the shape of the recovery varies by stage:

![nasdaq_valuation3-2](https://nnamdi.net/content/images/2022/12/nasdaq_valuation3-2.png)

**Valuation dynamics depend on stage:** Early stage valuations drop 1-1.5% for every 1% decline in the Nasdaq, while growth and late stage valuations decline 2-2.25%.

**Note: these aren't exact estimates.** Uncertainty grows as we look further out from initial impact, so don't pay much attention to the numbers at the 12-quarter mark. It's the shape and length of the recovery that matters most.

With some back-of-the-envelope math, we can estimate both the magnitude and the length of the current downturn:

-   The peak to trough decline in the Nasdaq this past year was about 25%, **implying a ~25-35% haircut to early stage valuations and a ~50-55% cut in growth and later stage valuations**
-   That might sound extreme, but the sudden explosion of late stage valuations these past few years was itself quite unusual. It's not implausible that prices could fall as dramatically as they rose

In fact, the correction is already well underway:

![nasdaq_valuation4](https://nnamdi.net/content/images/2022/12/nasdaq_valuation4.png)

As predicted, late stage valuations drop much more than the Nasdaq, while early stage valuations move roughly in line. Prices have fallen faster than my forecast, but the magnitudes are on point.

Timing the bottom based on my regression here is a bit tricky, as the market began its decline in Q1 2022 and mostly ran its course by Q3 2022. If we use the midpoint of Q2 2022 as the "start" of the venture recession:

-   **Private valuations should bottom in Q2 2023**
-   I'm hesitant to forecast the heydays of 2021 ever returning, but if they did, it'd happen around Q4 2024. _I wouldn't hold your breath though_

## Coming to terms

Again, let's focus on a 1% decline in the Nasdaq:

![nasdaq_deals3](https://nnamdi.net/content/images/2022/12/nasdaq_deals3.png)

Financings follow an interesting "sine wave" recovery in the aftermath of a plunge in the Nasdaq. Again we see more severe impact in the later stages: The quarterly volume of Seeds and As drops by 0.75-1% for each 1% decline in the Nasdaq. Late stage deals sink 2%.

One ray of hope – financing activity seems to come back _stronger_ after a venture recession.

Pulling out that scribbled envelope again, these numbers suggest **early stage deal activity will drop 20-25%; growth and late stage deal volumes should slide about 50%.**

Again, the data bears this out:

![nasdaq_deals4](https://nnamdi.net/content/images/2022/12/nasdaq_deals4.png)

Remember, the venture deals should only take about a year to hit their low, so activity should stabilize in or around Q1 2023, notably ahead of valuations.

As I've said, these are rough estimates. All the typical caveats apply: "this time could be different," etc.

## Conclusion

The bubble burst, but the pain won't last forever.

The historical relationship between the public and private tech markets lets us trace out the likely path of the venture recovery.

The first year is the hardest. Prices drop precipitously, in many cases, much further than their public tech comparables.

Things get better from there, with deal activity back on track after another year or so. Prices, however, remain depressed a little while longer.

All-in-all, it's a three year odyssey from start to finish.

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div> ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[It's Valuations (Almost) All the Way Down]]>
                        </title>
                        <description>
                            <![CDATA[Venture funding hasn't grown as much as you think]]>
                        </description>
                        <link>https://whoisnnamdi.com/its-valuations/</link>
                        <guid isPermaLink="false">its-valuations</guid>
                        <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Tue, 1 Nov 2022, 15:48:34 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512285493-decomp_g-1.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
Venture funding hasn't grown as much as you think.

Soaring valuations over the last decade inflated financing volumes.

As a result, "real", price-adjusted funding growth looks quite different from unadjusted growth, similar to traditional economic measures like GDP.

Adjusting for rising valuations, real venture funding at the early stage is only growing at _half_ the unadjusted pace.

At the later stages, valuation-adjusted venture funding isn't growing \*at all\*.

It's valuations (almost) all the way down.

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div>

## Pre-Algebra

First, a quick math refresher:

-   Total funding volume equals the number of financing rounds multiplied by the average round size
-   Assuming a standard equity round, average round size further separates into valuation multiplied by dilution (the proportion of shares sold to investors)

That gives us three components – deals, valuations, and dilution:

![SCR-20221031-wdu](https://nnamdi.net/content/images/2022/11/SCR-20221031-wdu.png)

This framework is simple but powerful.

With it, we can explain the individual sources of growth in the venture funding ecosystem, cumulating into overall funding growth.

## Algebra II

Let's walk through each component one-by-one.

I'll plot the growth over time of each component since 2010, leveraging data from Pitchbook for all venture deals completed in Western markets through 2021.

Importantly, I'll plot the cumulative _log growth_:

-   This makes the components additive rather than multiplicative, which will help the analysis later
-   This also means you can read compound annual growth rates (CAGRs) off of the y-axis. Just take the value on the y-axis and divide by 11, which is the number of years this analysis covers

Let's start with growth in funding volume by stage:

![capital](https://nnamdi.net/content/images/2022/11/capital.png)

-   Across stages, annual venture funding grew by 2.5 to 3 log points, with CAGRs ranging from 23-27% CAGR (again, just divide the log growth by 11)
-   The earliest (Seed) and latest (Series D+) stages grew fastest

Next, let's dive into each of the components.

Deal counts have ballooned across all stages, but the Seed stage stands out:

![deals](https://nnamdi.net/content/images/2022/11/deals.png)

-   Seed deals grew the most by far over this period, 2.1 log points or 19% on an annual basis
-   On the other hand, Series C deals and later only grew ~0.75 log points, a 7% CAGR
-   As and Bs were somewhere in the middle, growing 10% annually over this time

It should be noted: that Seed deals grew so much faster than the rest implies the "graduation" or "survival" rate of startups fell materially, as they've had more than enough time to mature.

**More early-stage startups has not led to many more late-stage startups.**

We'll revisit this.

Next up, valuations:

![valuations](https://nnamdi.net/content/images/2022/11/valuations.png)

-   Unlike deal counts, which grew the most at the early stage, valuations grew the most _at the later stages_
-   Series C valuations grew at a ~20% CAGR, while Series D+ grew at ~24%
-   Series As and Bs again formed the middle of the pack, growing 1.75 log points or 16% year-over-year
-   Seed valuations "only" grew 10% annually, which is still exceptional if you think about it

Lastly, let's look at dilution. Note again these numbers are in log points (_not_ percentage points):

![dilution](https://nnamdi.net/content/images/2022/11/dilution.png)

-   Dilution has fallen in every stage since 2010
-   Dilution at the Series A and C has fallen the most, while dilution at the Series B has fallen the least
-   Dilution evolved similarly across stages through 2019, but for whatever reason, VCs over the last few years were much less desperate to "get their ownership" in Series A, C, and D+ rounds

Note that falling dilution _negatively_ impacts funding volume since less equity gets sold.

## Multivariable calculus

And now, the main event.

For every financing stage, let's aggregate the change in deals, valuations, and dilution to explain the cumulative growth of funding:

![decomp_g](https://nnamdi.net/content/images/2022/11/decomp_g.png)

-   At the Seed stage, most funding growth came from more deals getting done. Valuations rose too, but deal count explains most of the overall funding growth

This seems healthy – growth in funding should ideally come from growth in the total number of deals.

-   It's a completely different story at every other stage: _valuations explain most of the growth in funding at Series A and later._
-   **At the later-stage, valuation inflation explains nearly \*all\* growth in venture funding over the last eleven years**

In contrast to the Seed stage, this seems unhealthy: growth in the majority of the venture ecosystem (at least as measured in dollars) over the last decade was primarily driven by rising valuations.

Now, if you've spent a lot of time staring at data like I have, you'll know that these sorts of analyses can be very dependent on the starting year (in this case, 2010).

Just to be safe, we can avoid privileging any particular year as the starting point by instead calculating the variance in funding levels over time rather than the growth. Variance decomposes the same way as growth, but this time there won't be a "base year" affecting the results.

Do this, and the conclusion is the same – year-to-year variation in valuations accounts for most of the variance in funding volume over time at the later stages:

![decomp_v](https://nnamdi.net/content/images/2022/11/decomp_v.png)

-   At the seed stage, two-thirds of funding variance over time is accounted for by deal volume and one-third by valuations
-   At Series A and B, 65-70% of funding variance is explained by valuations
-   **At Series C and later, valuations account for more than 80% of the annual variance in venture capital funding**

In economics, when quantities and prices rise together, you can be fairly sure it's driven by growth of demand, or equivalently, you know the market in question is supply constrained:

-   The supply of startup equity at the later stages is constrained; there's only so much of it to go around
-   On the other hand, investor demand for venture assets has exploded, especially at the later stages where companies have been significantly de-risked

In this sense, the valuation inflation we've seen "comes from" the incredible growth in demand and lack of supply of startup equity.

## Real analysis

Here's another trick – if we go back to our decomposition and divide both sides by valuation, we get the following:

![SCR-20221031-wg9](https://nnamdi.net/content/images/2022/11/SCR-20221031-wg9.png)

In other words, we can adjust funding volume by the average valuation at which these financings occurred to get a "valuation-adjusted" funding metric. This is analogous to how economists "deflate" nominal GDP by inflation (e.g. [CPI](https://www.bls.gov/cpi/) or similar) to arrive at "real" GDP.

It's also the result of multiplying deal count each year by average dilution, i.e. the total equity bought and sold in the venture market each year.

This lets us measure "real" growth in venture funding rather than dollar-based growth, which conflates deal activity with valuation movements.

Compare this "real" funding metric to the "nominal" one I showed earlier:

![adj](https://nnamdi.net/content/images/2022/11/adj.png)

Wow.

Real venture funding is growing much, much slower than you think:

-   At the Seed stage, real funding growth has a 17% CAGR vs. 27% for raw funding dollars
-   At the Series A and B, real funding grew at a 7% CAGR vs. 23% for nominal funding, one-third the rate
-   **Real funding was flat for a decade at the later stages** before finally ticking up in the heydays of 2021. Real annual growth is only 2% and 4% at the Series C and D+ respectively, vs. 23 and 27% without adjustment

I want to state this as directly as possible: **there was no growth in real late-stage funding activity for a decade until the bonanza of 2021.**

Said differently, no more late-stage equity traded hands in 2020 than in 2010, measured in terms of points of cap table ownership.

## Now you're ready for Economics 101

This data contradicts the common narratives of the past decade of growth in the venture ecosystem.

Venture capital is about two things, ventures and capital.

Capital has been in excess supply the last decade, but ventures haven't.

Valuations can't rise forever, so over the long-run venture capital can't grow much faster than the number of ventures themselves.

In a way it all makes sense:

-   By the time startups graduate to the later stages, they are real companies, typically with meaningful revenue, operations, and headcount
-   Accordingly, their total count _should_ grow roughly in line with the real growth rate of modern, industrialized economies. We wouldn't expect an order of magnitude difference, otherwise late-stage startups would overtake the whole economy

In other ways, it's highly concerning:

-   **The supply of late-stage startup equity hasn't risen to match investor demand**
-   Perhaps this explains why late-stage funding seems so anemic right now - activity has reverted back to the zero-growth trend, which feels quite slow relative to the flurry of 2021 deal-making

The venture ecosystem is supply-constrained – there isn't nearly enough startup equity out there to satisfy investor demand.

Additional capital drives opportunistic company formation at the Seed stage. However, the additional capital doesn't improve _survival_ to the later stages – it simply drives prices up for the remaining companies.

I've long felt this but never had the data to back it up.

Now I do, and I think it's a big problem.

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div> ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Beats and Misses Are Forever]]>
                        </title>
                        <description>
                            <![CDATA[Revenue surprises permanently shift the trajectory of SaaS companies]]>
                        </description>
                        <link>https://whoisnnamdi.com/forever/</link>
                        <guid isPermaLink="false">forever</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Wed, 14 Sept 2022, 09:29:07 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512283659-forever.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
Subscription revenue is a double-edged sword:

-   It's predictable, making the business much easier to forecast
-   It's persistent – over or underperformance today reverberates far into the future

Contrary to popular belief, SaaS companies do not in fact "pull forward revenue." A beat today puts the company on a permanently higher trajectory. Revenue doesn't mean revert later.

Likewise, deals might "slip," but there's no "catch up growth" – a missed quarter doesn't get made up for next quarter. A miss today predicts lower revenue for years to come.

[Diamonds](https://youtu.be/92FCRmggNqQ?t=56), beats, and misses are forever 💎.

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div>

## Long vs. short

Public market investors are often criticized for being too "short-term focused."

Politicians and CEOs alike love to hate on investors for overreacting to current business results, bidding up the stock or kicking off a fire sale depending on how quarterly earnings play out

> A widely-held view among Washington policymakers, corporate executives, the media, and the public is that frenzied, short-term stock market trading has coupled with Wall Street’s unquenchable thirst for immediate results to disrupt US firms and badly hurt the economy – [How Big Is the Problem of Stock-Market-Driven Short-Termism?](https://www.americanbar.org/groups/business_law/publications/blt/2022/05/short-termism/)

Baked into this criticism is a view that the long-run is hard to influence – fluctuations in the short-run tend to mean revert over time. One's view of a company should change only slowly and in the face of extraordinary evidence.

In defiance of this heavy-handed finger-wagging, there's an alternate view that short-run performance matters and is informative _about the long-run_. The future is bound to the present, and we should decisively update our forecasts in light of new information.

In summary:

-   Long-termists believe revenue will eventually revert back to some long-run trend based on the fundamentals of the business, its market opportunity, etc. Short-term fluctuations are noise and should be ignored. Quarterly beats and misses should have little, if any, impact on fundamental valuation analysis.
-   Short-termists (though they would never refer to themselves as such) believe the present moment matters and says much about the future. Short-term performance is a valuable signal that should factor into valuations. Mean revision dominates mean reversion.

Who is right? And how would we know?

## A random or deterministic walk down Wall Street?

There's a simple way to resolve this dispute. We only need to rephrase the positions of the long/short-termists, and the resolution will be clear.

Long-termists think today's revenue fluctuations poorly predict future revenue. They don't often phrase it that way, but that's the core underlying logic behind the belief in a stable, long-run trend.

This has multiple important implications.

For one – revenue can be "pulled forward," but this will tend not to influence the long-run trajectory of the company:

> Adobe was downgraded to neutral from buy at UBS... Analyst Karl Keirstead said after speaking with 14 large enterprise IT executives and services partners of Adobe, he's worried that spending was pulled forward in 2020 and 2021, which will pressure its growth rate this year. ([Link](https://markets.businessinsider.com/news/stocks/adobe-downgraded-to-neutral-at-ubs-on-concerns-that-spending-was-pulled-forward-10896540))

Further, a missed quarter today can be made up for next quarter. Missing the target leads to revenue in the future quarters to be slightly higher, as revenue reverts back to trend. You could call this "catch up growth."

Meanwhile, short-termists think today's revenue movements predict future revenue. One's long-term revenue forecast should be quite sensitive to beats and misses in the present day.

So, pulling forward revenue doesn't come at the cost of future revenue. Similarly, missing your target today doesn't mean you're any more likely to hit tomorrow's target. In fact, you're less likely, since you're now on a lower revenue path.

The clean and simple test?

Simply run a regression of future revenue on current over/underperformance! The coefficient tells us to what extent our estimates of future revenue should shift in response to strong or weak results today:

-   A coefficient near or greater than one tells us future revenue rises/falls by at least $1 for a $1 beat/miss today (the short-termists win)
-   A coefficient well below one means forward revenue is insensitive to today's performance and tends to mean revert (the long-termists win)

## Back to the future

The chart below plots revenue growth over four quarters against revenue growth today for 35 public software companies, demeaned by the respective average rate for each company. Positive values mean revenue came in higher than normal, and vice versa:

![future](https://nnamdi.net/content/images/2022/09/future.png)

**Future and current revenue over/underperformance are positively related.** The coefficient is 1.16, implying **a $1 beat today forecasts a $1.16 beat four quarters from now.** The exact reverse is true of a miss – a dollar of missed revenue this quarter lowers our expected revenue in four quarters by $1.16.

Additionally, the R^2 is decently high – 0.49, so about half of the variation in expected revenue four quarters out can be explained by revenue performance in the current quarter.

One quick aside since I know what certain folks are thinking here: I am not merely saying that current revenue predicts future revenue. I'm saying _changes in revenue_ predict changes in future revenue.

So far, the short-termists seem to be winning. But one year is not a long-time – perhaps mean reversion takes longer? Let's check by extending the analysis to eight and twelve quarters out. That's too crowded for a single scatterplot, so I'll summarize the results and only show the coefficients at each horizon:

![forever-3](https://nnamdi.net/content/images/2022/09/forever-3.png)

If anything, the case for short-termism only gets stronger. The two-year revenue forecast shifts by $1.25 for a $1 beat/miss today. The three-year forecast changes by $1.43.

Even three years out, we see no evidence of mean-reversion among the typical public software company. Beats and misses permanently shift the trajectory of the company.

Now, dollars are a nice unit of account, but they're admittedly hard to contextualize. A $1 beat/miss that turns into a $1.43 beat/miss three years out could be more or less meaningful depending on the scale of the company and its growth rate. Many of these companies were likely to be much larger in three years anyway.

Let's do the same analysis with percentages instead. What impact does a 1% beat/miss have on future revenue, also in percentage terms?

![persists](https://nnamdi.net/content/images/2022/09/persists.png)

Persistence persists, but this tells a slightly different story. Revenue three years from now is still 0.85% higher/lower than it would have otherwise been.

My interpretation: the additional (or lost) revenue from a beat (or miss) grows at a somewhat slower pace than the remaining revenue base, so we see some convergence. In other words, "surprise" revenue doesn't grow as fast as "expected" revenue.

You could frame this as slight mean reversion. Personally, I'd say the short-termists still have it.

## Working as intended

Why does this happen? Why is the future so sensitive to the present?

Rather than a surprising phenomenon, I see this as **the defining characteristic of subscription business models.**

To say revenue "recurs" is merely to say revenue today generates revenue tomorrow. Said differently, a good test of subscription revenue quality is **the degree to which it persists and predicts future revenue**.

In that light, these results are expected. If a change in revenue today didn't predict a change in revenue tomorrow, it'd be hard to call it recurring.

For some intuition, look at those [cohort revenue charts](https://thetaclv.com/resource/c3/) that have become so popular among public software companies (at least among those with good retention dynamics to show off):

![Pasted-image-20220910172026](https://nnamdi.net/content/images/2022/09/Pasted-image-20220910172026.png)

Missing a quarter means losing a slice of the cohort stack. Assuming positive net dollar retention, that slice would have grown over time; the opportunity cost of weak performance grows over time.

Ironically, a miss for a company with high retention hurts more than one for a company with low net retention, since the high retention company has more (future revenue) to lose. I examined this phenomenon from a slightly different angle a few years back:

> Both in theory and in practice, **better retention drives higher volatility** – ["High Retention = High Volatility"](https://whoisnnamdi.com/high-retention-high-volatility/)

**This is why SaaS beats and misses are so consequential.**

If investors think a miss or beat is likely to stick, that will meaningfully impact their valuation views, as it must. On the other hand, the stock shouldn't move much if short-term performance reflects merely temporary dynamics in a company's go-to-market engine.

That SaaS valuations tend to react so strongly implies investors do believe these GTM gyrations are permanent and must be reflected in future projections. It's possible there's some mean reversion happening under the hood, but this effect is totally swamped by the magnitude and persistence of the beat/miss itself.

## Revision of the mean

A bad reaction to this analysis would be: your sales leader says the quarter came in light because some deals slipped, you look them in the eyes and confidently tell them "there's no such thing as a deal slipping," and point them to this essay.

**No.** That's not what I'm saying at all. (Please do send them this essay though!)

At the level of individual sales, deals slip from one quarter to another all the time. But at the level of aggregate revenue that doesn't seem to matter. Revenue won't be higher next quarter simply because revenue came in low this quarter. Every quarter is, for the most part, a blank slate.

Likewise with pulling forward revenue – it happens, but on average you can't find it in the data. COVID is a good exception to this – companies that benefitted from the transition to remote work like Zoom or DocuSign are now slowing, reverting back to their long-run trend line:

> While DocuSign beat revenue expectations last quarter, full-year guidance came in far lower than expected. DocuSign… was perceived to be a "COVID winner." There are now fears that the company merely pulled forward years of sales over the course of the past two years, and that its revenue growth trajectory will be lower going forward. ([Link](https://www.theglobeandmail.com/investing/markets/stocks/APPN-Q/pressreleases/7587079/why-snowflake-appian-and-twilio-plunged-today-again/))

> While Coupa's upside certainly isn't at risk long term, it does appear some of its future growth was pulled forward during the COVID-19 crisis. ([Link](https://www.nasdaq.com/articles/down-over-30-is-coupa-software-stock-a-buy-2021-03-27))

> Unfortunately for Twilio, it belongs to the “high beta growth” club, which began a secular decline early last year. The San Francisco-based Cloud Communications company boomed from the work-from-home trend... But, like so many other tech companies, it pulled forward too many gains, setting itself up for a nasty re-pricing. ([Link](https://finance.yahoo.com/news/trade-twilio-stock-wednesday-earnings-175540896.html))

**Note:** this analysis only includes software companies. Businesses with more transactional business models probably see less persistence and more mean reversion. Think about companies like Peloton:

-   Despite having a subscription service, the vast majority of revenue was in physical bike sales that are one-time in nature
-   Peloton really did pull forward demand, which left little market left to grow into by the time things returned to normal

However, if you have significant recurring revenue, mean reversion is much less relevant.

This analysis continues a line of thinking I touched on in a [prior piece](https://whoisnnamdi.com/covid-hurt-software/) exploring COVID's impact on software companies:

> … mean reversion is a strong force, but **COVID was stronger**, especially on the downside. Once knocked down, the typical software business never got back up…
> 
> _Mean revision_ is at least as important as mean reversion. Not only are software companies below their pre-COVID trend, _the trend itself_ has changed for the worse – [COVID Hurt Most Software Companies](https://whoisnnamdi.com/covid-hurt-software/)

For software companies, every day is [Day 1](https://s2.q4cdn.com/299287126/files/doc_financials/annual/Shareholderletter97.pdf) or, at least, dependent on it.

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div> ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Layoffs Don't Tell the Whole Story]]>
                        </title>
                        <description>
                            <![CDATA[Hiring freezes matter more than layoffs]]>
                        </description>
                        <link>https://whoisnnamdi.com/layoffs/</link>
                        <guid isPermaLink="false">layoffs</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Wed, 31 Aug 2022, 11:00:43 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512280834-exit.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
Layoffs grab headlines. Hiring freezes, less so.

Sites like [layoffs.fyi](https://layoffs.fyi/) track layoffs in gory detail, as do journalists and tech writers:

> Large layoff rounds in tech, sadly, are still on.  
>   
> Olive AI let go ~30% of staff. They were one of the biggest unicorns in the Midwest (valued $4B)  
>   
> Capsule (digital pharmacy startup, valued >$1B) had large layoffs. I'm still not sure of the exact % here (feel free to DM)
> 
> — Gergely Orosz (@GergelyOrosz) [July 20, 2022](https://twitter.com/GergelyOrosz/status/1549765632790601729?ref_src=twsrc%5Etfw)

However, it's slow hiring, not layoffs, that most drives unemployment.

Layoffs don't tell the whole story.

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div>

## Stocking up

Employment and unemployment are both "stocks".

Stocks are quantities measured at a point in time that change over time. For example:

-   The volume of water in a lake
-   The stock (hint hint) of inventory in a retail store
-   Cash on a company's balance sheet

On the other hand, "flows" are rates of change of a particular stock. For example:

-   Water from various rivers flowing (hint hint) into a lake
-   Shipments of new inventory
-   A startup's monthly burn rate

A single stock can have multiple associated flows, and the sum total of these flows determines how the stock changes over time.

In the labor market, workers flow between employment and unemployment (stocks) at various rates (flows):  
![SCR-20220831-4nv](https://nnamdi.net/content/images/2022/08/SCR-20220831-4nv.png)

In equilibrium, the various stocks of a closed system are stable. However, if the individual flows change, the related stocks change too:

-   During recessions, flows from employment to unemployment (job separation) increase, while the flows out of unemployment into employment (job finding) decrease
-   Vice versa for recoveries – people find jobs more quickly and fewer people get laid off in the first place

In this way, variation in job separation and job finding "explain" changes in unemployment.

## As the labor market cools, hiring freezes

We commonly associate unemployment with layoffs. During recessions, we imagine heartless employers suddenly laying off droves of workers:  
![CleanShot-2022-08-18-at-12.54.17@2x](https://nnamdi.net/content/images/2022/08/CleanShot-2022-08-18-at-12.54.17@2x.png)

While this story feels intuitive, the data doesn't bear this out.

The below chart plots the U.S. "employment exit rate" (a euphemism for layoffs or voluntary quitting that ends in unemployment) along with the unemployment rate itself. Think about the exit rate as the share of all employed workers who leave employment each quarter:  
![exit-1](https://nnamdi.net/content/images/2022/08/exit-1.png)

Employment exits jiggle up and down at a much higher frequency than unemployment, and the timing of those short term fluctuations doesn't align well with granular movements in unemployment. Over longer periods though, they do seem loosely connected.

Now let's look at the other piece of the puzzle. Here, I've swapped out employment exit for the "job finding rate," i.e. the proportion of unemployed workers who find a job each quarter:  
![job](https://nnamdi.net/content/images/2022/08/job.png)

**The lines perfectly mirror each other.**

The fluctuations closely track the state of the economy. When job finding plummets, unemployment rises at the same time. As the economist Robert Shimer put it:

> … a decline in the job finding rate… contributed to every increase in the unemployment rate during the post-war period – [Reassessing the Ins and Outs of Unemployment](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1014798)

As the job finding rate slowly recovers, so does unemployment.

So, rather than massive layoffs defining recessions and spikes in unemployment, the more powerful explanatory story is one of hiring freezes – employers simply stop hiring during recessions.

Per our stocks-and-flows model, the stock of unemployed workers grows and shrinks primarily due to variation in the _outflow_ from unemployment to employment rather than the _inflow_ of employment to unemployment.

In other words, **hiring freezes are the real story**, not layoffs.

## Hiring or firing?

What's the relative importance of hiring freezes vs layoffs?

Here's how we can estimate this:

-   Let's say we hold variable A constant and allow variable B to move around. If unemployment still moves around a lot then we know most of its variation is driven by variable B
-   Similar logic applies in the opposite scenario (allowing A to fluctuate while holding B constant)

This table quantifies the relative contributions of job entry and exit rates to overall unemployment variability by calculating counterfactual scenarios in which either variable is held constant while the other is allowed to fluctuate. The first row holds job separation constant and the second holds job finding constant ([Source](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1014798)):  
![Pasted-image-20220814173836](https://nnamdi.net/content/images/2022/08/Pasted-image-20220814173836.png)

The result?

> ... since 1987, including the recessions in 1990–1991 and 2001, and 2008–2009, the job finding rate accounted for virtually all fluctuations in the unemployment rate – [Reassessing the Ins and Outs of Unemployment](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1014798)

**The job finding rate explains the vast majority of variation in unemployment**, and the proportion has only increased over time.

From 1987 to 2010, **variation in the job finding rate explained 90% of the variance in unemployment,** while employment exit only explained 10%.

Here's Shimer again:

> The bulk of the reason unemployment rises and stays high throughout the recovery is because the unemployed workers stay unemployed for longer…

This is precise, quantitative evidence for the overwhelming importance of hiring over firing in explaining unemployment.

## Hyper-(de)growth

If the prior data seemed counterintuitive, the following example shouldn't.

Imagine you're a high-growth startup with big ambitions to grow the employee base in the coming year:

-   You are currently 100 employees
-   You want to hire an additional 100 people over the next year
-   We ignore voluntary attrition just to keep the numbers simple

Suddenly, the environment changes:

-   Economic conditions worsen, capital gets tighter, you start to miss plan, etc
-   You need to make cuts, both in the current employee base _and_ the future one
-   So, you lay off 20% of the team

This is extremely painful, demoralizing, and controversial, only made easier by the fact that everyone else is doing it too:

> My sense is that it's partly - "if we layoff people at the same time everyone else does, it'll look more like bad market conditions were to blame and less like we've ran this thing poorly"  
>   
> i.e. if everyone stampedes at the same time, no one is to blame for trampling anyone
> 
> — Nnamdi Iregbulem (@whoisnnamdi) [May 5, 2022](https://twitter.com/whoisnnamdi/status/1522354980111347712?ref_src=twsrc%5Etfw)

You also institute a hiring freeze, which only seems fair after laying off so many team members:

> 3\. Immediately re-hiring after layoffs paints the picture of leadership of what it is:  
>   
> Incompetent.  
>   
> Couldn't even figure out what % of people to lay off.
> 
> — Gergely Orosz (@GergelyOrosz) [July 8, 2022](https://twitter.com/GergelyOrosz/status/1545358652424568833?ref_src=twsrc%5Etfw)

It doesn't take an HR savant to realize you aren't going to hit your original 200 headcount target for next year.

Now do the gap analysis – between layoffs and slowed hiring, which is more responsible for missing the original headcount target?

It's the hiring freeze of course!

-   You laid off 100 x 20% = 20 people
-   Meanwhile, you cut the hiring plan from 100 to zero

In effect, you "fired" or "laid off" 100 _future_ employees while only laying off 20 _current_ ones:  
![SCR-20220831-4vs](https://nnamdi.net/content/images/2022/08/SCR-20220831-4vs.png)

As one founder friend whose recent layoff was leaked on social media told me:

> Sure, the layoff news leaked, but when we modeled this stuff out we realized it's the reduced hiring that would save us the most cash.

A hiring freeze is a **much** bigger story, even if the media only reports layoffs:

-   Internally, management knows the hiring freeze will have a larger impact on team velocity (and, frankly, cash burn) than the layoffs.
-   Yet externally, everyone will focus on the layoffs.

This misses the raging forest fire for the merely singed trees.

## A new perspective

Recognizing the importance of hiring freezes should change your focus regardless of whether you're a founder or an employee.

As a tech worker:

-   The biggest risk you face in an economic downturn is not so much losing your current job but instead **being unable to find a new one** (due to hiring freezes)
-   In addition, as you assess the viability of various startups, pay equal, if not more, attention to slowed hiring at the company than highly visible layoffs
-   When interviewing, ask about the hiring plan and how it's changed in recent quarters

If you're a founder:

-   Slowed hiring is the real sign of weakness and vulnerability in your competition
-   Even with no layoffs, your competitors could be in a dire financial straits, necessitating a complete freeze in hiring
-   If you're well-capitalized, your biggest opportunity during a downturn isn't picking up people who were laid off (though you should definitely consider this) but rather hiring the people your competitors _would have wanted_ but now can't due to hiring constraints

## Lay off the layoffs

Layoffs are a noisy estimate of company performance:

-   Layoffs are tainted with worries about public perception and internal morale, making otherwise troubled companies hesitant to do them
-   Companies conduct layoffs during downturns in part because everyone else is doing it, and they know they won’t look as bad if they move with the herd

Hiring freezes are a clean signal of a viability and confidence of management:

-   Few companies would cut back on their hiring plans _simply to fit in_
-   If anything, knowing others are pulling back makes companies _more_ aggressive in hiring

That said, hiring freezes are opaque from the outside:

-   Outside of large organizations, most never publicly announce hiring freezes
-   In many cases, hiring freezes aren't even openly discussed _internally_ outside of the core executive team and the board of directors

Keep in mind both the seen _and_ the unseen. It's easy to focus on the shock and awe of layoffs, but you miss so much in doing so.

Layoffs aren't even half the story – they're 10% of it.

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div> ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[COVID Hurt Most Software Companies]]>
                        </title>
                        <description>
                            <![CDATA[COVID put software companies on a permanently lower growth trajectory.]]>
                        </description>
                        <link>https://whoisnnamdi.com/covid-hurt-software/</link>
                        <guid isPermaLink="false">covid-hurt-software</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Tue, 2 Aug 2022, 15:25:00 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512277694-mm.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
COVID hurt most software companies.

While many [claim](https://www.mckinsey.com/business-functions/strategy-and-corporate-finance/our-insights/how-covid-19-has-pushed-companies-over-the-technology-tipping-point-and-transformed-business-forever) COVID accelerated technology adoption and digital transformation, this didn't show up in the revenue of most software vendors.

In fact, the typical public software company remains 10-12% below its pre-COVID revenue trend, and the gap is only widening.

**COVID put most software companies on a permanently lower growth trajectory.**

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div>

## A few case studies

The COVID shock generated divergent outcomes across software companies.

Let's focus on three examples to see what I mean: Zoom, Cloudflare, and Splunk.

The solid lines below represent the ([log-transformed](https://people.duke.edu/~rnau/411log.htm)) quarterly revenue of each company. The dashed line represents a constant-growth linear trend based on their two-year _pre-COVID_ revenue trajectory. Since the scale is in logs, straight lines imply _constant quarter-over-quarter growth_:

![trends](https://nnamdi.net/content/images/2022/08/trends.png)

We see some interesting behavior as we cross the dashed line, which represents Q4 2019, the last quarter before COVID struck:

-   **Zoom** immediately takes off, "zooming" above its pre-COVID trend as millions move to online communication. The trend eventually catches up
-   **Cloudflare** is totally in the clouds, floating along like nothing happened, perfectly matching its pre-COVID trend of constant, exponential growth
-   **Splunk** exhibits strong seasonality, so the initial decline is no surprise. However, it never recovers to trend, even two years later. Splunk got dunked

These symptoms run the gamut from manic to lethargic. Clearly, COVID was not a tide that raised all boats.

## Mean reversion, or mean revision?

We can repeat the above analysis for all public software companies with enough data to establish a two-year pre-COVID trend. Given the different revenue scales, for visualization purposes let's normalize by dividing each company's actual performance by its extrapolated trend. This gives us the relative revenue performance of each vs. trend:

![deviation](https://nnamdi.net/content/images/2022/08/deviation.png)

Each line represents performance vs. trend for one of fifty different public software companies.

With this we can begin to see the rich variety of post-COVID outcomes. Let's highlight our three case studies once more:

![equal](https://nnamdi.net/content/images/2022/08/equal.png)

-   Notice how dramatic and unrepresentative Zoom is among all software companies. **Zoom peaks at ~150% above trend** before gliding back down to Earth
-   On the other hand, Splunk comes out as one of the worst performers, among others like Alteryx and Benefitfocus

There's a lot going on here, making it difficult to get a sense of the underlying distribution. Let's flip the axes and look at it a different way:

![dispersion](https://nnamdi.net/content/images/2022/08/dispersion.png)

The above chart plots histograms of performance relative to trend across public software companies from Q1 2020 onward:

-   Upon impact, performance is normally distributed, centered somewhat below zero
-   Over the subsequent quarters, the distribution _shifts left and widens_, implying worse outcomes and greater dispersion between companies
-   By eight quarters out, **performance is unambiguously below trend**, while variance continues to grow

Notice the skew: **a handful of winners among a majority of losers.**

Another way to look at the data is to calculate the proportion of software companies that climbed above their pre-COVID trend over time:

![aversion](https://nnamdi.net/content/images/2022/08/aversion.png)

-   Initially, only a quarter of software vendors were above trend
-   One year out, one-third of companies have returned to trend
-   The proportion stabilizes thereafter. **Mean reversion is over**

_This is interesting._ As dispersion grew in the first year of the pandemic, some companies returned to their pre-COVID trend. After these initial quick recoveries, however, **mean reversion completely shuts down.** The last lifeboats cast off, and if you aren't onboard, you're [ngmi](https://nftska.com/what-is-the-meaning-of-ngmi-and-wagmi-nft-terminology/).

A key point I want to drive home: mean reversion is a strong force, but **COVID was stronger**, especially on the downside. Once knocked down, the typical software business never got back up.

## Weakness, in numbers

Finally, let's summarize those performance distributions by their means and medians:

![mm-1](https://nnamdi.net/content/images/2022/08/mm-1.png)

This is what I've been building up to – **COVID materially impaired the typical software company, knocking it off trend**:

-   On average, software companies were down **~5% relative to trend 4 quarters out and down ~10% 8 quarters after impact**
-   The median dampens the impact of outliers like Zoom. Notice, median performance is _even worse_, **~7.5% at 4 quarters and ~12.5% at 8 quarters**
-   Meanwhile, underperformance relative to trend is still increasing. The gap between actual revenue and trend is growing ~4% per year

Thus, COVID's impact was not transitory: **COVID permanently reduced the annual growth rate of public software companies by 4 percentage points.**

_Mean revision_ is at least as important as mean reversion. Not only are software companies below their pre-COVID trend, _the trend itself_ has changed for the worse.

## Conclusion: Conflation

I think there's been a massive conflation between:

1.  the equity returns of software companies during the first year and half of COVID (exceptional, but temporary),
2.  the financial performance of a select few COVID beneficiaries like Zoom (again, exceptional, but transitory), and
3.  the business performance of the broader software market (poor, and durably so)

The temporarily exuberant stock prices of most software companies distracted from the toll COVID took on business performance. While investors got high on the Fed's supply, businesses themselves were silently suffocating.

This corroborates the findings of my [last essay](https://whoisnnamdi.com/dark-matter/), where I explored how the mysterious "GDP factor" explains some the the "dark matter" in software valuations:

> Despite some claims to the contrary, investors believe software companies are quite sensitive to the broader macroeconomic environment. In fact, it's the variable that correlates _most highly_ with valuations after accounting for the individual financial performance of each company – [The Dark Matter of Software Valuations](https://whoisnnamdi.com/dark-matter/)

Here again, I find that software companies are sensitive to economic conditions, even in a best case scenario that supposedly favored them. "Digital transformation" can't come quickly enough it seems.

So let's set the record straight: **COVID left a nasty scar on us all, software companies included.**

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div> ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[The Dark Matter of Software Valuations]]>
                        </title>
                        <description>
                            <![CDATA[Exploring the vast "dark matter" of the software universe]]>
                        </description>
                        <link>https://whoisnnamdi.com/dark-matter/</link>
                        <guid isPermaLink="false">dark-matter</guid>
                        <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Tue, 19 Jul 2022, 09:56:45 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512275936-varexp-1.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
There's something strange about software valuations.

Investors tend to focus on [revenue growth and profitability](https://whoisnnamdi.com/rule-40/) as significant variables for valuing software companies, but those metrics only explain about _half_ of the overall variation in valuations.

This leaves a vast amount of "dark matter" in the software universe – variation in valuations that goes _unexplained_.

I want to explore this vast darkness.

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div>

## Who knew there was such skew?

![avg](https://nnamdi.net/content/images/2022/07/avg.png)  
_Note: this analysis only includes public software companies that traded continuously from January 2020 to May 2022. Companies that went public or got acquired during that period are excluded._

It's been said before, and it's worth repeating: **averages don't tell the whole story.**

You can begin to see why by plotting average revenue multiples against median multiples. _They aren't the same._

In fact, the distance between them tends to fluctuate over time.

-   In the lead up to COVID they were quite close
-   Then, they grew apart
-   Only recently, they've gravitated closer again

A single line at best summarizes the data and invariably misses important details.

Let's zoom in:

![muldist](https://nnamdi.net/content/images/2022/07/muldist.png)

Like [software monetization](https://whoisnnamdi.com/software-fat-tailed/), software valuations are [fat-tailed](https://en.wikipedia.org/wiki/Fat-tailed_distribution):

-   Most companies have unremarkable valuation multiples
-   A few, however, skew well to the right, creating a long tail of multiples

This tail skews the distribution of multiples and increases the variance between public software companies. I'd imagine the situation is even more extreme among private software companies.

That variance changes over time, rising and falling:

![mulvar](https://nnamdi.net/content/images/2022/07/mulvar.png)

_Note: for statistical reasons I won't explain here, the remainder of this essay will refer to log-transformed software multiples instead of their raw values._

I'd love to explain this dynamic.

It's at this point that any good software investor turns to ol' reliable: regressions of valuation multiples on revenue growth and profitability:

![gm](https://nnamdi.net/content/images/2022/07/gm.png)

We see a similar, if subdued, pattern in the variance that can be explained by financial performance. But notice how much variance our standard regression leaves on the table, unexplained.

Growth and profitability do not perfectly predict valuations.

I call this unexplained variance, the "dark matter" of software valuations:

![varexp](https://nnamdi.net/content/images/2022/07/varexp.png)

Growth and profitability never explain much more than half of the variation in multiples:

![pvarexp](https://nnamdi.net/content/images/2022/07/pvarexp.png)

What's hiding down in the deep, dark depths of the software universe?

Let's grab our telescope and zoom in once again.

## Stranger things (and valuations)

We know that growth and profitability are imperfect predictors of valuation.

Let's remove their effect and focus now on the remaining, residual, variance in valuations. This means that we'll focus on relative rather than absolute valuations – relative to what growth and margins would predict, that is.

Here's how it looks:

![resdist](https://nnamdi.net/content/images/2022/07/resdist.png)

The fat tail is less severe but still persists, even after accounting for financial performance.

Many companies hover near zero, which is to say their valuations are perfectly predicted by growth and profitability, but a number are quite "overvalued" on the basis of their financials alone.

This gap between expectations and reality is quite persistent. Over/undervaluation _does not_ meaningfully mean-revert, even over multiple years:

![resplot](https://nnamdi.net/content/images/2022/07/resplot.png)

-   A 50% overvalued company in January 2020 by May 2022 still traded **~40% above** the valuation implied by its growth and profitability
-   A 50% undervalued company continued to sag **~35% below** its predicted valuation by the end of this period

Some did see major revisions:

-   Shopify, Okta, Coupa, DocuSign, and RingCentral all fell back down to Earth after flying high for some time
-   Cloudflare, Appian, Alteryx, MongoDB, Atlassian, Anaplan, and Guidewire all ended up much higher than where they were pre-COVID

In addition to individual companies, the overall distribution of unexplained "valuation inequality" held in place the past few years, with some fluctuations:

![percplot](https://nnamdi.net/content/images/2022/07/percplot.png)

-   Investors value the 90th percentile company **~75% more** than you'd expected from its growth and profitability
-   Meanwhile, they peg the 10th percentile company **~50% lower** than you'd predict.
-   In terms of ratios, 90th percentile software vendors are worth **3-5 times** their 10th percentile peers.

Remember, _we've already accounted for revenue growth and cash flow_, yet we still see massive variability:

![resgm](https://nnamdi.net/content/images/2022/07/resgm.png)

-   On the revenue side, among companies growing ~50% annually we see valuation gaps as large as **~70%** (i.e., same growth, yet one company is worth 70% more).
-   It's even worse for software companies growing closer to 20% year-over-year, where the gaps are even bigger: the most valuable company is worth **~2.5 times** the least valuable.
-   On the free cash flow side, if we look at companies within +/- 10 percentage points of breakeven, we see companies worth **five times** as much as the least valuable

Needless to say, the variation in software valuations is extreme, and that's _after_ factoring out what we'd already expect based on financial profiles.

## Darkness into light

I hope I've driven the point home that growth and profitability aren't everything.

OK, but what _is_ this dark matter stuff?

Clearly the most obvious metrics fail to account for a sizable share of the variation in valuations among software stocks. Some sort of unobserved or latent characteristics drive the remaining differences.

[Principal Component Analysis (PCA)](https://setosa.io/ev/principal-component-analysis/) is a great way to identify such latent factors. It's a statistical technique for summarizing the key informational content contained within a larger data set.

A full explanation of PCA is beyond the scope of this essay. Just know that, like alchemy, PCA lets us put dark matter in and get some nuggets of gold out.

When I run PCA, here's what I find:

![fac](https://nnamdi.net/content/images/2022/07/fac.png)

PCA spits out two latent "factors" that highly explain our up-till-now unexplained valuation puzzle.

Unfortunately, these factors don't come out of the box with much explanation. We must interpret them manually.

After staring at "Factor 1" for a while, it hit me – the line looks eerily similar to the path of the U.S. economy over that period!

Folks, we have a winner:

![gdpfac](https://nnamdi.net/content/images/2022/07/gdpfac.png)

Beautiful.

Our first hidden factor is not a particularly mysterious one.

Despite some claims to the contrary, investors believe software companies are quite sensitive to the broader macroeconomic environment. In fact, it's the variable that correlates _most highly_ with valuations after accounting for the individual financial performance of each company.

The latent factor tracks the COVID recovery quite well, in some ways even better than the official GDP statistic, which is only released quarterly – a significant delay.

The factor provides a daily, real-time estimate of economic performance, at least that portion which is relevant for software companies (also called [nowcasting](https://www.oecd.org/economy/weekly-tracker-of-gdp-growth/)).

I had less success figuring out the second factor. If you have any ideas, please send them my way!

With PCA, it's customary to plot the data against the first two factors, with the X and Y axes representing each company's correlation with the factor:

![load](https://nnamdi.net/content/images/2022/07/load.png)

-   Companies on the right side correlative _positively_ with the GDP factor, suggesting they've benefitted disproportionately from the COVID bounce-back
-   Conversely, stocks hovering to the left had _less_ to gain from the rebound

I could spend (and may have already spent) hours staring at this plot. Rather than continuing to play "[Where's Waldo](https://en.wikipedia.org/wiki/Where%27s_Wally%3F)," here are a few clusters I noticed (Q: What did I miss?):

-   **Financial engineering:** Developer-centric tooling and infrastructure like Datadog, Elastic, MongoDB, Atlassian, PagerDuty
-   **Easy storage, easy life:** Storage companies Dropbox and Box
-   **Trust, but verify:** Zero-trust and identity solutions like Okta, Ping Identity, Zscaler
-   **Call me maybe:** Cloud-based contact center and communications companies Five9, RingCentral, 8x8

## A step in the right direction

Now let's come full circle and incorporate the GDP factor into our previous regression while also allowing each stock to have different sensitivity to the factor.

Here's what we get:

![varexp2](https://nnamdi.net/content/images/2022/07/varexp2.png)

The GDP factor helps explain some of the mysterious "dark matter" but not always the same amount:

-   In the weeks before COVID turned everything upside-down, the GDP factor didn't explain much variation in valuations
-   Then, as the pandemic began, the GDP factor rose in importance, granting significant predictive power
-   As the first positive results of the mRNA vaccines come out, it recedes again, and for about three quarters the GDP factor doesn't explain much over and above individual financial performance
-   Finally, as the COVID recovery finally stalls out, the GDP factor rises in importance again

Here's how it looks in [R-squared](https://www.investopedia.com/terms/r/r-squared.asp) terms:

![pvarexpfac](https://nnamdi.net/content/images/2022/07/pvarexpfac.png)

Still a lot of dark matter out there, but we've made great progress:

-   At it's highest, the combination of (1) individual financial performance and (2) sensitivity to economic conditions explains ~80% of the cross-sectional variance in software valuations
-   That said, there remain times when it explains no more than 50% of valuation variance

I'm fascinated by these dynamics, especially the middle period where, for whatever reason, economic sensitivity temporarily stopped mattering for how software companies were being valued, at least relative to one another.

## Conclusion

So we've managed to chip away at some of the dark matter hiding among software valuations.

It's often said that the long-term growth trajectory of cloud companies is relatively secular, which is to say it's independent of general economic fluctuations.

It turns out software companies \*are\* sensitive to broader economic conditions in the short-run, at least in eyes of public investors.

The relevance of this sensitivity rises and falls over time:

-   In periods of economic uncertainty (e.g. early pandemic, Q2 2022, etc), the GDP factor begins to matter quite a bit. Public investors care who gains and loses from a weakening economy.
-   Meanwhile, when times are good, investors throw caution to the wind and ignore the economic sensitivity of software companies.

More sophisticated [factor modeling](https://en.wikipedia.org/wiki/Factor_analysis) could yield even more insights. No one pays me to do that though, so I think I'll [move on](https://www.youtube.com/watch?v=waEC-8GFTP4).

If you do have thoughts on that second mystery factor, shoot me a note!

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div> ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Series A Rounds Are a Math Test]]>
                        </title>
                        <description>
                            <![CDATA[Low monetization requires extraordinary traction, and vice versa.]]>
                        </description>
                        <link>https://whoisnnamdi.com/series-a-math/</link>
                        <guid isPermaLink="false">series-a-math</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Mon, 27 Jun 2022, 17:01:07 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512274243-header.jpg" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
For your seed stage startup to graduate to the next level, it must pass a new test.

Suddenly, a PowerPoint deck isn't enough. _Traction and monetization matter._

Your startup might have more of one than the other, and that's OK.

But less of one means you need more of the other. That's just math.

How much more? It turns out – more than you think.

**Low monetization requires extraordinary traction, and vice versa.**

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div>

## 2 + 2 = 4

In addition to factors like _team quality_ and _market size_ (which also matter at the pre-seed/seed stage), investors evaluate Series A startups on their **traction** and **monetization**.

By traction, I mean some rough mix of:

-   bottom-up adoption
-   free usage / free trials
-   organic, inbound interest in the product
-   proliferation of an open source project
-   downloads
-   a vibrant, growing community
-   product-led growth

By monetization, I'm loosely referring to:

-   revenue (duh!)
-   customers
-   top-down / direct sales
-   production deployments
-   pilots / POCs (proofs of concept)
-   qualified pipeline

_Note: monetization is a catch-all for activities and metrics that tend to correlate with revenue, so a company with zero revenue can still have a decent monetization "score."_

If traction and monetization are collectively high enough, you will successfully raise your Series A round.

You can even make a little equation out of this:

$$\text{Fundraising Success} = \text{Traction} + \text{Monetization}$$

There's a threshold, a sort of activation energy, required to successfully raise a round at the early-stage. Hit this bar and you're in the clear. Miss it, and you're not:

![1](https://nnamdi.net/content/images/2022/06/1.png)

Let's make up some numbers.

Say the threshold is "4". You'd need some combination of traction and monetization to get to a sum of four:

-   A score of two on both traction and monetization would work. We could call that "moderate traction and moderate monetization", or (2, 2)
-   But a (3, 1) combo would also work – perhaps "high traction, low monetization", and so on.

As long as you can get to a total of four, you're good.

## 2 x 2 = 4

What if our model is wrong though?

Imagine investors evaluate your startup not on the _sum_ of traction and monetization, but rather the _product_. In other words, traction and monetization multiply, rather than add:

$$\text{Fundraising Success} = \text{Traction} \times \text{Monetization}$$

Through this lens, the marginal value of one factor depends on the other. Traction makes monetization more valuable, and vice versa. Monetization is more valuable when you have more traction.

This is a much better model for how venture investors think about startups.

Think about it:

-   Strong traction makes additional monetization more valuable, since there's a preexisting, large group of interested, engaged users to sell into.
-   Strong monetization increases the value of additional traction, since you know that traction will be monetizable in the future.

The multiplicative model has an important implication however – it disproportionately penalizes low traction and/or low monetization.

To successfully fundraise, low monetization requires extraordinary traction, and vice versa:

![2](https://nnamdi.net/content/images/2022/06/2.png)

Here's an analogy – a stock that drops by 50% (OK, maybe more reality than analogy) must rise by much more than 50% to reach its original price level, since returns are multiplicative (50% x 150% is only 75%). Likewise, **low monetization requires more than proportionally higher traction** to make up for it.

(2, 2) still works, since 2 x 2 = 4. But the (3, 1) combination is no longer viable, since 3 x 1 only equals 3. If your startup is merely a "1" on the monetization scale, you now need to be a "4" on traction. 0.5 on monetization? You need to be an 8 on traction!

You can see where this is going.

## 3 x 1 = ☠

Unfortunately, founders fall into the trap of thinking low monetization or traction can be made up for with merely moderate amounts of the other factor. This is wrong.

Many founders think investors will evaluate them with the linear, additive model. While many investors are quite linear in their thinking (sorry, had to do it), the multiplicative perspective is the more accurate one.

This clash of reality vs. expectations creates a "[no man's land](https://en.wikipedia.org/wiki/No_man%27s_land)" between the additive and multiplicative perspectives: founders think they'll be fine when they go out to fundraise, only to be rudely mugged by the valley reality:

![3](https://nnamdi.net/content/images/2022/06/3.png)

I've noticed many startups in the space between the linear and convex curves attempting to raise Series A rounds lately. **This invariably goes badly.** The founders are surprised by the lack of investor interest and end up taking much longer to raise their round, at a lower valuation than expected.

Some never raise at all.

## Quick maths

Avoid this needless suffering. Be honest with yourself – if you're lacking in either traction or monetization, be sure you're killing it on the other dimension.

I've made the point that low monetization requires extraordinary traction (and vice versa), but I haven't defined what "extraordinary" means. Frankly, it's subjective at best, and I'd be lying if I said there was an objective standard or rubric.

When you know, you know:

> you can always feel product/market fit when it's happening – [Marc Andreessen](https://pmarchive.com/guide_to_startups_part4.html)

> … once you have product-market fit… \[business\] performance accelerates so meaningfully that any ambiguity goes out the window. You get it "in your gut." It becomes obvious, as nearly everything about the business gets better – [Product-Market Fit is Lindy](https://whoisnnamdi.com/product-market-fit-is-lindy/)

> If it is true that product-market fit yields much better business performance, then product-market fit should be relatively obvious, even with little data – [Product-Market Fit is Lindy](https://whoisnnamdi.com/product-market-fit-is-lindy/)

Further, I've simplified the framework here and taken other factors like team, market, etc as given. Realistically, investors will evaluate those things too, complicating our math a bit:

$$\text{Fundraising Success} = \text{Traction} \times \text{Monetization} \times \text{Team} \times \text{Market} \times \ldots$$

That's a lot harder to visualize, but the same ideas apply. You need to really spike on one or more factors to make up for lower scores on the others. Get this math wrong, and you'll be in for a nasty surprise.

Rather than cram for the exam, delay your Series A until you pass the simple math test I've outlined here. Until then, keep practicing those multiplication tables!

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div> ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Funding Simply Shifts the Bottleneck]]>
                        </title>
                        <description>
                            <![CDATA[A frothy funding environment means more competition for talent. Funding gets easier; hiring gets harder.]]>
                        </description>
                        <link>https://whoisnnamdi.com/funding-bottleneck/</link>
                        <guid isPermaLink="false">funding-bottleneck</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Fri, 22 Apr 2022, 12:56:12 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512272380-Funding-Simply-Shifts-the-Bottleneck_2022-04-18-15.51.14.excalidraw-2.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
Abundant capital is not a panacea for founders.

For the best companies, raising large war chests along with the branding effect that comes with them can serve as an "unfair advantage" against competitors, compounding gains and maintaining pole position.

This unfair advantage often comes through the hiring channel, which is high-octane leverage for fast-growing companies:

> **The “10x” in “10x engineer” conveys leverage just as the “3x” in “3x S&P” does.** As an investor, leverage can multiply the returns you receive from a great investment thesis. And similarly, if you’re a founder with a great vision, your superstar employees will help you realize it faster and better – [Talent as leverage](https://www.dwarkeshpatel.com/p/talent-as-leverage)

For most however, **a frothy funding environment means more competition for talent.** If everyone is raising tons of cheap capital and plowing those funds into hiring, your job as a founder hasn't necessarily gotten easier, since presumably you'll be working that much harder to recruit against every other startup doing the same.

The capital "bottleneck" hasn't disappeared – it's moved to another part of the production chain. **Funding gets easier; hiring gets harder.**

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div>

## Cheap money, expensive people

He deleted the tweet, but [Mike Speiser](https://twitter.com/laserlikemike) once said:

> When money is cheap, people are expensive. When money is expensive, people are cheap. #dilution

The idea that hiring gets harder when funding gets easier (and vice versa) has two foundational components. The first one is easy, so let's get that out of the way first.

When companies are cash rich, they tend to spend more (\*shocker\*). Startups raise funding in large part to expand their teams. When the entire startup ecosystem simultaneously raises tons of capital, the additional money sloshing around inevitably shows up in the W2s of startup employees and the "voluntary churn" column of board of directors presentations. Recruiters spam the LinkedIn inboxes of every warm body in sight, and the hiring market heats up.

That's fairly intuitive. The second reason is much less so – discount rates.

Go back to the Mike Speiser quote. What does it mean for money to be "cheap?" Financial theory introduced us to the concept of the "discount rate" or equivalently the required rate of return or cost of capital. Money is "cheap" when discount rates are low, implying that investors and lenders require only a small rate of return on their investments in order to be satisfied.

Typically, low discount rates are associated with high valuations, since the present value of a series of cash flows rises as discount rates falls. Thus we get the frequent gyrations of the stock market, which summarize this dynamic in a single number:

> the large movements in the value of the stock market arise mainly from changes in discount rates and only secondarily from changes in the dividend or profit flow capitalized in the stock market – [High Discounts and High Unemployment](https://web.stanford.edu/~rehall/High%20Discounts%20and%20High%20Unemployment%20AER)

It's quite natural to apply the notion of discount rates to the value of financial assets, like company equity. But why limit our thinking to financial assets?

## The present value of talent

Discount rates apply to human capital too.

If you don't believe me, check out the following chart. It plots the unemployment rate over time along with the _inverse_ of the detrended (i.e. accounting for the size of the U.S. economy) S&P 500 index (so down is up and vice versa). Notice the tight relationship between the two:  
![Pasted-image-20220126122928](https://nnamdi.net/content/images/2022/04/Pasted-image-20220126122928.png)

> Figure 2 suggests that **the stock market and unemployment respond to the same underlying forces**, especially in the past few decades – [High Discounts and High Unemployment](https://web.stanford.edu/~rehall/High%20Discounts%20and%20High%20Unemployment%20AER)

You might ascribe this correlation to general economic conditions rather than discount rates, but unemployment and output per worker don't actually track each other very well:  
![Pasted-image-20220416164600](https://nnamdi.net/content/images/2022/04/Pasted-image-20220416164600.png)

To see how discount rates apply to workers in the same way they do to financial assets, imagine the net value of a worker to a company being their productivity minus their cost, i.e. wage. This "job value" takes into account all future value the worker generates along with their cost of employment, in present value terms:

$$\text{Job Value} = \text{Present Value }(\text{Productivity} - \text{Wage})$$

Job value tends to increase during booms, for two reasons:

1.  Discount rates are low, which inflates present value calculations,
2.  Productivity is more variable than wages, which tend to be sticky

Thus, the present value of productivity moves up and down by more than wages, driving job value up or down with the market cycle:

> **A higher discount lowers the job value**… The present value of the wage moves in the same direction as productivity, but less than in proportion. In this sense, the wage is sticky. Because the wage falls less than productivity, the job value… falls more than in proportion to the fall in productivity – [High Discounts and High Unemployment](https://web.stanford.edu/~rehall/High%20Discounts%20and%20High%20Unemployment%20AER)

Naturally, the higher the "job value," the more eager employers are to hire workers. We get cutthroat competition among employers during good economic times and workers being dropped like hot potatoes during bad times. When the present value of hiring an additional worker is high, employers clamor to grow their ranks.

It's possible to use economic data to estimate job value across the entire U.S. economy. **Job value and the stock market move together in lockstep**:  
![Pasted-image-20220126125737](https://nnamdi.net/content/images/2022/04/Pasted-image-20220126125737.png)

> The similarity of the job value and the stock market value is remarkable. The figure strongly confirms the hypothesis that **similar forces govern the market values of claims on jobs and claims on corporations** – [High Discounts and High Unemployment](https://web.stanford.edu/~rehall/High%20Discounts%20and%20High%20Unemployment%20AER)

The up and down movement of job value is incredibly consistent across industries too:  
![Pasted-image-20220416154129](https://nnamdi.net/content/images/2022/04/Pasted-image-20220416154129.png)

> The strikingly similar responses in diverse industries strongly supports the hypothesis that an aggregate driving force dominates the movements of the job value at the industry level. **This evidence points in the direction of aggregate forces such as rising discounts in recessions** – [High Discounts and High Unemployment](https://web.stanford.edu/~rehall/High%20Discounts%20and%20High%20Unemployment%20AER)

## When hiring falls apart

**When interest rates are low, leverage becomes more attractive.** Although the venture capital world likes to think of itself as distinct from debt-laden private equity land, in fact technology startups engage in all sorts of leverage, especially during boom times. I talked about one form of it, [preferred equity](https://whoisnnamdi.com/schrodingers-balance-sheet/), in a prior essay:

> In this way, startups become **synthetically levered**. No debt appears on the balance sheet, yet the returns of the common equity (any really anyone below the most senior preferred stock) get juiced by the lower cost of capital achieved via preferred equity – [Schrodinger's Balance Sheet: When Equity Becomes a Liability](https://whoisnnamdi.com/schrodingers-balance-sheet/)

I referred earlier to the notion that exceptional employees are a form of leverage. Just like financial leverage, _talent leverage_ allows a company to do things they wouldn't otherwise be capable of with existing resources.

To attract such special individuals, startups have to promise the world to these ambitious folks, a sort of "debt." **These superstar employees wouldn't join otherwise** – the cash compensation alone is rarely worth it. They must be promised quickly-appreciating equity, as well as the Silicon Valley cache and prestige associated with a successful startup. This is related to Keith Rabois' idea of "[talent arbitrage](https://leveragingideas.com/talent-arbitrage/)."

This Silicon Valley "spin" peaks during market highs – frothy periods where slick storytellers paint increasingly fantastical and implausible visions for what their company will achieve. It's a competition to see who can lever up their fragile enterprise the most.

If things start to go south, the bubble bursts, and these 10x employees are often the first to abandon ship:

> The leverage you get from hiring really talented people is a huge risk during rough times, because these people have lots of other options and the ambition to pursue them.  
> …  
> If any of of the people in this chain stop believing the hype around which their project is organized, then the hype becomes unjustified… And once the bubble starts to wobble, 10x employees will move on to the next compelling tech vision, causing the leveraged death spiral mentioned in the last section. **Leveraging your company with talent increases your volatility - either you orchestrate a revolution, or you implode** – [Talent as leverage](https://www.dwarkeshpatel.com/p/talent-as-leverage)

It's never pretty when leverage blows up, and talent leverage is no different. Troubled startups face double jeopardy – preferred equity implodes, as does the talent stack.

## Easy funding =/= easy life

![Funding-Simply-Shifts-the-Bottleneck_2022-04-18-17.19.06.excalidraw](https://nnamdi.net/content/images/2022/04/Funding-Simply-Shifts-the-Bottleneck_2022-04-18-17.19.06.excalidraw.png)

I'm increasingly skeptical of the notion that easy funding makes the lives of founders easier. _Funding merely shifts the bottleneck._

Difficulty hiring is a recurring theme I've seen among founders during red-hot markets. **The financial capital comes easy; the human capital doesn't.**

In fact, hiring can be _tougher_ after raising at a big valuation because candidates might think all the upside is gone already, making equity compensation less attractive:

> At Retool, we decided to raise the smallest Series C we could, at a valuation substantially lower than most offers we received. Why? Because even though maximizing our valuation and money raised might be optically good (the press loves to report on large financing rounds at large valuations), it hurts the core constituency that got us here: our team… **Reaching peak valuations too early results in substantially less upside for employees.** – [Raising less money at lower valuations](https://retool.com/blog/series-c/)

Between the increased effort that hiring requires during boom periods and the prospect of employee exodus after hitting choppy waters, founders should be wary of getting caught up in the irrational exuberance of the broader market.

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div> ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Breaking Apart the Rule of 40]]>
                        </title>
                        <description>
                            <![CDATA[Rules are meant to be broken, and the Rule of 40 is no exception]]>
                        </description>
                        <link>https://whoisnnamdi.com/rule-40/</link>
                        <guid isPermaLink="false">rule-40</guid>
                        <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Thu, 3 Mar 2022, 09:49:27 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512270632-header.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
Rules are meant to be broken, and the Rule of 40 is no exception.

The Rule of 40 is a popular heuristic for evaluating financial performance and predicting valuation multiples.

But it's got 99 3 problems:

-   It's too rigid
-   It's too short-term
-   It's too narrowly focused

In this essay, I break apart the Rule of 40, fix it, and put it back together again.

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div>

## Laying down the rules

After flying high for some time, software valuation multiples have come back down to Earth (Note: LTM multiples used throughout this article):  
![rev_mul](https://nnamdi.net/content/images/2022/03/rev_mul.png)

While there are [all sorts](https://whoisnnamdi.com/grow-valuation/) of [explanations](https://whoisnnamdi.com/high-retention-high-volatility/) for the recent tech correction, the [Rule of 40](https://feld.com/archives/2015/02/rule-40-healthy-saas-company/) concerns itself not so much with the ebbs and flows of software valuations but rather their determinants at _a single point-in-time_. It provides a simple rule of thumb for evaluating the performance of software companies and estimating their likely valuation multiples:

$$\text{Rule of 40} = \text{Growth} + \text{Profitability}$$

The Rule of 40 states that the sum of annual revenue growth and profitability (either EBITDA or free cash flow margin) should equal 40 or more for the best performing companies, as in the following slide from [Battery Ventures' Software 2021 deck](https://www.scribd.com/document/497410287/Battery-Ventures-Software-2021-Report):  
![Pasted-image-20220222124207](https://nnamdi.net/content/images/2022/03/Pasted-image-20220222124207.png)

In general though, the "40" isn't particularly important. Higher is better; that's it.

## "Houston, we have three problems"

I see three problems with standard Rule of 40 analysis.

### Problem #1: Revenue and margin may not be equally important

Because the Rule of 40 adds revenue and margin together with no particular weighting of each (it's analogous to a simple average), in a subtle way it implies they are of equal importance.

This isn't necessarily true of course. An additional point of revenue growth could be worth more, the same, or less than an additional point of profitability.

**Fix #1: include revenue and margin separately in our regressions.**

### Problem #2: The long-run may look quite different

Even if revenue and margin were equally important on average, their relative importance could change over time.

Folks tend to run Rule of 40 regressions at a particular point in time, revealing how the market values growth and margin today, and today alone, saying nothing about how those relationships change over the long-run.

In a bull market, where growth is highly valued, our regressions fool us into thinking growth will be forever important. When the market subsequently turns, growth gets crushed, needlessly surprising many analysts and investors.

**Fix #2: pool data from multiple time periods.**

### Problem #3: Growth and margin aren't the only factors

Simple regressions inflate the importance of the Rule of 40 by misattributing the influence of other factors to growth and margin.

For example: Snowflake is a high flying company, both in terms of it's growth rate and revenue multiple. But Snowflake really is a special snowflake – it has a lot of other things going for it too, like the massive market for data warehouse solutions.

Thus, naively comparing Snowflake and other companies with smaller markets in the same regression will tend to overstate the value of revenue growth, leading to the false impression that, "if only we grow as fast as Snowflake, we'll be valued just as highly." This is wrong.

**Fix #3: control for the average influence of unobserved factors.**

## If it's broken, fix it

Let's implement each of these fixes one-by-one to see how they influence the results.

Here's the impact of revenue growth and free cash flow margin on valuation multiples with each of the three fixes implemented sequentially:  
![regs](https://nnamdi.net/content/images/2022/03/regs.png)

**First**, let's split up growth and margin to see how they separately impact valuations. To start, we pick a random date, June 30th 2021:

-   According to this simple analysis, a single additional percentage point of revenue growth increases valuation multiples by 3%, while an additional percentage point of free cash flow margin only adds 0.5%.
-   By this measure, growth is six times as important as margin.

**Second**, we'll pool together two years of trading data, excluding any companies that went public or got acquired during this time so we maintain a balanced sample throughout. The regression reveals the _average_ importance of growth and profitability over _the entire period_:

-   An additional point of revenue growth leads to a 2.7% higher revenue multiple (down from 3%) while an additional point of free cash flow margin yields 1.1% higher valuation (up from 0.5%).
-   Growth matters only ~2.5x as much as margin.

**Lastly**, the results still suffer from the fact that we're attempting to explain all the complexities of valuation with only two variables (the "Snowflake problem"), giving credit to growth and margin where credit isn't due. The final regression controls for the average impact of these unobserved factors ([fixed effects](https://www.youtube.com/watch?v=sFvV9b1cGFc) for the econometrics nerds):

-   1 point of either revenue growth or free cash flow margin increases valuation multiples by 1%.
-   **Growth and margin matter equally.**

So it turns out growth and margin are equally important after all!

Note: it may be much easier to increase growth or margin by one percentage point, so we have to be a little careful in saying they "matter equally."

Still, we've much more rigorously confirmed that growth and margin have similar effects on valuation. Very cool.

## How the rules have changed

Here's how the market value of growth and profitability have evolved over time:  
![fe](https://nnamdi.net/content/images/2022/03/fe.png)

We can see a number of interesting features:

-   The market value of profitability temporarily spiked in the early days of the pandemic but then fell significantly, bottoming out at almost zero in Q1 2021
-   Meanwhile, the market's thirst for growth surged from only 0.5% per percentage point of growth to 1.5% at the peak in late 2020
-   Beginning in early 2021, margin began a steady comeback, and **profitability is now as important as it was in the most uncertain days of the pandemic**
-   The value of growth fell in early 2021, recovered somewhat, and crashed even more spectacularly later in the year, **returning growth to its pre-pandemic value**

We can also plot the "spread" between the value of growth vs. margin, an interesting chart in its own right:  
![spread](https://nnamdi.net/content/images/2022/03/spread.png)

-   Growth and margin start off fairly close, with a slight market preference toward margin
-   After a brief period of uncertainty, the value of software growth relative to profitability roars back, peaking at a ~1% premium at the end of 2020
-   Again, we see the first blow to growth in the first half of 2021 and a second blow landing in late 2021
-   **Relative to profitability, growth is even less valuable than it was pre-COVID**

## Rewriting the rules

A few conclusions:

-   On average, **growth and profitability are equally impactful to valuation**, but one may win out over the other at various periods
-   Growth matters _much less_ than simple cross-sectional regressions imply
-   As of early 2022, **the market has turned decisively against growth in favor of profitability**

In a way, these results affirm the utility of the Rule of 40: growth and margin are similarly important. However it adds a wrinkle: the _degree_ of similarity varies over the market cycle.

Though the days of "growth at _all_ costs" are clearly over, it's still fair to ask: growth at _what_ cost? In other words, **how should companies trade off growth and margin?**

I'll leave that to a future post. 👀

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div> ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Schrodinger's Balance Sheet: When Equity Becomes a Liability]]>
                        </title>
                        <description>
                            <![CDATA[Preferred equity exists in a constant state of quantum superposition. It's neither equity nor debt, until it is.]]>
                        </description>
                        <link>https://whoisnnamdi.com/schrodingers-balance-sheet/</link>
                        <guid isPermaLink="false">schrodingers-balance-sheet</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Mon, 7 Feb 2022, 14:55:54 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512268990-header.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
[Preferred equity](https://www.svb.com/blogs/lewis-hower/startup-founders-should-know-preferred-stock) is synthetic leverage.

It combines an equity-like security with a debt-like security. Which one it is depends on the success of the business:

-   When things go well, preferred stock converts to standard common equity.
-   When things go poorly, preferred stock turns into debt.

In this way, preferred equity exists in a constant state of [**quantum superposition**](https://scienceexchange.caltech.edu/topics/quantum-science-explained/quantum-superposition). It's neither equity nor debt, until it is.

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div>

## Schrödinger's balance sheet

![Schrodingers_cat](https://nnamdi.net/content/images/2022/02/Schrodingers_cat.svg)

[Erwin Schrödinger](https://en.wikipedia.org/wiki/Erwin_Schr%C3%B6dinger) was a renowned physicist and one of the first to study the strange world of quantum theory. Among us mortals, he is most known for his famous thought experiment, eponymously called "[Schrödinger's cat](https://en.wikipedia.org/wiki/Schr%C3%B6dinger%27s_cat)." In it, he posited that a cat sitting in a box along with a flask of poison could be considered to be both dead \*and\* alive until we open the box to find out if the flask has shattered, killing the poor kitty.

**Preferred equity is like Schrödinger's cat.** Open the box: if a startup is very valuable, preferred stock all becomes normal equity. If the box contains a dead or slow growing startup that hasn't created much value, preferred stock becomes debt instead.

You can think about preferred equity as debt combined with a deeply out of the money [call option](https://www.investopedia.com/terms/c/calloption.asp). "[_Out of the money_](https://www.investopedia.com/terms/o/outofthemoney.asp)" is just a fancy way of saying the investor won't make much of a return unless the company gains significant value. If the company creates no value from that point forward, the option component is worthless, and the equity merely becomes debt. If the company creates significant value, the importance of the debt element wanes and the option component dominates the return profile.

Strangely enough, this is the exact _reverse_ of what you'd want to have happen as a founder.

This point has been made by [John Cochrane and others](https://www.gsb.stanford.edu/healthy-banking-system-goal) in the context of bank capitalization:

> Ensuring that banks are funded with significantly more equity should be a key element of effective bank regulatory reform. Much more equity funding would permit banks to perform all their useful functions and support growth without endangering the financial system by systemic fragility.

Things go very wrong when banks are overly-funded with debt, as debt tends not to "fail gracefully" the way equity does. In financial crises, banks "break" in a very _hard_ way due to the significant debt they carry, which must be paid back, necessitating government bailouts in the worst case scenarios. On the other hand, equity tends to break in a _soft_ way: if the company isn't worth anything, nothing is owed. Thus a sudden decline in the value of the company doesn't cause a severe panic.

Cochrane goes so far as to advocate that in times of financial distress, debt on the balance sheet of struggling banks should simply [convert to equity](https://static1.squarespace.com/static/5e6033a4ea02d801f37e15bb/t/5ee802f76e77d94cab19b981/1592263416691/across-the-great-divide-ch10.pdf) rather than be bailed out by the government. Notice what he's proposing – debt on the _upside_ and equity on the _downside_.

Likewise, as a founder, you'd want your capital structure to be equity when things go badly, making it a variable cost that declines gracefully as your business success declines, and you'd want it to be debt in the upside case, effectively acting as a fixed cost over which you get leverage.

In startup land, however, we do the opposite – _we convert equity to debt in times of distress._

## Spin up

Preferred equity is more valuable to investors than common equity, so they're willing to pay more for it, necessitating the issuance of fewer shares and leading to less dilution for the company. Thus, _relative to raising common equity_, preferred equity has a similar benefit as debt, in that it allows you to raise more capital with less dilution.

In this way, startups become **synthetically levered**. No debt appears on the balance sheet, yet the returns of the common equity (really anyone below the most senior preferred stock) get juiced by the lower cost of capital achieved via preferred equity.

The value of preferred stock can be inferred from its cost, i.e. its premium relative to common equity. The preferred equity premium is rarely laid out explicitly, but researchers at Stanford and the University of British Columbia estimate that these days [**preferred equity is 50% more valuable than common equity**](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3725240) and was even more valuable in the past:  
![Pasted-image-20211201152745](https://nnamdi.net/content/images/2022/02/Pasted-image-20211201152745.png)

> This paper develops the first option pricing model of venture capital-backed companies and their security values that incorporates the dilutive future financing rounds prevalent in the industry. Applying our model to 19,000 companies raising 37,000 rounds shows that preferred contractual features make **the most recently issued preferred shares worth on average 56% more than common shares** – [A Valuation Model of Venture Capital-Backed Companies with Multiple Financing Rounds](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3725240)

A quick illustration: you're a founder, and you know you need to raise roughly $60M to achieve a meaningful outcome. Because preferred equity is worth 50% more to investors than common, you could either raise $60M in preferred stock with some level of dilution, or raise it all in common stock for 50% more dilution. Let's say the common approach would dilute you by 60%. Then raising the same amount via preferred would only dilute you by 40%:

Preferred

Common

Capital raised

$60M

$60M

Total shares

100

100

Blended price/share

$1.50

$1.00

Shares purchased

40

60

**Dilution**

**40%**

**60%**

This obviously makes a massive difference in an upside scenario. Simply put, **you get to keep much, much more of your company.** When things work out, preferred equity has a higher cost to investors, and a lower cost to startups.

This is obviously great in a bull market. Cheap, bountiful leverage, available oftentimes for a fraction of the painful diligence associated with raising real debt.

The availability of preferred equity drives companies to raise more than they otherwise would. We'll look at such a scenario next.

## Spin down

When rates of return are sufficiently low, preferred equity begins to look eerily like debt. When are rates of return lowest? [When valuations are highest](https://whoisnnamdi.com/grow-valuation/):

> **High valuation multiples corresponds to lower returns**, and vice versa. After periods of frothy valuations, returns end up lower than expected, bringing lofty valuations multiples back down to reality – [Companies Rarely Grow Into Their Valuations](https://whoisnnamdi.com/grow-valuation/)

The mountains of preferred equity being layered into private companies follows the same behavioral vein typically seen during the peak of a market cycle, that is, **massive overuse and abuse of debt that juices returns.** This has predictably disastrous consequences when things go south.

As is common in finance, **the problems with debt reveal themselves only once it blows up.** In a downside scenario, the returns of common equity holders get crushed by the weight of the liquidation preference attached to preferred equity.

Remember, venture capital investors are willing to pay $1.50 for preferred equity granting _the same ownership_ they could get for $1 of common equity. In an upside scenario, they effectively pay $1.50 for $1's worth of equity. What happens in a downside scenario?

Let's return to our previous example. This time, the company chooses a certain, fixed level of dilution that it's willing to endure, 60%, and raises money up to that point. Since preferred stock can be issued at a premium, the company can raise $90M of it for the same dilution as $60M of common:

Preferred

Common

Capital raised

$90M

$60M

Total shares

100

100

Blended price/share

$1.50

$1.00

Shares purchased

60

60

**Dilution**

**60%**

**60%**

So far so good. The preferred approach gives you $30M more to put to work in the business. But let's say [you never quite achieve product-market fit](https://whoisnnamdi.com/product-market-fit-is-lindy/) (not uncommon, even among companies that have raised tens of millions of dollars) and you woefully underperform expectations. You never become a unicorn, and the business is sold for only $100M:

-   If you only raised common equity, the founders and employees walk away with $40M. Not as much as hoped for, but not bad!
-   If you raised preferred however, founders and employees walk away with only $10M, since the equity is now debt, and the investors must receive their "1x [liquidation preference](https://www.seedinvest.com/blog/angel-investing/liquidation-preferences)" (their "principal" effectively). Brutal.

This may offer a clue into why the hedge funds of the world have moved so swiftly into venture. As sophisticated, multi-asset investors, they are quite used to lower-returning, low-risk credit investing combined with potentially high-returning, high-risk derivatives. Rather than "fish out of water", as they are often portrayed by traditional venture investors, could they in fact be quite shrewd [(tiger)sharks](https://randle.substack.com/p/playing-different-games)?

The rabbit hole goes further. Define "overvaluation" as the premium at which preferred stock is valued relative to common equity. Analysis shows **overvaluation predicts poor exit outcomes**, measured in terms of (1) exit value and (2) the probability of going public or being bought out (note: [low R^2 implies exit outcomes are difficult to predict](https://blog.minitab.com/en/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit), not that the relationships aren't statistically significant):  
![Pasted-image-20220206175842](https://nnamdi.net/content/images/2022/02/Pasted-image-20220206175842.png)

> … **overvaluation predicts low exit outcomes**… The extent of this overstatement predicts poor subsequent performance – [A Valuation Model of Venture Capital-Backed Companies with Multiple Financing Rounds](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3725240)

**Abuse of preferred equity predicts worse outcomes, fewer exits, and lower returns.**

Synthetic leverage, as with all forms of financial leverage, makes bad situations _even worse_ **AND** _more likely_.

## Superposition

Now, let's finally explore the quirky quantum world of superposition.

Preferred equity starts as neither debt nor equity: you have to "open the box" to see if the company is successful. Unfortunately, actions that make your upside better (raising a ton of money and putting it to work in the business) also make your downside worse. Things that look smart in the upside scenario (raising a "war chest") look disastrous in retrospect on the downside. Gain and pain are entangled; success and failure are superimposed on one another.

**Perversely, the Silicon Valley ecosystem creates more synthetic debt (i.e. preferred equity) exactly as returns fall:**

1.  Valuations rise
2.  Companies take advantage by issuing more preferred stock
3.  Returns fall due to inflated valuations
4.  Preferred stock converges toward debt

Let's return to financial crises. The capital stack of a company is very similar to that of a home. From the perspective of the founder, common stock is the "equity" in the home and preferred stock is the debt or mortgage. If value of home falls below the principal on the mortgage, the homeowner is "[underwater](https://www.investopedia.com/terms/u/underwater-mortgage.asp)", destroying the value of their equity.

Likewise, when a startup's valuation falls too low, the value comes out of the common shareholders; they get squeezed first. This is most severe in a down round where [anti-dilution provisions](https://www.dlapiperaccelerate.com/knowledge/2017/what-is-anti-dilution-and-why-does-it-matter-to-me-as-a-company-founder.html) come into play. This is the Silicon Valley version of a good ol' Wall Street bailout: **the debt (preferred) holders get saved, and the equity holders get wrecked.**

What tends to happen when homeowners go underwater? We know this from history – [they abandon the home](https://www.abi.org/newsroom/chart-of-the-day/serious-delinquency-rates-on-single-family-mortgages-decreased-slightly-in):  
![Pasted-image-20220130120610](https://nnamdi.net/content/images/2022/02/Pasted-image-20220130120610.png)

Now I ask: **what might founders and employees do once they are underwater on the preferred equity they've raised?**

Equity value motivates founders and employees. If there's no hope for their equity to be worth anything, could we see discouraged founders and employees abandon ship in droves? Even if they don't leave the company entirely, could they effectively go "delinquent" and "check out," significantly reducing their company-focused effort and output?

In startups, we see acceleration of value impairment: as things get worse, they get even worse. Not a great underwriting scenario for a debt-like instrument huh?

Poor performance alone doesn't create a "[zombie startup](http://www.daniellemorrill.com/2013/03/zombie-startups/)." Zombification requires _the total evisceration of team vitality and morale_. Thus, Schrödinger's cat becomes Schrödinger's zombie:

> "… it's very easy to get into this 'zombie mode,' where your startup is **neither truly succeeding nor dying**… This is actually worse than failing." – [Drew Houston, Founder of Dropbox](https://www.businessinsider.com/drew-houston-dropbox-startup-advice-2018-12)

## Schrodinger's Pandora's box

Imagine you're selling your home and the buyer offers to pay you 50% above the asking price as long as they can get "preferred equity" in your home. You'd be ecstatic about the richer sale price, but you'd also be really curious to know what in the world "preferred equity" is and why it's so valuable to them!

Likewise, as a founder or startup employee, even if corporate finance isn't your favorite topic, you should wonder why investors value preferred equity so highly.

What could investors be so concerned about that they would willingly overpay by 50%? And if they're so concerned, do you think, maybe, you should be too?

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div> ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Introducing a New and Improved SaaS Metric: Weighted ACV]]>
                        </title>
                        <description>
                            <![CDATA[ACV isn't as useful a concept as people think. We need a different SaaS monetization metric.]]>
                        </description>
                        <link>https://whoisnnamdi.com/weighted-acv/</link>
                        <guid isPermaLink="false">weighted-acv</guid>
                        <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Tue, 11 Jan 2022, 18:23:10 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512267660-weighted_ACV.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
ACV, or average/annual contract value, isn't as useful a concept as people think.

Though it's easy to forget, ACV is a _customer-weighted_ metric: it tells us something about the customers of the business. However, despite its name, it doesn't tell us much about the _revenue profile_ of the business, which is arguably more important.

Further, the standard ACV metric obscures valuable information about the skewness of monetization / revenue. Thus, we can't use it to compare companies with differing customer concentration.

We need a different SaaS monetization metric.

I present to you: **weighted ACV (WACV)**.

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div>

## Customer-centric or dollar-centric?

In [Enterprise Software Monetization is Fat-Tailed](https://whoisnnamdi.com/software-fat-tailed/), I talked about the follies of focusing on simple averages, especially in enterprise software:

> In enterprise software, the "average" customer is a meaningless concept.
> 
> Paying too much attention to the "average" customer leads many founders and investors astray.

[Average/annual contract value](https://blog.hubspot.com/sales/annual-contract-value-acv), or ACV, is one such misleading metric.

$$\text{ACV} = \frac{\text{Total Annual Contract Value}}{\text{Total Customers}}$$

ACV tells you something about the typical _customer_. Products with small ACVs tend to serve smaller, more fickle customers with a high probability of churning, like small businesses and startups. High ACV products address the needs of larger companies with more mission critical use cases, leading to less churn.

However, while ACV tells you something about how the typical customer might behave, it doesn't tell you much about how the typical _dollar of revenue_ behaves. If 70% of the revenue comes from 20% of the customers, then for the purposes of analyzing revenues at a point in time, it's really only that subset of customers that matter. Such [extreme concentration](https://whoisnnamdi.com/software-fat-tailed/#the-evidence) is not unusual for public software companies:  
![Pasted image 20211113192246](https://nnamdi.net/content/images/2020/10/share.png)

> The implied top-20% and top-1% revenue concentration are quite large for most companies (blue = top 20%, red = top 1%)
> 
> So, **the top 20% typically represent ~70% of revenue**, while **the top 1% represent ~40%**. Not quite Pareto 80/20, but pretty close!

This distinction between **customer behavior** and **revenue behavior** is critical, as the behavior of the typical dollar of revenue is much more informative about the quality of revenue and the likelihood that revenue will be retained or expanded. **For companies with high revenue concentration, simple averages are largely useless.**

> This is largely true to this day - for many public enterprise SAAS, a very small # of customers contributes a huge chunk of total rev.  
> e.g. ~0.95% of [$NET](https://twitter.com/search?q=%24NET&src=ctag&ref_src=twsrc%5Etfw) paying customers rep >50% of rev in Q3/21  
>   
> So why do some analysts still use metrics like ARPU/AARPU for enterprise saas? [pic.twitter.com/WXSpWJYfNA](https://t.co/WXSpWJYfNA)
> 
> — Masterly Inactive (@masterly\_in) [December 30, 2021](https://twitter.com/masterly_in/status/1476610484371660812?ref_src=twsrc%5Etfw)

Rather than the exception, high concentration is the _norm_ in certain verticals (e.g. cloud infrastructure) or pricing models (consumption/pay-as-you-go) where a "customer" can be as small as tens of dollars per month:

> Interestingly, many companies tied to infrastructure in some way like Datadog, Fastly, and Twilio... have 80/20 monetization distributions, at least in some years.

Unfortunately, the standard ACV metric is incomparable across companies with vastly different customer concentration. A company with many small or "free tier" customers will have an artificially low ACV metric, even if contract sizes are reasonably high among its paying customers. You can't naively compare such a skewed company to another business where every customer is small.

This is why you see public software companies with tons of bottom up but low-paying usage report ACV or other customer metrics including only the customers larger than some nominal spending level, like $5,000 (screenshot from the [GitLab S-1](https://sec.report/Document/0001628280-21-019932/gitlab-sx1a2.htm)):  
![Pasted-image-20211114131342](https://nnamdi.net/content/images/2022/01/Pasted-image-20211114131342.png)

Using arbitrary thresholds is a clunky way of dealing with this problem, as companies can pick any value they want. **ACV calculations with different thresholds are effectively incomparable.**

## Weighing in

A customer-weighed metric helps you understand customers. ACV is a _customer_ or _equal-weighted_ metric, which is to say every customer is weighted equally. So if there are 1000 customers, the contract value of each customer is "weighted" or multiplied by 1/1000 and then summed up. Because each _customer_ is weighted equally, the resulting metric tells us something about the typical _customer_. Thus we can rewrite ACV as:

$$\text{ACV} = \sum\_n \text{% of Total Customers}\_n \times \text{Contract Value}\_n$$

**However, if you want to understand revenue, you need to use a revenue-weighted metric.**

Just like the [market capitalization-weighting](https://www.investopedia.com/terms/c/capitalizationweightedindex.asp) of the S&P 500 index signals the changing value of the overall equity market (but not of the individual _companies_ in the index), a revenue or contract value-weighted metric tells us much about the overall revenue of the business (rather than the individual customers).

Introducing: **weighted ACV (WACV)**.

Revenue-weighting the revenue each customer generates might sound like double counting, but we need to weight individual customer revenues by _something_. Nothing limits us to equal-weighting. Revenue-weighting is not only perfectly reasonable, it's the more appropriate methodology when analyzing quality of revenues.

A perfect example is [net dollar retention](https://whoisnnamdi.com/high-retention-high-volatility/), a common tool for understanding the quality and behavior of software revenues:

-   Believe it or not, _net dollar retention is also a revenue-weighted metric_: it weights the individual dollar retention of each customer by the revenue that customer represents. That's not how the calculation is typically conceptualized, but that is the underlying math (grab some data and prove this to yourself)
-   Thus, larger customers matter more for net dollar retention; the churning of a tiny customer has infinitesimal impact.

To calculate weighted ACV:

-   Take the contract value of each customer and multiply it by the proportion of revenue that customer represents. A customer representing 1% of revenue gets a 1/100 weight, a customer representing 3% of revenue receives a weight of 3/100, and so on.
-   Then sum all those numbers (i.e. SUMPRODUCT, for all you spreadsheet monkeys out there)

$$\text{Weighted ACV} = \sum\_n \text{% of Total Contract Value}\_n \times \text{Contract Value}\_n$$

**Weighted ACV tells you where to look if you want to best understand the revenue of the business.** For example, a WACV of $25,000 tells us roughly that the typical dollar of revenue is generated by a $25,000 customer. Thus, revenue retention/churn and other revenue dynamics of the business will tend toward what we'd expect for those kinds of customers. Revenue-weighting is especially helpful if the company in question has a large number of customers spending little or zero, contaminating the equal-weighted ACV calculation.

Meanwhile, changes in WACV correspond to changing monetization among the largest chunk of the revenue base. Customers with the biggest revenue impact will have the largest influence on WACV, as with net dollar retention.

If we think about a company as a sort of physical object, its "center of mass" is concentrated at its WACV. Logically, if we want to do physics on this thing or model its movement, what matters most is its center of mass (its weighted ACV), not necessarily its literal "center" (equal-weighted ACV). That's where revenue is most "densely" concentrated.

This concept flips standard monetization logic on its head. Instead of saying "the average customer generates X revenue," as if all revenue comes from a single type of customer, WACV says "Y-size customers generate the average dollar of revenue," respecting the fact that certain customers are disproportionately impactful. ACV asks "what's the expected value of a randomly selected _customer_" while WACV asks "what's the expected contract size associated with a randomly selected _dollar of revenue_."

As I'll demonstrate in a second, _this is really powerful_. In a single number, we capture much more information than the equal-weighted ACV metric by accounting for the skew and concentration of the customer base. In statistics parlance, WACV gives us some sense of the [variance](https://www.scribbr.com/statistics/variance/) of revenues across customers, rather than merely the mean.

Note: this essay focuses on enterprise SaaS monetization, but you can apply the same concept to B2C and other models. E.g. Average Revenue Per User (ARPU) becomes Weighted ARPU (WARPU), and so on.

### An example worth its weight in revenue

Two businesses could have the same equal-weighted ACV but wildly different weighted ACVs.

Here's an example to make this vivid.

Imagine two, $50K revenue companies with five customers each, equivalent equal-weighted ACVs, but very different revenue distributions. Company 1 has five equal-sized customers, while Company 2 has a fairly typical skewed distribution, with 20% of customers representing 80% of revenue:

Company 1

Company 2

Customer 1

$10,000

$40,000

Customer 2

$10,000

$2,500

Customer 3

$10,000

$2,500

Customer 4

$10,000

$2,500

Customer 5

$10,000

$2,500

**ACV**

**$10,000**

**$10,000**

**Weighted ACV**

**$10,000**

**$32,500**

**Notice how sensitive WACV is to the more skewed revenue distribution of the second company.** It's telling us Company 2's large $40K customer is really dictating the revenue dynamics of the company. For Company 2, the typical dollar of revenue will behave like it's coming from a $32.5K customer, not a $10K customer. The standard ACV metric totally misses this.

Now that we've seen how WACV helps us at a single point in time, let's see what happens to the metric _over time_. Next month, both companies land a large, $100K deal with a new customer:

Company 1

Company 2

Customer 1

$10,000

$40,000

Customer 2

$10,000

$2,500

Customer 3

$10,000

$2,500

Customer 4

$10,000

$2,500

Customer 5

$10,000

$2,500

Customer 6

$100,000

$100,000

**ACV**

**$25,000**

**$25,000**

**Weighted ACV**

**$70,000**

**$77,500**

Both metrics increase, but weighted ACV increases by a lot more, reflecting the dominance of the new large customer within the revenue base. The standard ACV calculation unwittingly discounts the effect of large customers, muting their impact.

Again, the weighted version of ACV correctly points us towards the largest customers. The "typical" dollar of revenue for both companies will now behave like it's coming from a roughly $70-80K customer, _not_ a $25K customer.

Lastly, the fact that WACV across the companies has converged suggests their revenue concentration is now much more similar, which is exactly right.

### VICs: Very Important Customers

One last trick.

Just like dividing total revenue or contract value by ACV yields the total customer count, dividing by weighted ACV tells us something about the customer base. The calculation tells us, on a revenue basis, **how many customers really matter**. In other words, what minority of customers effectively determines overall revenue?

Let's call them **very important customers (VICs)**:

$$\text{Very Important Customers} = \text{Total Contract Value } / \text{ Weighted ACV} $$

It turns out, this will be a small fraction of the overall customer count for the typical enterprise software company.

Returning to the first example, total revenue was $50K, while weighted ACV was $10K for Company 1 and $32.5K for Company 2. Thus, Company 1 has 5 VICs while Company 2 has 1.5. This tells us that while Company 1 has five relevant and discernibly important customers, Company 2 only has one or two. This accords with the data – in terms of revenue risk or quality of revenue, Company 2 is effectively a one (point five) trick pony:

Company 1

Company 2

Total Contract Value

$50,000

$50,000

Weighted ACV

$10,000

$32,500

**Very Important Customers**

**5**

**1.54**

In the second example, total revenue is now $150K, with weighted ACV of $70K for Company 1 and $77.5K for Company 2. Some quick math yields ~2 very important customers for each company, reflecting their respective revenue concentration. Said differently, you can summarize the entire revenue profile of each company by looking at the two largest customers:

Company 1

Company 2

Total Contract Value

$150,000

$150,000

Weighted ACV

$70,000

$77,500

**Very Important Customers**

**2.14**

**1.94**

Frankly, I find this much more representative of the volatility of revenue. These companies have serious concentration risk, and from a diversification perspective really only have two customers each.

Public software companies have similar concentration, so the same logic applies: very important customers will only be 30-40% of the overall customer base.

With WACV and VICs, founders with high revenue concentration can now rigorously quantify the number and magnitude of meaningful customer relationships they have while avoiding awkward conversations that go something like, "well... we have X total customers, but only Y many are paying us more than Z per year, so if you remove those then our business is really..."

### Conclusion

Are there any downsides to weighted ACV?

No, none at all.

Just kidding! The negatives of WACV mirror those of other size-weighting schemes like the S&P 500: it's extremely sensitive to idiosyncratic fluctuations in your largest customers that may not be representative of the broader customer base or long-lasting in nature.

These are fair critiques. However, all can be easily addressed by calculating **both** weighted and equal-weighted ACV. Coincidently, the difference between the two is a nice measure of customer concentration and the [fat-tailedness of monetization](https://whoisnnamdi.com/software-fat-tailed/): the further apart they are, the more revenue is concentrated among a few customers, and vice versa:

$$\text{Weighted ACV} - \text{Equal-weighted ACV} \approx \text{Customer Concentration}$$

In fact, the main disadvantage of revenue-weighting is that it requires knowing the revenue or contract value of **each and every customer**. So an outsider can't calculate the metric; it must be reported by the company. You can often approximate it with just data of the top 30-40% of customers, but that won't be perfect.

If you _do_ have enough information to calculate weighted ACV, you should _always_ do it. WACV contains more information than the standard ACV metric, and by definition you can always calculate the latter if you can calculate the former.

_If you found this helpful, need help with the calculation, or end up implementing this in your analytics, do let me know!_

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div> ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Companies Rarely Grow Into Their Valuations]]>
                        </title>
                        <description>
                            <![CDATA[Companies don't catch up to their valuations; their valuations catch up to them.]]>
                        </description>
                        <link>https://whoisnnamdi.com/grow-valuation/</link>
                        <guid isPermaLink="false">grow-valuation</guid>
                        <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Tue, 9 Nov 2021, 16:28:00 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512264775-header-2.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
Investors often justify high valuation multiples with the claim that the companies "will grow into their valuations."

It turns out **this rarely happens**.

Most often, companies don't grow fast enough to compensate for rising valuation multiples. Instead, high valuations today imply slower value appreciation in the future, i.e. lower returns.

Thus, companies don't catch up to their valuations; **their valuations catch up to _them_.**

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div>

### Expanding on multiple expansion

Investors can justify rising [valuation multiples](https://en.wikipedia.org/wiki/Valuation_using_multiples) with a belief that:

-   growth will be better than expected ("they will grow into their valuation"),
-   returns will be lower than expected (the valuation will appreciate more slowly), or
-   multiples will be even higher in the future ("someone else will pay more for this thing")

If growth accelerates, valuation multiples fall due to the denominator growing faster than the numerator (revenue or cash flow grows faster than valuation).

On the other hand, if future returns decline, the numerator grows more slowly (valuations grow slower than revenue or cash flow), also causing multiples to fall. Either outcome leads to declining valuation multiples:

$$\frac{\text{Valuation} \uparrow}{\text{Revenue or Cash Flow} \uparrow \uparrow \uparrow} \rightarrow \text{Declining Valuation Multiples}$$

Think of the "multiples are higher today because multiples will be higher tomorrow" justification as a sort of "bubble mentality" or "[greater fools](https://en.wikipedia.org/wiki/Greater_fool_theory)" explanation. Someone else will pay more for this later, so I'm OK paying more today. This of course can't go on for long: bubbles eventually pop; manias eventually subside. So, here too, multiples return to "normal."

These three explanations for valuation multiples are collectively called the [Campbell-Shiller Decomposition](https://www.youtube.com/watch?v=CIMMkDWiqRE), originally conceived in the late 1980s. It lets us cleanly account for changing valuation multiples with changing beliefs about growth, returns, and future multiples. We can even make an equation out of it (\\(\\Delta\\) = Change):  
$$\Delta \text{Valuation Multiples Today} = \Delta \text{Future Growth} - \Delta\text{Future Returns} + \Delta\text{Future Valuation Multiples}$$

Therefore, higher multiples today imply (1) faster growth in the future, (2) lower future returns, or (3) even higher multiples going forward.

Importantly, this works for both forward and backward-looking analysis: changing _expectations_ of future growth, returns, or multiples affect valuations today and changing valuation multiples today also _forecast_ future growth, returns, and multiples. In other words, investors' **subjective** expectations about the future influence valuations today and those valuations also predict the **objective** future itself.

Regardless of the specific cause, valuation multiples eventually [mean-revert](https://www.investopedia.com/terms/m/meanreversion.asp); subjective expectations eventually collide with objective reality.

### Subjective expectations

Let's start with _subjective expectations_. When [researchers](https://onlinelibrary.wiley.com/doi/10.1111/jofi.13016) decompose variation in price-earnings (PE) ratios into changing expectations about cash flows, discount rates (e.g. expected returns), and future PE ratios using the Campbell-Shiller Decomposition, they find fluctuating cash flow expectations (CF in the table below) explain more than 90% of all movement in valuations ratios, while expected returns (DR) explain little:

![Pasted-image-20211105145313](https://nnamdi.net/content/images/2021/11/Pasted-image-20211105145313.png)

> “In the 2003 to 2015 sample, **one-year subjective earnings growth expectations account for virtually all (94%) price-earnings ratio movements.**”  
> ...  
> “**high prices can largely be accounted for by high expectations for future cash flows.** Cash flow growth expectations are volatile and significantly correlated with prices. In comparison, return expectations are much less volatile and positively correlate with prices, meaning that **high prices cannot come from low discount rates**” — [Subjective Cash Flow and Discount Rate Expectations](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3841696)

[Another paper](https://www.ssrn.com/abstract=3740792) breaks down the expected returns vs growth dynamic between different classes of market participants, finding that sell-side analysts, institutional investors, and corporate CFOs all use growth expectations to explain fluctuating valuations (\\(g\_t\\) in the table below) rather than expected returns (\\(\\mu\_t\\)):

![Pasted-image-20211105145501](https://nnamdi.net/content/images/2021/11/Pasted-image-20211105145501.png)

> “**both Wall Street and Main Street believe... the expected cash flow process is the main economic force driving asset price variations**”  
> ...  
> “**cash flow expectations alone explain 99% of the price-dividend ratio variation**, while return expectations only explain about 10%.” — [Subjective Return Expectations](https://www.ssrn.com/abstract=3740792)

Investors see valuations move and ascribe these gyrations to changing expectations for future business growth. But does the growth ever materialize?

### Objective reality

**Not really:**

> “the variance decomposition results reveal that **all three types of investors seem to overestimate how much cash flow variation contributes to the variation in asset prices** when compared to objective measures of return expectations, which show that **discount rate variation should contribute the most to asset price variation**” — [Subjective Return Expectations](https://www.ssrn.com/abstract=3740792)

Now we turn to _objective reality_. The chart below plots expected (solid blue) and realized (dotted green) 1-year earnings growth of the S&P 500 over the last 40 years. Expected earnings growth consistently exceeds actual growth, implying investors tend to overestimate earnings growth. Notice as well how expected earnings growth tend to show "upside volatility", whereas actual earnings show more downside volatility.

![Pasted-image-20211031150624](https://nnamdi.net/content/images/2021/11/Pasted-image-20211031150624.png)

> “**Earnings growth expectations typically fail to predict the change in earnings during busts**, but they do tend to predict recoveries and track future earnings growth reasonably well during normal times.” — [Subjective Cash Flow and Discount Rate Expectations](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3841696)

Investors rarely predict poor earnings growth in advance. They often correctly predict strong earnings growth after recessions, but that's kind of cheating isn't it? Quick bounce-backs follow recessions almost universally.

When financial researchers do look-backs, they find that **high valuation multiples precede low returns**. In fact, they literally _predict_ lower returns. Per finance legend [John Cochrane](https://www.johnhcochrane.com/)'s [2011 Presidential Address at the American Finance Association](https://www.youtube.com/watch?v=ZDsOiftolUI) (apologies for the pixelated 360p screenshot, YouTube was less generous back then):

![Pasted-image-20211105153730](https://nnamdi.net/content/images/2021/11/Pasted-image-20211105153730.png)

> "These regressions are telling us that **all variation in price-dividend ratios, all of that volatility, corresponds to variations in expected returns.** None of it corresponds to variation in expected dividend growth and none to rational bubbles or prices that are ever higher."  
> ...  
> "**The valuation ratio translates one to one to expected returns** and doesn't forecast the cash flows or price change that we might have expected." — John Cochrane

High valuation multiples correspond to lower returns, and vice versa. After periods of frothy valuations, returns end up lower than expected, bringing lofty valuation multiples back down to reality.

Despite this sobering reality, **investors are extremely stubborn about future return expectations**, even in the face of rising prices. Undeterred by nose-bleed valuations, they continue to expect roughly the same return in all periods, leading actual returns (dotted green) to be much more volatile than expected returns (solid blue), which look nearly constant in comparison (right-hand chart):

![Pasted-image-20211031153941](https://nnamdi.net/content/images/2021/11/Pasted-image-20211031153941.png)

Equity investors always expect the same rate of return, regardless of how high or low valuations multiples are today, which doesn't make a ton of sense.

### Waiting for Godot growth

The data from the public markets is clear: **valuations rise, returns fall.**

Investors don't like to think this way, and thus they continue to expect the same return in all periods, leading to inevitable disappointment. Meanwhile, they justify their stubbornness about returns by convincing themselves that growth will save the day. It typically does not.

This data only covers public equity investors. It'd be fair to point out that growth among startups is much more variable than that for public companies, so much so that variation in startup growth could explain much more valuation variation.

That said, I find it hard to believe the future for startups could brighten so dramatically, practically overnight:

![Pasted-image-20211013191138](https://nnamdi.net/content/images/2021/11/Pasted-image-20211013191138.png)

If anything, the high variability of startup growth may be a cognitive trap, seducing us to think that herculean growth alone can shoulder weighty valuations.

I think it's past time we in the venture world get serious about the valuation craze. Do we really think the future is so much brighter? Are we merely accepting lower future returns, and if so, do our LPs know this? Is it all just a bubble?

Investors rely too much on growth to justify current valuations and underestimate the risk that higher multiples portend poor returns. Investors like to think companies will grow into their valuations, but more often than not, the stock simply underperforms.

The burden is on venture investors to prove there's something special about the asset class that will help it escape the fateful relationship between valuations and returns we see in public equities.

Until then, growth is just a cop out.

> 1/ How investors justify high valuation multiples: "they will grow into their valuation"  
>   
> Pay it, and growth will come  
>   
> This rarely happens  
>   
> Companies don't catch up to their valuations; their valuations catch up to \*them\* 🤯 [pic.twitter.com/PRbaj8xskc](https://t.co/PRbaj8xskc)
> 
> — Nnamdi Iregbulem (@whoisnnamdi) [November 11, 2021](https://twitter.com/whoisnnamdi/status/1458872942960398346?ref_src=twsrc%5Etfw)

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div> ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[WebAssembly-ing the Pieces: Vectorized’s Data Policy Engine]]>
                        </title>
                        <description>
                            <![CDATA[Rather than ship data to code, which is expensive and latency-prone, why not ship code to the data?]]>
                        </description>
                        <link>https://whoisnnamdi.com/data-policy-engine/</link>
                        <guid isPermaLink="false">data-policy-engine</guid>
                        <category>
                                        <![CDATA[ Developers ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Sat, 9 Oct 2021, 13:00:00 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512265840-header-1.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
In [Why Developers Love Redpanda](https://whoisnnamdi.com/why-developers-love-redpanda/), I talked about the exciting developments around WebAssembly and how [Vectorized](https://vectorized.io/) is leveraging the technology to build it’s Intelligent Data API, [Redpanda](https://github.com/vectorizedio/redpanda):

> [_WebAssembly_](https://webassembly.org/)_, or WASM, is one of the most exciting up-and-coming technologies in software development today. WebAssembly lets developers write code in any major language, translate that code to the compact WASM format, and run it on the web with the high performance of a native application._  
>   
> _Redpanda is one of the first infrastructure technologies to take advantage of WASM, enabling developers to “write and edit code in their favorite programming language to perform one-shot transformations, like guaranteeing GDPR compliance by removing personal information or to provide filtering and simple aggregation functions.”_  
>   
> _JavaScript, Python, Rust, Go — anything that compiles to WebAssembly (basically everything at this point) can be used to transform data. Again the key is accessibility — inline WASM transforms in Redpanda represent just that._

Ever since partnering with Vectorized, we at [Lightspeed](https://lsvp.com/) knew that Redpanda was much more than a streaming engine. Redpanda truly is an Intelligent Data API, and it’s [Data Policy Engine](https://vectorized.io/blog/wasm-architecture/) unlocks one more piece of that puzzle.

Today, I want to expand on this and talk a bit more about the exciting developments coming down the pipe at Vectorized.

Here’s the big idea: rather than ship data to code, which is expensive and latency-prone, why not ship code to the data?

## The keystone problem in streaming

Streaming has seen a ton of innovation over the last few years, but one area that remains painful today is stream processing. While a full explanation of these difficulties probably warrants its own article, I’ll cover the most salient points here.

The great thing about streaming is the immutable message queue. The not so good thing about streaming is, also, the immutable message queue.

What do I mean by this? The immutability of the message queue is great because it guarantees that data produced to the topic can be replicated one-to-one by simply consuming the whole queue from the start. This lets you effectively “replay” data like a VHS tape and also gives you built-in auditing, since you know the ordering hasn’t changed. It also decouples producers of data from the consumers of that data, letting the two operate independently without significant coordination work.

However, there are some problems. Immutability not only means you \*can\* consume the data in the same order it was produced — it means you must (at least as implemented today in the Kafka-API world). Further, immutability means one must consume at least as much as was produced, often many times as much, leading to additional costs in the form of network traffic and other resources. These issues can be avoided to some extent by being very diligent in setting up your architecture __a priori__, careful in segregating your use cases, etc., but few do this in practice.

In summary, the things that make streaming great also make stream processing complex and difficult.

## Shipping code to data

Here’s where WebAssembly comes in. WebAssembly lets us do a small amount of extra compute work to save us a ton of cost in the form of network utilization and latency. Mission critical streams can stay as large as they want without saturating the network.

We’re excited about the power and promise of WebAssembly, and we think it has the potential to do for server-side compute today what JavaScript did for the web in the late 90s. Like JavaScript enabled the shipping of code to the user’s client, WebAssembly lets engineers ship code to the storage engine.

Here’s how.

The Redpanda Data Policies Engine simultaneously extends the Kafka API and what you can do with server-side compute. It’s a server-side compute API for Redpanda that’s compatible with all existing Kafka tooling and the broader ecosystem, including Spark Streaming, Kafka Streams, Flink, and more.

With Data Policies, all your favorite tools can benefit from server-side WebAssembly filters, allowing for easy and simple data scrubbing, cleaning, filtering, normalization and more. As a runtime, WebAssembly allows code to execute inside Redpanda, allowing code to be injected into an active cluster at runtime.

![](/https://nnamdi.net/content/images/2021/11/classical_scrubbing_vectorized_redpanda_graph.png)

Redpanda Transforms enable extremely low-latency transformations by avoiding the “data ping-pong” that happens in many systems and stream processors today, where data gets sent back and forth between storage and compute, even for very minor operations. Further, Redpanda Transform is built on the [V8 engine](https://v8.dev/), the high-performance JavaScript and WebAssembly engine that also powers Chrome and NodeJS.

Transforms in Redpanda are effectively a stateless [predicate pushdown](https://medium.com/microsoftazure/data-at-scale-learn-how-predicate-pushdown-will-save-you-money-7063b80878d7). In simple terms, that means performance is massively improved by “pushing down” code to the data, only touching the relevant data (and ignoring the rest), cutting down on wasteful network traffic, I/O, and latency.

![](/https://nnamdi.net/content/images/2021/11/galaxy.png)

Redpanda Transforms are also completely auditable. The controller can report on which specific piece of code is touching which specific piece of data, on which machines, at which topic offsets.

## As easy as one, two, THREE

![](/https://nnamdi.net/content/images/2021/11/Uppercase_filter.png)

Few things are simple in data engineering, but, as usual, Vectorized is changing the game. For all the cool things happening under the hood, getting started with Data Policies is shockingly easy.

First, write your policy/transform in a WebAssembly compatible language (read: everything). To make this even easier, Redpanda provides native plugins for inline transforms. In the end, you end up with something that looks like fairly standard (JavaScript in this case) code. Here’s a simple proof of concept: transforming all lowercase alphabetic characters in a record to uppercase:

```javascript
import { InlineTransform } from “@vectorizedio/InlineTransform”;
const transform = new InlineTransform();
transform.topics([{“input”: “lowercase”, “output”: ”uppercase”}]):
…
const uppercase = async (record) => {
    const newRecord = {
        ...record,
        value: record.value.map((char) => {
            return char.toUpperCase();
        }),
    };
    return newRecord;
}
```

Next, apply the policy to a Kafka topic. Again this is incredibly straightforward. All it takes is a single terminal command:

```bash
> bin/kafka-topics.sh \
— alter — topic my_topic_name \
— config x-data-policy={…}
(redpanda has a tool called `rpk` that is similar to kafka-topics.sh)
```

Now, Redpanda handles the rest.

-   Upon a TCP connection to consume from the topic, Redpanda initiates a [V8 context](https://v8docs.nodesource.com/node-0.8/df/d69/classv8_1_1_context.html)
-   Bytes are read from disk
-   Payload is transformed and re-checksumed (ensures Kafka protocol remains unchanged)
-   Redpanda returns the transformed record

![](/https://nnamdi.net/content/images/2021/11/Uppercase_filter_2.png)

Pretty slick.

## Simplicity is the ultimate sophistication

I’ve [talked before](https://medium.com/lightspeed-venture-partners/why-developers-love-redpanda-30bf2f3b8231) about Vectorized’s deep understanding and appreciation for the power of simplicity:

> ___Redpanda abstracts away the complexity that often prevents the typical developer from adopting real-time streaming.___

The Data Policy Engine continues that tradition.

This was just an example use case, but they only get more interesting from here. GDPR compliance via masking rules. Encryption at-rest, on-disk. All at runtime, with near-native performance.

Curious to learn more after this quick introduction? [Alex Gallego](https://twitter.com/emaxerrno/), founder of Vectorized, gave a sneak preview of the Data Policy Engine at this year’s Kafka Summit, which you can check out here:

> I'm about to show a sneak preview of on the most anticipated features by the community here, as a sponsored talk.  
>   
> tl;dr - take an \*unmodified spark app\* and use a push-down predicate in [#wasm](https://twitter.com/hashtag/wasm?src=hash&ref_src=twsrc%5Etfw) 🤯🤯🤯  
>   
> Demo at the end.  
> [#redpanda](https://twitter.com/hashtag/redpanda?src=hash&ref_src=twsrc%5Etfw) [#kafka](https://twitter.com/hashtag/kafka?src=hash&ref_src=twsrc%5Etfw) [#KafkaSummit](https://twitter.com/hashtag/KafkaSummit?src=hash&ref_src=twsrc%5Etfw) [https://t.co/NiL2SNHrvq](https://t.co/NiL2SNHrvq)
> 
> — 🕺💃🤟 Alexander Gallego (@emaxerrno) [September 15, 2021](https://twitter.com/emaxerrno/status/1438214386124873728?ref_src=twsrc%5Etfw) ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[You Can't Eat Relative Growth]]>
                        </title>
                        <description>
                            <![CDATA[In startup land, we talk way too much about relative growth. We'd do better to ground our thinking in absolute growth.]]>
                        </description>
                        <link>https://whoisnnamdi.com/relative-growth/</link>
                        <guid isPermaLink="false">relative-growth</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Thu, 23 Sept 2021, 09:07:29 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512261584-Pasted-image-20210922213513.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
In 2006, Howard Marks, co-founder of Oaktree Capital Management, published a memo titled "[You Can't Eat IRR](https://www.oaktreecapital.com/docs/default-source/memos/2006-07-12-you-cant-eat-irr.pdf)." In doing so, he popularized the idea that [IRR](https://www.investopedia.com/terms/i/irr.asp), or internal rate of return, is a misleading measure of fund performance, as it gives no sense for the amount of capital put to work or the absolute dollars returned to investors:

> "A high internal rate of return does not in and of itself put money in one’s pocket. Only when it’s applied to a material amount of invested capital for a significant period of time does IRR produce wealth"

I've noticed a similar problem in how we measure and evaluate startup success, particularly **growth rates**.

There are two kinds of growth: **absolute** and **relative**:

-   Absolute growth is the _absolute_ increase in a metric, like dollars of revenue added or customers acquired in a given period. The metric this period minus its value last period gives you the absolute growth:

$$\text{Absolute Growth}\_t = \text{Metric}\_t - \text{Metric}\_{t-1}$$

-   Relative growth is measured _relative_ to what came before: "We ended the year with 150% more customers", or "We grew revenue 3X this year." It's effectively unitless, but we tend to measure it in percentages, as in "percentage points of growth." It's the absolute growth divided by the metric's value last period

$$\text{Relative Growth}\_t = \frac{\text{Metric}\_t - \text{Metric}\_{t-1}}{\text{Metric}\_{t-1}}$$

**In startup land, we talk way too much about relative growth.** It muddles our thinking and makes compound, exponential growth seem easier than it really is. **We'd do better to ground our thinking in absolute growth.**

Obsessing too much over relative growth injects a number of "bugs" into our thinking:

-   It screws up our language
-   It makes us forget what startups actually do
-   It sets up unrealistic expectations

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div>

## No sense, this makes

![Pasted-image-20210906134123](https://nnamdi.net/content/images/2021/09/Pasted-image-20210906134123.png)

The first problem with the concept of relative growth is that it warps our language in nonsensical ways.

A common scenario: a startup previously growing 100% Y/Y "slows" to 50% the subsequent year.

The board of directors asks "why did growth slow so much?" External investors evaluating the company think, "Wow, they really hit a wall."

**But they did no such thing.** Yes the growth rate measured in _percentage terms_ has declined, but the growth measured in terms of _dollars of revenue added_ hasn't slowed at all.

For example, let's say revenue went from $5M to $10M to $15M, which corresponds to 100% growth followed by 50%. Technically, the company added $5M in revenue each year, so growth hasn't slowed, per se. The problem isn't so much that growth "slowed" so much as it is that absolute growth _didn't increase_, hence the fall in the relative growth rate.

Think back to your high school physics class. Imagine we have an object, say a car, traveling in some direction. That car has a position, x, and a velocity, v. In business terms, imagine the level of revenue (or whatever metric) is the position of that car and it's absolute growth maps to the velocity.

![drawing-2021-09-22-17.21.13.excalidraw](https://nnamdi.net/content/images/2021/09/drawing-2021-09-22-17.21.13.excalidraw.png)

As long as that car keeps moving at the same velocity, we would say it neither accelerates nor decelerates. In startup land however, a business growing at constant absolute growth is said to to somehow be _decelerating_ because its relative growth is declining.

![drawing-2021-09-22-17.30.35.excalidraw](https://nnamdi.net/content/images/2021/09/drawing-2021-09-22-17.30.35.excalidraw.png)

This is super strange.

In fact, the only way to make sense of this thinking is to measure the size of the business on a logarithmic scale. As I discuss in "[You Don't Understand Compound Growth](https://whoisnnamdi.com/you-dont-understand-compound-growth/)", a constant slope (velocity, effectively) on a log scale implies constant relative growth. In this view, adding a constant $5M in revenue annually is in fact a declining amount of _log revenue_ added, thus matching the language of deceleration we tend to use.

OK fine, but this subtle rescaling goes completely unsaid. Further, as I'll discuss later on, constant relative growth is fairly unrealistic, yet it is effectively implied by our language on the topic.

We talk as if a reduction in relative growth implies something fundamentally changed about a business. And it would, if that's what companies actually did — that is, grow "relatively". However, more often than not, a decline in relative growth in fact suggests that **not enough changed** about the business.

## As Yoda said, "Grow absolutely. There is no relative"

![Pasted-image-20210906134610](https://nnamdi.net/content/images/2021/09/Pasted-image-20210906134610.png)

The second problem with relative growth is that it misleads us about what businesses fundamentally do.

The fundamental unit of growth is a dollar of additional revenue, not an additional _percentage point_ of growth. In other words, absolute growth is the more fundamental concept. We can calculate a relative growth rate after the fact if we want, but the core result is absolute growth.

Relative growth is merely a derived concept, a way to normalize absolute growth into a metric that can be compared across companies. That's great, and it's a valuable tool. But we seriously pervert the concept when we go from using it as an _ex post_ calculation to thinking that the primary activity of companies is to generate "percentage points of relative growth". That is putting the cart before the horse, the percentage points before the dollars.

**Relative growth is not what literally happens.** It's a way to benchmark what happened. **Absolute growth is what actually happened**, what was actually accomplished.

**Absolute growth is the "[hard thing](https://www.amazon.com/gp/product/0062273205/ref=as_li_tl?ie=UTF8&camp=1789&creative=9325&creativeASIN=0062273205&linkCode=as2&tag=whoisnn-20&linkId=e0d8ea2cb4b222b0461b55b48ca46c06)" that startups must do each and every day.** Relative growth imperfectly normalizes those efforts in order to make them comparable across scales of companies. It falls well short of its own goal. Further, the choice to normalize absolute growth by the previous level of revenue is simply a convention and a somewhat arbitrary choice. Hypothetically we could normalize growth by _anything_.

The job of the business is to generate absolute growth, to bring in more dollars this year than last year. The most informative information about the team's ability to do that comes from how much they were able to add in the past. Knowing that revenue tripled last year is not in fact informative about the likelihood of revenue tripling this year, because "tripling" is not what the company is in the business of doing. **It's in the business of generating more absolute dollars of revenue each year.** That it tripled last year doesn't really tell you whether it can triple this year. That it added $5M in revenue last year tells you A LOT about the likelihood of generating $5M in additional revenue this year.

Thinking in relative growth terms causes you to lose your sense of scale. Conversely, thinking in absolute growth makes things quite vivid. "How exactly are we going to find $5M in additional revenue next year" is a much more tangible and precise discussion than "How do we grow 80% next year" even if they mean the exact same thing.

The same relative growth rate can describe many different situations. For example, you hear that a company has grown 3X year-over-year, a relative growth metric. That single number is consistent with all of the following scenarios:

-   $300K to $900K
-   $1M to $3M
-   $3M to $9M

Yet there is nothing similar about any of these growth stories. It is a fundamentally different task (and therefore, accomplishment) to go from $300K in revenue to $900K than to go from $3M to $9M. To describe these in the same terms ("3X growth") is a complete misnomer that obfuscates an incredible amount of context. _These are nothing alike._ In this way, relative growth represents a **loss of information** compared to absolute growth.

It's seductive to think that, "well, we grew 100% last year so we should expect to grow similarly this year" when in fact the two tasks are completely different. Constant relative growth of 100% two years in a row in fact requires _doubling_ your growth rate in absolute terms because your existing revenue base is now twice as large. Absolute growth is the "real" growth target you need to hit, and for most companies that won't be easy. This is sobering.

Said differently, you can't eat _percentage points_ of growth, you can only eat **dollars** of growth.

## Rational expectations

![Pasted-image-20210906133733](https://nnamdi.net/content/images/2021/09/Pasted-image-20210906133733.png)

It's cliche to point out that [humans don't understand compound, exponential growth](https://whoisnnamdi.com/you-dont-understand-compound-growth/). In startups we have the opposite problem — people are so familiar with the concept that they apply it everywhere, in situations where exponential growth isn't likely or even plausible.

Exponential growth implies _constant relative growth_. Rather than seeing relative growth fall off, the business continues to grow at nearly the same rate for multiple periods.

Exponential growth is a great aspirational target to set for companies. Y Combinator does this and it works fantastically for focusing the minds of startup founders. When exponential growth is the goal, one realizes there are really very few ways to achieve it, and most growth hacks won't get the job done.

However, exponential growth is not a rational _expectation_ to have or _prediction_ to make. Few phenomena exhibit this sort of behavior outside of the natural world, like bacteria growing in a petri dish. Modern economies are often modeled as perpetual, exponential growth machines, but for most of human history, economies did not grow exponentially at all. Perhaps it's a fine approximation for a large economy, which aggregates an uncountable set of activities that sum up to something approximating constant exponential growth. But it's certainly not a good model for a single company.

Only at low growth rates is past relative growth predictive of future relative growth. At high growth rates the two are not very correlated at all. A business (or an economy for that matter) that grew 2% last year has a very good chance of growing roughly 2% this year. For a business that grew 200% last year... this year's growth is anyone's guess.

There's nothing inherent about startups that makes them grow exponentially other than the fact that it is easier to pull off for small companies than it is for larger ones. Any other correlation between startups and exponential growth is aspirational and attitudinal: startups grow exponentially because **we will them to**. It is not otherwise a reasonable expectation.

For forecasting purposes, constant absolute growth is a much better "baseline" or "base case" than constant relative growth. Constant relative growth may be a great target for setting the tone in the organization, raising ambitions, etc., but mark my words: it probably won't happen.

Constant growth in absolute terms is totally normal and unremarkable: startups often add similar amounts of revenue as they did the previous year. Constant _relative_ growth is totally _abnormal_. The idea that a startup growing 100% should somehow manage to grow close to that the subsequent year is a mental bias we tend to have in Silicon Valley that doesn't properly reflect reality. It rarely happens.

It is much more common to see a company go from $5 to 10 to 15 to 20 (constant absolute growth of $5M per year) than go from $5 to 10 to 20 to 40 (constant relative growth of 100% per year). Doing the latter requires a fundamental re-architecture of the company and go-to-market motion every 6 months. Going from $5-10M looks very different than going from $20-40M, even though they both involve a doubling of the business.

I find that founders and operators tend to understand this all too well, while investors consistently underappreciate and underestimate the difficulty of scaling at a constant relative rate.

Operators, by dint of living in the real world, viscerally understand that what got them _here_ won't necessarily get them _there_. Investors on the other hand like to think of companies "2X-ing year-over-year," as if the 2X relative growth is some fundamental property of the business, like its mass or electric charge. It is not, as they often discover 12 months later.

The investor mentality is in part driven by their relationship with the business. They invest at a certain valuation, and returns are measured relative to that entry price. Founders on the other hand start from **zero** — so they don't really measure themselves "relative" to anything. What matters is building as large a business as they possibly can in absolute terms.

## Deal in absolutes

![Pasted-image-20210906133514](https://nnamdi.net/content/images/2021/09/Pasted-image-20210906133514.png)

To close, I want to make sure I'm not misunderstood in my diatribe against relative growth.

There are actually very good theoretical reasons to focus on relative growth. For example, when relative growth is predictable and easy to pin down, we can use it to calculate the net present value of an asset, which is a direct function of the relative growth of cash flows. This is how discounted cash flow analysis works, for example.

Speaking of assets, the relative growth of asset prices (in other words, returns) is a fundamental building block of modern finance theory and is one of the primary traits that determines how attractive an asset is to potential investors (and the other is the risk or volatility of those returns).

So investors' focus on relative growth is not totally unfounded. That said, these notions all rely on either precisely knowing the relevant parameters (growth, discount rate, etc) or knowing precisely the variability of those metrics (their volatility, variance, etc). This is the world of quantifiable risk, and upon its core principles rests trillions of assets and millions of pages of academic finance research.

But startups do not live in the world of risk, they are squarely domiciled in the world of uncertainty (and the state of Delaware). Here, we barely know what we think we know and reliable invariants are hard to come by. Nothing can be taken for granted, certainly not growth.

So in our board rooms, pitch decks, and quarterly business reviews, let's ground ourselves in the harsh, brutal world of absolutes rather than the theoretically and mathematically convenient domain of relatives. We'll likely do better on both metrics, that way.

> This month I wrote about the difference between absolute and relative growth and the fact that Silicon Valley (esp. VC) focuses WAY too much on the latter  
>   
> Inspired by an old [@HowardMarksBook](https://twitter.com/HowardMarksBook?ref_src=twsrc%5Etfw) memo...[https://t.co/JlIT9T60Y4](https://t.co/JlIT9T60Y4)
> 
> — Nnamdi Iregbulem (@whoisnnamdi) [September 24, 2021](https://twitter.com/whoisnnamdi/status/1441460616917504008?ref_src=twsrc%5Etfw)

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div> ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Do Wealthy Investors Have an Edge?]]>
                        </title>
                        <description>
                            <![CDATA[The super-rich earn more on their investments than the rest of us. 

Something nefarious, or something else?]]>
                        </description>
                        <link>https://whoisnnamdi.com/wealthy-investors-edge/</link>
                        <guid isPermaLink="false">wealthy-investors-edge</guid>
                        <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Wed, 18 Aug 2021, 07:47:55 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512259274-header.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
The super-rich earn more on their investments than the rest of us.

Something nefarious, or something else?

## The rich get richer, faster

Wealth and financial returns are positively correlated. Here's a plot of average annualized portfolio returns against ([log](https://en.wikipedia.org/wiki/Logarithm)) wealth, coming from a [study](https://conference.nber.org/conf_papers/f155140/f155140.pdf) using data from [Addepar](https://addepar.com/), a wealth management technology company:

![Pasted-image-20210725110126](https://nnamdi.net/content/images/2021/08/Pasted-image-20210725110126.png)

There's a strong positive relationship between wealth and financial returns. To translate the range on the x-axis into something a bit more meaningful, you can think of the left side as representing investors with less than $3 million in assets and the right side representing investors with more than $100M in wealth.

Here are the exact returns at various levels of wealth:

![Pasted-image-20210725105724](https://nnamdi.net/content/images/2021/08/Pasted-image-20210725105724.png)

Comparing the <$3M group to the >$300M group, **the richest investors earn roughly 2 percentage points more on their investment portfolios annually**, a sizable advantage.

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div>

## High class asset classes?

Richer investors could be allocating their capital to different asset classes than other investors, and that could be driving the return differential.

Is it true that asset allocations differ meaningfully across wealth groups? **Yes**:

![Pasted-image-20210815202907](https://nnamdi.net/content/images/2021/08/Pasted-image-20210815202907.png)

Richer investors put more of their money to work in [alternative investments](https://money.usnews.com/investing/investing-101/articles/a-beginners-guide-to-alternative-investments) (venture capital, private equity, hedge funds, etc.), private companies (owned directly, not via venture capital or private equity funds), and housing.

However, it turns out that while asset allocations do tend to correlate with wealth levels, the differences in returns cannot be fully or even mostly explained by high-level asset allocation decisions.

To test for this, the authors calculated the expected returns of various wealth groupings based purely on the typical asset allocation of those groups. So stocks are assumed to get the returns of the S&P 500, bonds are expected to earn the returns of the US Aggregate Bond Index, and so on.

When they do this, they find that the expected returns are not divergent enough to explain the rich investor advantage:

![Pasted-image-20210725110742](https://nnamdi.net/content/images/2021/08/Pasted-image-20210725110742.png)

Using this approach, the gap between investors with less than $3M in assets and those with $100M+ is only **0.35 percentage points**, whereas the difference in raw returns was a full _2 percentage points_. **So asset mix does not even come close to explaining the differences.**

We can also look at returns across wealth levels within each asset class. The table below shows the results of a regression of realized returns of individual investors on various wealth buckets, repeated for different asset classes. Interpret the numbers as the annualized returns of that group relative to the returns of investors with less than $3M in assets. For now, focus on panel A at the top:

![Pasted-image-20210725110838](https://nnamdi.net/content/images/2021/08/Pasted-image-20210725110838.png)

**Richer investors earn significantly higher returns in public equities, alternatives, and privately held companies.** The only major category they don't dominate is fixed income, which perhaps makes some sense given the spread of returns at the individual bonds tends to be much more compressed that other assets.

## You have to risk money to make money

So what gives? Why do the super rich earn so much more on their investments than everyone else, even after controlling for asset class?

Here's a thought: **is raw, realized returns even the right metric to compare?** (Hint: No)

Risk vs. reward is a classic dynamic in finance. Financial returns compensate investors for risks they take, with riskier investments generating higher returns, which encourages investors to buy them in the first place.

The [Sharpe Ratio](https://www.investopedia.com/terms/s/sharperatio.asp) quantifies the degree to which a portfolio earns excess returns relative to the risk undertaken. The Sharpe Ratio divides the excess returns of a portfolio relative to some risk-free asset by the standard deviation of the portfolios excess returns, which represents volatility and risk, yielding "risk-adjusted" returns.

When the researchers do this, the relationship between wealth and returns completely breaks down:

![Pasted-image-20210725110133](https://nnamdi.net/content/images/2021/08/Pasted-image-20210725110133.png)

The Sharpe Ratios of wealthier investor portfolios are no better than those of less wealthy investors:

> “having controlled for risk, the point estimates of the Sharpe ratio for very wealthy investors is not higher, and **the relationship between returns and wealth remains statistically insignificant**”

So wealthy investors earn more but those earnings are explained by more risk-taking.

Remember, this risk-taking is happening within asset classes rather merely than across, so this result implies that the rich are investing in riskier individual assets (e.g. companies) within each category. For example, the wealthiest investors hold a much larger share of their public equity portfolio in individual stocks (vs. ETFs or mutual funds) relative to other investors, leading to more volatile portfolios:

![Pasted-image-20210815223154](https://nnamdi.net/content/images/2021/08/Pasted-image-20210815223154.png)

High net worth investors also tend to invest in smaller, higher growth companies, naturally riskier. Interestingly, this behavior generates a lower market "beta" for rich investors in the [CAPM model](https://www.investopedia.com/terms/c/capm.asp), as their portfolios correlate less well with the overall equity market than do those of less wealthy investors, who have betas close to 1:

![Pasted-image-20210815224634](https://nnamdi.net/content/images/2021/08/Pasted-image-20210815224634.png)

> “Higher-wealth households load more heavily on the SMB factor, reflecting an **increased focus on small-cap companies**. High net worth portfolios also load more negatively on the HML factor, meaning that they have **more exposure to growth companies** than lower wealth investors.”

## Looking at all the alternatives

There's one more wrinkle though. Here's that last table again, showing returns across wealth groups and asset classes. This time focus on panel B, which focuses on risk-adjusted returns:

![Pasted-image-20210725110838-1](https://nnamdi.net/content/images/2021/08/Pasted-image-20210725110838-1.png)

**For public equities and privately-held companies, controlling for risk largely eliminates / accounts for the return premium earned by wealth investors.** However, accounting for risk doesn't totally do the trick for alternative investments, where richer investors do better even taking risk into account. In fact, less wealthy investors do _so_ badly in alternatives that the average investor with less than $3M in assets only earned an annualized return of 1.75%, which is not much higher than the risk-free rate during the same period of 1.33%, despite taking on significant risk.

We can dive a layer deeper by examining the returns across subgroups within alternatives, primarily hedge funds, venture capital and private equity, and real estate. **Richer investors earn higher raw and risk-adjusted returns in hedge funds and VC/PE** but do no better in real estate, where they actually earn _lower_ risk-adjusted returns:

![Pasted-image-20210725113425](https://nnamdi.net/content/images/2021/08/Pasted-image-20210725113425.png)

**The least wealthy earn less than 1% annually on their hedge fund, venture capital, and private equity investments**, which is stunning.

It's hard to pinpoint the exact drivers of the alternative investments advantage of the richest investors. What we do know is that the alternative investment portfolios of the richest investors differ quite substantially from those of less wealthy investors:

![Pasted-image-20210815213652](https://nnamdi.net/content/images/2021/08/Pasted-image-20210815213652.png)

For example, investors in the sub-$3M group have only two alternative investments on average, while the $100M+ group has **35**. On that basis alone we would expect the better diversification to generate lower volatility for the richest investors and potentially even higher returns based on my argument in "[Why Don't VCs Index Invest?](https://whoisnnamdi.com/vcs-index-invest/)" (citing [Abe Othman](https://angel.co/blog/venture-returns)):

> ... investors increase their expected return by indexing as broadly as possible at the seed stage (i.e., by putting money into every credible deal), because any selective policy for seed-stage investing—absent perfect foresight—will eventually be outperformed by an indexing approach.

Another quirk: **the alternatives positions of richer investors are updated much less frequently**, meaning the valuations at which the companies are held are updated less often. This makes the portfolios of richer investors appear less volatile. Further, as folks in the industry know, companies that are doing well will typically see their valuations rise faster and their carrying values updated more frequently. So less updating means less volatility, and any volatility they do see is largely to the upside:

> **The smoothed returns of private equity understate the true economic risk** and are an artifact of the lack of mark-to-market for illiquid assets — [Demystifying Illiquid Assets: Expected Returns for Private Equity](https://www.aqr.com/Insights/Research/White-Papers/Demystifying-Illiquid-Assets-Expected-Returns-for-Private-Equity)

Still, having stared at the data for a while, my sense is the alternative advantage represents a real returns premium and isn't merely an artifact of mark-to-market gamesmanship. Let's keep digging.

## Real estate: location, location, location. Alternatives: access, access, access

We've crossed out anything nefarious in all the other asset classes. In all asset classes outside of alternatives, wealthier investors earn higher returns in the same way anyone earns them — by taking more risk. But alternatives remain a mystery. What's going on there?

**Is it skill?** Probably not, at least not in traditional sense of "stock picking". Most investments in alternatives are intermediated by managers — hedge, venture capital, and private equity funds — who make the actual investments. Therefore, it's unlikely that better stock picking ability on the part of the wealthy investors themselves explains their enhanced returns.

But that's also why the returns gap is _so strange_. If the individual investor is not actually making the investments, then why should their personal net worth correlate so highly with their portfolio returns in alternatives? The only explanation is differences in skill at picking great managers or access to those managers across wealth levels:

> "Higher-wealth investors may receive preferential access to better-performing managers because they can offer larger amounts of funds at once, which reduces marketing and related overhead costs to the fund manager... In contrast, lower-wealth investors may only receive access to hedge fund and private equity solutions that are distributed through advanced marketing networks and are originated by large platform operators. However, **funds that provide more accessibility may deliver worse performance**"

For example, the authors find that [fund-of-funds](https://www.investopedia.com/terms/f/fundsoffunds.asp), investment funds that invest in other funds rather than in securities directly, earn "almost exactly two percentage points lower" returns on an annualized basis "evidence for fees imposed by additional layers of management." Another piece of evidence — the fewer investors who hold the same alternatives security, the higher than returns, and vice versa: "assets with limited investor participation are significantly related to higher investment returns." In other words, **exclusivity generates better returns**. This points to the importance of access.

One highly suggestive piece of evidence comes from the returns of investors whose funds are managed by a single family office (SFO), which is an investment firm setup solely to manage the wealth and investments of a single family or person. **Portfolios managed by family offices earn significantly higher returns** than portfolios of investors in the same wealth bracket who do not have a family office. Not only that, the return differential across wealth groups disappears when we look only at portfolios managed by family offices:

![Pasted-image-20210725112023](https://nnamdi.net/content/images/2021/08/Pasted-image-20210725112023.png)

Depending on the asset class, among portfolios managed by family offices, investors with less than $3M in assets earn **more** than investors with over $100M in assets:

> "While lower-wealth investors earn substantially lower returns on their hedge fund and private equity investments than ultra-high net worth investors, **this does not apply to investors with smaller portfolios that are managed by an SFO**. Even SFO investors with less than three million, or 3-10 million earn roughly the same return as those with more than 100 million on their investments in hedge funds and private equity, and **substantially more than investors in the same wealth brackets but without SFO management.**"

Something about the nature of family offices and their influence on manager selection causes the wealth-returns relationship to disappear entirely. The study's authors speculate that this "likely \[relates\] to the difficulty of identifying and **accessing** high-performing alternative investment funds."

**Access to assets matters.** In the public markets, access is effectively democratized, so we can completely explain the advantage of richer investors in the public markets via differential risk-taking. However, the advantage in alternative assets reflects more than mere risk tolerance or preference. Ironically, family offices, which likely _exacerbate_ inequality between the rich and the rest, **level the playing field \*among\* the rich**.

## The 1% vs. the 0.1%

So do wealthy investors have an edge? Relative to non-wealthy, not really. It's risk-loving turtles all the way down.

But do the wealthiest have an edge versus their somewhat less wealthy peers? Yes, though I won't lose much sleep over it. It's an **access advantage**, and it's limited to the assets where access remains important: the relatively opaque and exclusive worlds of hedge funds, venture capital, and private equity.

Candidly, I'm more focused on expanding access to these alternative asset classes in general than on helping already well-off investors get access to the best funds. That said, the excess returns of alternatives are quite low for investors with only a few million in assets. On that basis, they aren't much better than public equities except for the extremely wealthy.

If these new investors only receive access to the worst-performing funds, the effort will be worthless. Worth pondering further.

> The super-rich earn more on their investments than the rest of us.  
>   
> In the battle between the 1% and the 0.1%, the richest get richer, faster.  
>   
> Something nefarious? Or something else?[https://t.co/Mc8imZTiH3](https://t.co/Mc8imZTiH3)
> 
> — Nnamdi Iregbulem (@whoisnnamdi) [August 20, 2021](https://twitter.com/whoisnnamdi/status/1428770455414992899?ref_src=twsrc%5Etfw)

_Thanks to [Cynthia Balloch](https://www.lse.ac.uk/finance/people/faculty/Balloch) and [Julian Richers](https://sites.google.com/site/julianrichers/home), the authors of the study from which much of this essay is derived._

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div> ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[PhDs Aren't Starting Companies Like They Used To]]>
                        </title>
                        <description>
                            <![CDATA[The burden of scientific knowledge and managerial complexity is crushing our best and brightest.]]>
                        </description>
                        <link>https://whoisnnamdi.com/phd-founders/</link>
                        <guid isPermaLink="false">phd-founders</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Developers ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Thu, 8 Jul 2021, 07:01:00 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512257557-header.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
The burden of scientific knowledge and managerial complexity is crushing our best and brightest.

STEM PhDs aren't starting companies like they used to. Meanwhile, fresh PhD founders earn _less_ than they did 20 years ago.

Cushy tech jobs are looking better and better to our most highly-educated as entrepreneurship loses its relative attractiveness.

Here's why.

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div>

## Two decades of declining PhD entrepreneurship

[STEM PhDs are creating fewer startups than they used to](https://www.nber.org/papers/w27787):

> "**We... show a decline over the past 20 years in both the rate of startups founded and the share of employment at startups by the highest-educated science and engineering portion of the U.S. workforce.** The declines are wide-ranging and not driven by any particular founder demographic category or geographic region or scientific discipline."

Here's what the data shows: today, of STEM PhDs working in the private sector, only ~20% start companies, down from 30%+ in the late 90s:

![izoe3lP-QH](https://res-1.cloudinary.com/whoisnnamdi/image/upload/q_auto/v1/whoisnnamdi/izoe3lP-QH.png)

> "Figure 1 displays the large decline over the past two decades in the share of founders among PhDs in science and engineering. **In 1997, 34 percent reported being a founder of a startup, but by 2017 this rate had declined to 21 percent, a decline of 38 percent.**"

Further, STEM PhDs are joining startups at lower rates in general, even outside of founder roles. **Startups are much less attractive to PhDs than they used to be.** This is true across demographic groups (gender, race, location, citizenship, etc.):

> "Since 1997, the share of founders in these startups has declined by around 38 percent, not limited to any particular founder demographic or ethnic group or occupation, and this decline is widespread across regions of the United States. The share of workers at startups has followed the same path of decline."

> "**the employment share of science and engineering PhDs at startups is also falling over time.** The downward trend is not driven by any particular category of PhDs. For example, figures similar to Figure 1 for males versus females, whites versus non-whites, California versus the rest of the U.S., and for U.S. versus non-U.S. born PhDs differ in levels but their dynamics are almost exactly the same as those in Figure 1... **The dramatically falling share of founders and employment at startups raises the prospect of a drying up of high-tech, high-opportunity startups.**"

Why the change of heart among our PhD graduates? As the study authors speculate:

> "A potential source of this decline is the exponential increase in the amount of scientific knowledge."

As the fields from which these PhDs come have become increasingly complex and extensive over time, the amount of scientific knowledge one must cram in to operate at the cutting edge of the field has exploded. Take any field and you see the same trend — becoming an "expert" is much, much harder than it used to be, and this burden of scientific knowledge weighs on potential founders.

Not only are fewer PhDs starting or joining startups, they're also waiting longer after completing their graduate coursework to found companies:

> "Consistent with the burden of knowledge increasing for founders, **the years of work experience among founders shows a steady increase.** The regression results imply that **the average founder had about 14 percent longer post-PhD work experience in 2017 than in 1997.**"

Here again we see the impact of the burden of scientific knowledge. With so much existing research and scientific knowledge out there to assimilate, PhD graduates are delaying entrepreneurship, taking more time to gather work experience before setting off on their own.

## Learn to earn

Even worse, relative to their more experienced peers, newly minted PhDs who do found startups earn much less than they used to, making founding a company an even less appealing proposition:

![Jy9Ww_-fUd](https://res-5.cloudinary.com/whoisnnamdi/image/upload/q_auto/v1/whoisnnamdi/Jy9Ww_-fUd.png)

> "Figure 2 illustrates **a pronounced decline over time in the earnings of less experienced founders relative to their more experienced peers.** Here, we separate less and more experienced founders by the median number of years after PhD (13 years), although other reasonable cut-offs lead to similar results. Between 1997 and 2001, founders with post-PhD experience at or below the median earned more... but by the 2010s, the situation is reversed, with **less experienced founders earning on average 30-40 percent less than other founders. Prior work experience apparently is becoming much more valuable for founders over time.**"

The figure compares the earnings of PhD founders with below-median and above-median work experience and demonstrates that this ratio has fallen meaningfully over the past two decades. Whereas the groups used to have earnings parity, today young PhD founders suffer a significant earnings disadvantage relative to their more experienced peers, making 30-40% less.

Absolute earnings for the highly-educated in general have risen over the same period, so this might not be terrible news. Young PhD founders could still earn more than they used to in absolute terms.

**Not so.** Earnings for PhD founders with below-median work experience **declined** in real (inflation-adjusted) terms, from $73K in 1997 to only ​$58K in 2017, **a 20%+ pay cut**:

> "**The earnings of less experienced founders declined not just in relative but also in absolute terms.** The average inflation-adjusted earnings of founders with below the median post-PhD experience were $72,616 in 1997, whereas 20 years later their earnings were ​$57,517, **a decline of more than 20 percent**"

The earnings of young STEM PhD founders are their _lowest level_ in more than 20 years.

What if we combine both inexperienced and experienced founders and look at the overall trend in PhD founder earnings? Surely this has trended positively?

Nope:

> "Founders' earnings decline on average by about 1.6 percent per year (column 1). However, this is offset by an opposite time trend in returns to experience. The mean number of years after founders receive a PhD is 15.9 years; hence... at the mean work experience, the negative baseline time trend is completely offset."

OK, this takes a bit of unpacking. Earnings for less experienced PhD founders have fallen, while more experienced founders have seen rising earnings. The breakeven point is ~15 years, which is to say founders with fewer than 15 years of post-PhD experience have seen declining real earnings over time, while founders with more than 15 years of experience have seen earnings growth. It just so happens the average PhD founder in the survey had 15.9 years of post-PhD experience, so the two stories roughly balance out.

_The good news:_ overall PhD founder earnings aren't declining! _The bad news:_ PhD founder earnings aren't rising either!

![Real-Earnings-of-PhD-Founders--1619555660589](https://res-1.cloudinary.com/whoisnnamdi/image/upload/q_auto/v1/whoisnnamdi/Real-Earnings-of-PhD-Founders--1619555660589.svg)

The craziest thing about all this? PhD _employees_ are doing much better:

> "In stark contrast to founders, **workers' real earnings grow over time**, although the increase is relatively small, 0.4 percent per year... The trend toward increasing returns to experience is much weaker among workers than among founders..."

Again, let's unpack. STEM PhDs who work at established companies rather than startups have seen their earnings rise across the board, regardless of experience level:

![Real-Earnings-of-PhD-Workers-1619555855360](https://res-4.cloudinary.com/whoisnnamdi/image/upload/q_auto/v1/whoisnnamdi/Real-Earnings-of-PhD-Workers-1619555855360.svg)

As discussed earlier, founding a company as a young PhD is less attractive than it used to be. While young PhD founders used to make slightly more than young PhDs at established firms, this relationship has completely reversed, with inexperienced PhD founders now earning significantly less than their less entrepreneurial associates:

![Real-Earnings-of-Inexperienced-PhDs-1619556012742](https://res-4.cloudinary.com/whoisnnamdi/image/upload/q_auto/v1/whoisnnamdi/Real-Earnings-of-Inexperienced-PhDs-1619556012742.svg)

In summary, **new PhD founders get the worst deal**, earning less than both their equally and more experienced peers:

![Real-Earnings-of-PhDs-1619556257369](https://res-1.cloudinary.com/whoisnnamdi/image/upload/q_auto/v1/whoisnnamdi/Real-Earnings-of-PhDs-1619556257369.svg)

That was all a bit complicated, so before moving on, let's summarize the trends.

-   Previously, experience mattered most within established companies, but over time the "gains from experience" grew much more for founders than workers
-   Experienced PhD founders went from being no better off than their less experienced peers to earning significantly more, whereas the gap between experienced and inexperienced PhD employees stayed relatively stable.
-   Inexperienced PhD founders used to make more than inexperienced PhD workers, but this relationship has flipped
-   Inexperienced PhD founders are now the worst-paid PhDs

## The division of tasks

So far we've established:

1.  PhDs are founding far _fewer_ companies, and
    
2.  When they do found companies, young PhD graduates earn _much less_ than they used to and significantly less than peers working at established companies
    

Not good. But it couldn't get any worse for our youthful, bright, wide-eyed PhDs could it?

Turns out, it can.

> "**established firms have an advantage over startups in creating a division of labor in R&D**... by introducing more hierarchical layers, reducing knowledge workers' span of control, and allocating more experienced workers to positions with greater managerial responsibility. Further, established firms compensate workers for performing more R&D tasks and supervising more individuals. These developments are not seen among founders. **The differences follow from the natural limits imposed by running a small firm with less division of labor and a high amount of multitasking by the founder.** The largest firms are even more active in reorganizing job tasks, increasing the depth of hierarchy at twice the rate of all established firms."

Big Tech and other large companies have become more attractive places to innovate for our most well-educated workers. Large companies have better dealt with the accumulated burden of scientific knowledge by enforcing a "division of labor" among employees. Large companies form _"knowledge hierarchies"_, such that no one person needs to know everything, and information can be aggregated up the chain in a rationale fashion. This lets organizations to scale to meet the needs of cutting edge scientific research and development, which increasingly incorporates impossibly vast sums of knowledge.

On the other hand, startup founders do not have armies of willing cadets to offload and delegate to. They are the captain of the ship and also it's most valuable crewmember, so as the burdens of both scientific knowledge and management complex grow, it all gets placed on their shoulders.

The job of a founder has gotten harder — running a startup today requires more management and R&D activities than it did 20 years ago. Quantified, this is about 15% more "tasks", with **R&D tasks growing by about 50% and management tasks growing 5%**:

![fig3a](https://res-2.cloudinary.com/whoisnnamdi/image/upload/q_auto/v1/whoisnnamdi/fig3a.png)

> "The average number of all tasks for founders increased by about 15 percent from the beginning to the end of our sample, a statistically significant difference... Furthermore, although the number of both R&D and management tasks increased, the increase is more pronounced in **R&D, for which it rose by more than 50 percent from 1997 to 2017**... As a result, R&D tasks that comprised about 25 percent of all tasks conducted by founders in 1997 increased to 34 percent of all tasks in 2017. As can be seen in Figure 3, this was not accompanied by any decline in management tasks, so the founders had to shoulder the burden of doing more R&D tasks while also running the same or more administrative tasks."

PhD workers at established firms also saw their total tasks grow, but to a lesser degree. **R&D tasks for workers only grew 12% over the same 20 year period, and management tasks did not increase at all**:

![fig3b](https://res-4.cloudinary.com/whoisnnamdi/image/upload/q_auto/v1/whoisnnamdi/fig3b.png)

> "Workers were also affected by the need to perform more R&D tasks, as the increase in the number of R&D tasks from 1997 to 2017 is about 12 percent. Notably, however, as the figure reveals, workers did not need to handle more management tasks, which remained relatively flat for them."

What gives? It goes back to the division of labor point I mentioned earlier. Let's start with R&D:

> "The number of R&D tasks is rising, but **established firms take advantage of a division of labor and knowledge hierarchies**"

The growing burden of scientific knowledge increases the number of R&D tasks that founder must occupy themselves with. With little hierarchy or staff to offload this work, **founders have no operating leverage**. Conversely, large, established firms can shift much of the burden to armies of researchers, more efficiently dealing with the growing stack of research output.

> "**We find that the number of different R&D tasks has increased more for founders than for workers.** And the returns to experience have increased over time for founders but not for workers, **highlighting the increasing need for a single person—the founder—to cope with the burden of knowledge in startups.** Workers at established firms have, instead, comfortably narrowed their span of control, employed more people indirectly under their control to support their work, and kept administrative duties low. They are also better rewarded for taking on more diverse work and managerial responsibilities than founders. ...**established firms have coped more effectively with the increasing burden of knowledge in science by better utilizing the division of labor in innovative work through reorganizing tasks and hierarchies.**"

Individual workers at established firms have limited scope, while the remaining load gets shared with other team members and subordinates. Further, the firm captures the gains from the successful division of labor and then shares this with employees in the form of better pay for the managers who corral these efforts internally.

> "Running a startup might constrain founders' ability to organize its hierarchy efficiently, at least until it has succeeded in growing well beyond its initial size. As science accumulates more knowledge, we would therefore expect PhD founders to have to take on more R&D tasks."

Founders have no ability to pull off similar organizational tricks until their startups reach meaningful scale. Thus, as scientific knowledge has accumulated, they've taken on the additional R&D work themselves.

This has implications for startup management too:

> "Also, **founders had to deal with significantly more management tasks than workers** in terms of levels: about 30-40 percent more at the beginning of the study period, **increasing to 50 percent more at the end**. The explanation for this difference likely lies in how the two types of firms differ in their organization of work..."

Founders and startups cannot afford the overhead that comes with increasing layers of professional management. This is why startup's delay building executive teams until at least a few years into a startup's lifecycle. Large organizations can and do invest in these management layers, gladly taking on the extra expense:

> "... the firm responds by increasing the number of layers... and... \[allowing\] greater job specialization. Increasing the number of layers of management adds a fixed cost of operations... **\[Larger\] firms are more likely to become hierarchically taller by adding more layers of management... as they can more easily absorb the added fixed cost.** Founders at startups appear not to have recourse to this mitigation strategy."

The study's authors quantify these additional layers by tracking two metrics:

-   the number of individuals _directly_ supervised by PhD founders and workers, and
-   the number of individuals _indirectly_ supervised (i.e. supervised by one's own direct reports)

The directly supervised corresponds to the **managerial burden** that each PhD-holder has (their span of control), while the indirectly supervised tells us how much **managerial leverage** each person has (the depth of hierarchy). The theory would suggest that workers within organizations should have fewer direct reports over time and more indirect reports, as organizations build out these knowledge hierarchies.

And that's exactly what we see:

![managerial](https://res-3.cloudinary.com/whoisnnamdi/image/upload/q_auto/v1/whoisnnamdi/managerial.png)

-   Workers with PhDs saw the number of employees directly managed _decline_ over time while the number of employees indirectly managed _increased_ over time. In others words, **the proliferation of middle management reduces the management burden for any individual worker but increases overall managerial leverage.**
    
-   Founders on the other hand saw no statistically significant trend in their span of control or depth of hierarchy. **The organizational structure of startups has not evolved to keep up with the increased burden of scientific knowledge.**
    

> "workers \[perform\] fewer R&D tasks as they age... the span of control decreases for workers at established firms... However... hierarchies deepen over time for workers at established firms... Together, the results suggest that **established firms cope with the increasing burden of knowledge on their workers by introducing additional layers of hierarchy, while simultaneously reducing the number of employees who report directly to managers**"

## Doing \*too many\* things that don't scale

We need to think harder about making knowledge work... work.

In summary:

> "Our findings suggest that if the goal is to restore business dynamism in the high-tech sector, **alleviating the burden of knowledge should be front and center** in the strategy to attain it."

The declining in PhD entrepreneurship mimics a broader drop in startup formation chronicled elsewhere. But PhD founders are special... and so these trends are especially worrying.

STEM PhDs have founded some of the world's most successful and impactful enterprises. Among them: Google, Intel, VMware, and others.

Ironically, however, today these same companies gobble up the brightest minds we have, discouraging or delaying their own entrepreneurial pursuits.

In Silicon Valley we talk a lot about management practices to help founders and executives "scale." But deeply technical founders face a separate but equally important problem: scaling _knowledge_.

> PhDs aren't starting companies like they used to  
>   
> Cushy tech jobs are more and more attractive to STEM PhDs  
>   
> The burden of scientific knowledge and managerial complexity is crushing our best and brightest  
>   
> I explore the precipitous decline in PhD founders:[https://t.co/NxpFn3NrZv](https://t.co/NxpFn3NrZv)
> 
> — Nnamdi Iregbulem (@whoisnnamdi) [July 8, 2021](https://twitter.com/whoisnnamdi/status/1413250293786021889?ref_src=twsrc%5Etfw)

_Thanks to [Thomas Astebro](https://www.hec.edu/en/faculty-research/faculty-directory/faculty-member/astebro-thomas), [Serguey Braguinsky](https://www.rhsmith.umd.edu/directory/serguey-braguinsky), and [Yuheng Ding](https://www.rhsmith.umd.edu/directory/yuheng-gavin-ding), the authors of [the study](https://www.nber.org/papers/w27787) from which much of this essay is derived._

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div> ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Awesome Developer Advocates Are Hiding in Plain Sight]]>
                        </title>
                        <description>
                            <![CDATA[Why you shouldn’t filter for social media following or prior experience]]>
                        </description>
                        <link>https://whoisnnamdi.com/developer-advocates-plain-sight/</link>
                        <guid isPermaLink="false">developer-advocates-plain-sight</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Developers ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Tue, 25 May 2021, 14:20:40 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512254789-header-1.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
[Developer relations](https://www.marythengvall.com/blog/2019/5/22/what-is-developer-relations-and-why-should-you-care) (DevRel) is one of the toughest hires to make in early stage, developer-centric startups.

In [Four Challenges Facing Developer Productivity Startups](https://nnamdi.net/developer-productivity-challenges/) I identified scaling developer relations as one of the key challenges facing developer productivity startups today:

> ... even if you're fully onboard with the idea of developer relations, doing it well is tough. To start, developer advocates are hard to come by. There just aren't many DevRel folks out there. The function is still relatively new, and career pipelines into the role have yet to fully materialize — [Four Challenges Facing Developer Productivity Startups](https://nnamdi.net/developer-productivity-challenges/#scalingdeveloperrelations)

To some extent this is a supply and demand issue — many more startups want to bring on developer advocates than there are available developer advocates.

But there are structural issues behind the small quantity of available developer advocates — it leverages a combination of skill sets not often found in a single package, making it a tough role to hire for:

> DevRel is a hard spec to hire for. Technical folks are usually introverts. Extroverts are typically not technical — Developer Advocate, Analytics Infrastructure Startup

> I want them to be technical, and I'm always looking for empathy. It's very hard to find these things in a single person, this combination — Head of Developer Relations, Web Development Startup

Even among the available pool of developer relations talent, companies have a hard time knowing who will be better at the role and what the key success factors are. How much does prior experience matter? How critical is it to have a large social media following?

I've spoken to numerous founders and developer relations professionals to tease out best practices for DevRel hiring. This piece focuses on how to find high quality developer advocates despite the scarce supply.

Here's some concrete advice:

## Most developer advocates are former engineers. So, talk to some current engineers

Current engineers are future developer advocates hiding in plain sight:

> Getting into this career can be hard, but there are many developers that I'd bet on to be great first time developer advocates — [Tessa Kriesel @ Devocate](https://www.devocate.com/devocate-for-developers/)

At some point, everyone gets their first gig in Developer Relations, and most of those people were developers themselves just before. Thus, current engineers are a great potential source of DevRel talent, if you're willing to do the work to pitch them on the role.

In fact, many people who could be great developer advocates don't know how to break into the field. For example, many engineers simply haven't spent the the time to build up a large social media following, but this doesn't mean they couldn't be successful in the role:

> ... the ones who could be good often have a hard time getting a foot in the door without a Twitter presence — Developer Advocate, Application Infrastructure Startup

Some companies have realized this and shifted most of their DevRel hiring internally, focusing mainly on convincing existing engineers to try it out.

The pitch? Developer advocates can often have impact much sooner and more frequently than engineers:

> We hire most of the DevRel team internally. Folks realized they could make an impact much sooner as DevRel than as engineers. Software releases take so long, but content takes much less time — Head of Developer Relations, Web Development Startup

Developer advocates "ship" faster. A piece of content — a blog post, a Twitch stream, etc — is often much quicker to assemble and publish than shipping some piece of software, like a new feature, to production:

> Sometimes it's enough just to write an article just to start a conversation. It doesn't have to be a full fledged feature release like in software — Head of Developer Relations, Web Development Startup

I haven't run the numbers, but it wouldn't surprise me if there were more _potential_ developer advocates among current software engineers than _actual_ developer advocates out there.

So if you're having difficulty finding candidates for your open DevRel role, talk to some current engineers, even one's working at your company.

And when you do, leave your biases at the door with respect to experience or social media following...

## Don't filter for DevRel experience or social media following

As with any role, it's super tempting to assume "only someone who's done this before could be good at it." Not so with DevRel:

> You can't look for experience, DevRel has barely existed for very long. If anything, non-traditional folks are often the best — Developer Success Engineer, Application Infrastructure Startup

DevRel as a formal practice has not existed for long. You can't expect most people to have meaningful experience doing it. Again, this reinforces the idea that many potentially great developer advocates aren't currently in the role.

Another important point to keep in mind is the job description for DevRel tends to be loose and ill-defined in many ways due to the nature of the role:

> The job description is extremely loose. Best people need to grow into it to some extent — Developer Success Engineer, Application Infrastructure Startup

Regardless of what _you_ think the person needs to do, the reality on the ground will look somewhat different. Accept this, and know that the best developer advocates will navigate this ambiguity regardless of whether they are an "obvious" fit based on their professional experience to date.

One of the "obvious" characteristics people often filter on is social media following. But not so fast:

> Social media is just one part of the toolbox. Further, social following in one area does not necessarily translate to another — Developer Advocate, Analytics Infrastructure Startup

It's debatable how much a social media following is in fact required for success in DevRel. It's typically helpful for sure, but it's not everything. Being able to authentically and credibly connect with other developers is much more important:

> You don't really need to have a following to be good at DevRel. It's much more important to have street cred \[with developers\] — Founder and Former Developer Advocate, Application Infrastructure Startup

In fact, social media can sometimes backfire (as we all know). In DevRel in particular, it can sometimes create the impression the person was merely "acqui-hired" for their social media sway rather than technical skills developers tend to appreciate and respect. I wouldn't say this is enough of a risk to be too worried about, but it's worth keeping in mind:

> Social media helps, but it can also be problematic in certain ways. It can sometimes create the perception that they were just hired for the followers they have, rather than their technical capacity. That can problematic for the person and the company — Product Manager and Former Developer Advocate, Big Tech

## Look for the core characteristics, regardless of current title

Rather than hire based on surface-level characteristics like current title or social media following, focus on identifying raw talent and passion for a key aspect of the role — [working in public](https://www.amazon.com/Working-Public-Making-Maintenance-Software/dp/0578675862/):

> There's a lot of raw talent out there. They are often already blogging. They are trying to get permission to speak at conferences — Director of Developer Relations, Web Development Startup

Look for people who are already doing DevRel without necessarily having a title for their efforts (yet). These are engineers who are writing blog posts (either for their employer or their personal blog), speaking at conferences, and publicly interacting with other developers. This is where social media can be value — it's not so much about having a _following_ as much as it's about having a _participatory presence_:

> Look for people who are already stepping up and doing the work. They are speaking at conferences, authoring blog posts, interacting on Twitter. Doesn't matter what their title is — Product Manager and Former Developer Advocate, Application Infrastructure Startup

In addition to behaviors, attitudes matter too. We've already discussed the importance of empathy, but a core desire to teach and help others is important too. Self-taught programmers are often great about this (since they know how hard it is to learn something on your own):

> Self-taught programmers are great. They often have a chip on their shoulder and a willingness to teach others. They understand how valuable help is — Developer Success Engineer, Application Infrastructure Startup

The founders and DevRel folks I spoke to agreed: these core characteristics are more critical to success in the role than prior experience or social media following. You likely have engineers in your team today with these traits. Maybe it's worth having a conversation about DevRel?

## Acres of DevRel diamonds

Constraints drive [creativity](https://www.fastcompany.com/3067925/how-constraints-force-your-brain-to-be-more-creative) and [innovation](https://hbr.org/2019/11/why-constraints-are-good-for-innovation). The market for developer relations talent is certainly constrained, and we need to get creative in response. Expanding your perspective for what DevRel "looks like" beyond social media following and title will help you find [acres of DevRel diamonds](https://www.amazon.com/Acres-Diamonds-Life-Changing-Classics-Audio/dp/0937539783) where you least expect.

Again, remember this simple logic:

-   IF most _current_ DevRel professionals were **past** engineers,
-   THEN most _future_ DevRel professionals are **current** engineers. ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[The Developer Productivity Manifesto Part 3 — Leaving Software on the Table]]>
                        </title>
                        <description>
                            <![CDATA[Quantifying the billion dollar impact of developer inefficiency]]>
                        </description>
                        <link>https://whoisnnamdi.com/leaving-software-on-the-table/</link>
                        <guid isPermaLink="false">leaving-software-on-the-table</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Developers ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Wed, 5 May 2021, 06:13:00 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512251791-header-software-table.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
Developer inefficiency drives hundreds of billions of dollars of lost software output annually.

The sum is striking but also predictable in light of data showing we don’t even come close to maximizing developer productivity.

Mountains of technical debt and poor development practices burden and bog down developers. We lose billions as a result.

The Developer Productivity Manifesto has three parts, this is part 3:

-   [Part 1: The Developer Productivity Flywheel](https://nnamdi.net/the-developer-productivity-flywheel/)
-   [Part 2: More (Developers) Isn’t Always More](https://nnamdi.net/more-developers-isnt-always-more/)
-   ****Part 3: Leaving Software on the Table (you are here)****

In part 1, I talked about falling developer productivity and how spinning [the developer productivity flywheel](https://nnamdi.net/the-developer-productivity-flywheel/) can counteract this trend.

In part 2, I argued that [more developers won’t solve all our software engineering problems.](https://nnamdi.net/more-developers-isnt-always-more/)

I’ve thrown around a lot of equations, literary references, and even some memes. In this third and final piece, I’ll talk dollars and cents, quantifying the impact of lost developer productivity and how much software we’re “leaving on the table” as a result.

### Receive my next long-form post

Thoughtful analysis of the business and economics of tech

 

Subscribe

## (Technical) debt to (developer) GDP

In [part 2](https://nnamdi.net/more-developers-isnt-always-more/), I made the case that software maintenance is not sufficiently value generating to cover engineering salary expenses. In other words, ****busy work doesn’t work****:

> __According to one analysis, an engineer engaged in purely non-innovative activity ****destroys**** nearly $600K in employer market value. On the other hand, the average engineer, working on a combination of maintenance and innovation activities, ****adds**** $855K in market value to their employer.__

In a global, cross-industry [survey](https://stripe.com/files/reports/the-developer-coefficient.pdf), Stripe asked developers why productivity was lower than it otherwise could be. ****Maintenance of legacy systems / technical debt**** took the top spot:

![](/content/images/2021/05/hindering.png)
*Source: The Developer Coefficient, Stripe*

Even developers themselves see maintenance work as unproductive. Asking them to quantify this reveals that the typical developer spends ****13.5 hours per week**** addressing technical debt:

![](/content/images/2021/05/debt-hours.png)
*Source: The Developer Coefficient, Stripe*

Add to that another ****3.8 hours per week fixing “bad code”**** (debugging, refactoring, etc.). That totals 17.3 hours per week spent fixing the past rather than building the future. The typical work week among the surveyed was 41.1 hours, implying that a full ****42% of developer time is lost to drudgery****:

![](/content/images/2021/05/dev-work-week.png)
*Source: The Developer Coefficient, Stripe*

I like to call this ****technical debt to developer GDP****. While some level of technical debt is unavoidable, it eventually becomes an unbearable drag on software output and developer productivity. It eats up engineering time leaves little for generative development work.

## Developers could be 46% more productive

In another question, Stripe asked developers to rate the productivity of their engineering teams on a scale of 0–100%. The average response? ****68.4%****:

![](/content/images/2021/05/how-productive.png)
*Source: The Developer Coefficient, Stripe*

Said differently, the average developer could potentially be (100% — 68.4%) / 68.4% = ****46%**** more productive that they are today, nearly 50%.

With a 41.1 hour work week, such a productivity boost would be the equivalent of an ****additional ~19 hours of productive development work****, enough to completely compensate for all that time spent on technical debt and bad, buggy code.

![](/content/images/2021/05/extra-productive-hours.png)

## A $425B dollar bill on the ground

Based on the survey, Stripe conducted a back-of-the-envelope calculation multiplying estimates of the value generated by software developers around the world with the estimated productivity losses to arrive at an estimate for the global GDP lost due to software developer inefficiency.

I take issue with some of the assumptions, but the calculation is illustrative regardless. Assuming $900B of aggregate developer GDP and 31.6 percentage points of productivity lost suggests a $300B annual GDP shortfall:

![](/content/images/2021/05/stripe-calc.png)
*Source: The Developer Coefficient, Stripe*

This is an underestimate, in fact. Stripe’s math assumes 100% — 68.4% = 31.6% lost relative to existing productivity, but as I showed above, it’s in fact 31.6% / 68.4% = 46%. With this efficiency loss relative to maximum productivity, GDP lost to developer inefficiency grows to ****~$425B****.

So our failure to maximize productivity is losing us hundreds of billions in software production. Not exactly chump change.

## SUM()-ing it all up

To end, I want to return to where I started qualitatively, my goal to increase total software output, and quantitatively, the decomposition of software output into developers and developer productivity, and finally connect the two perspectives.

****How much more software output could we get with more developers and higher productivity?:****

![](/content/images/2021/05/image-20210425201756326.png)

Let’s start with developers. I tend to be skeptical of perennial “shortages” unless there’s some specific, identifiable reason for it. Regardless, estimates suggest there’s a ~3M shortage of software developers globally, with [1.4M unfilled computer science jobs](https://www.daxx.com/blog/development-trends/software-developer-shortage-us) in the US alone.

To make the numbers easy, let’s round up Stripe’s estimate for the global software engineering labor force from 18M to 20M (I’ve seen other estimates in the 20–25M range, so this feels reasonable). That would imply a 3M / 20M = 15% developer shortage at current levels of demand.

So we have a 15% developer shortage and a 50% productivity “shortage”. Multiplied, that yields a massive ****73% potential gain in software output****:

![](/content/images/2021/05/software-calc.png)

Said another way, ****current software output is only about 100% / 173% = 58% of what it could be****.

That’s ****$670B**** of software we’re leaving on the table.

This calculation is admittedly simplistic. One could argue that we’d need fewer developers if they were more productive. I push back on that — there’s so much we’ve yet to build, and more developers working more productively would unlock entirely new opportunities that we haven’t had the capacity to explore or can’t even yet imagine. This would further spur developer hiring and employment, another [flywheel](https://nnamdi.net/the-developer-productivity-flywheel/).

## We can do better

In this series I’ve made the case that by investing in [__technical tools for technical people__](https://nnamdi.net/about-me/), we can reverse the trend of declining developer productivity, spin the productivity flywheel in the right direction, and see massive gains in software output as a result.

As I’ve hopefully made clear, we can do A LOT better of a job maximizing developer productivity. I hope you’ll join me in my mission to do exactly that.

__Thanks so much for reading this — I hope it resonated with you. If it did, please share that feedback!__

****Follow me on**** [****Twitter****](https://twitter.com/whoisnnamdi)****, subscribe to my monthly essays**** [****here****](https://nnamdi.net/)****, and reach out to me directly via nnamdi@lsvp.com**** ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[The Developer Productivity Manifesto Part 2 — More (Developers) Isn’t Always More]]>
                        </title>
                        <description>
                            <![CDATA[Adding more cooks to the kitchen rarely helps]]>
                        </description>
                        <link>https://whoisnnamdi.com/more-developers-isnt-always-more/</link>
                        <guid isPermaLink="false">more-developers-isnt-always-more</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Developers ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Fri, 16 Apr 2021, 06:13:00 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512244606-ballmer-compressed.gif" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
In software development, more (developers) isn’t always more.

In [part 1](https://nnamdi.net/the-developer-productivity-flywheel/) of “The Developer Productivity Manifesto,” I argued that developer productivity, measured in terms of new software, falls over time as low-hanging fruit is picked and new ideas become harder to find.

An understandable response would be — why not just throw more engineers at the problem?

If you’ve ever worked in software development, you know adding more cooks to the kitchen rarely helps. For those who __haven’t__ worked in software development, this essay tells you why.

The Developer Productivity Manifesto has three parts, this is part 2:

-   [Part 1: The Developer Productivity Flywheel](https://nnamdi.net/the-developer-productivity-flywheel/)
-   ****Part 2: More (Developers) Isn’t Always More (you are here)****
-   [Part 3: Leaving Software on the Table](https://nnamdi.net/leaving-software-on-the-table/)

### Receive my next long-form post

Thoughtful analysis of the business and economics of tech

 

Subscribe

## The Mythical Man-Month

![](/https://nnamdi.net/content/images/2021/05/image-20210405202322027.png)

[“The Mythical Man-Month](https://www.amazon.com/Mythical-Man-Month-Software-Engineering-Anniversary/dp/0201835959)” is a classic and hugely influential book about software engineering and project management. Across a series of essays, Fred Brooks (himself a former IBM project manager) lays out the cognitive errors software development teams tend to make in estimating the time to completion for software projects. His core thesis is best encapsulated by [Brooks’ law](https://en.wikipedia.org/wiki/Brooks%27s_law).

Brooks’ law of software project management states that:

> __Adding manpower to a late software project makes it later__

If anything, adding more cooks to the kitchen __lengthens__ cooking time. We shouldn’t assume a simple, linear, and directly causal relationship between developers as inputs and software as output. He lays this out in dire terms:

> __… when schedule slippage is recognized, the natural (and traditional) response is to add manpower. Like dousing a fire with gasoline, ****this makes matters worse, much worse****. More fire requires more gasoline, and thus begins a regenerative cycle which ends in disaster.__

![](/https://nnamdi.net/content/images/size/w2000/2021/05/ballmer_600.gif)

This counterintuitive phenomenon has multiple causes:

-   ****Ramp up time:**** Even seasoned developers take time to get up to speed on new projects, and newbie engineers must learn core skills on top of company-specific ones
-   ****Communication and coordination complexity:**** The larger the team, the harder it is to coordinate productive work and communicate progress across teammates
-   ****Indivisibility of work:**** The basic unit of work in software development can’t always be divided among multiple contributors

That last point forms the basis of the title and main thrust of the book, the fallacy of “man-months” — distinct units of work achievable by a single developer in a set period of time. As Brooks argues, there is no such thing, and thus we should be wary of simplistic solutions to complex endeavors like software development.

> __Software is not labor-intensive. Not many people are necessary in order to produce good software… What makes or breaks a project, it’s the amount of FOCUS developers can pour into it — [RedBeardLab](https://redbeardlab.com/2020/01/19/software-is-a-focus-intensive-industry/)__

## Diseconomies of scale

![](/https://nnamdi.net/content/images/2021/05/image-20210405115615714.png)
*Source*

While it is common to assume __economies of scale__, ****diseconomies of scale**** are arguably just as relevant in software development.

Diseconomies of scale are where unit costs (the costs of producing an additional unit of output) increase rather than decrease with scale. Here, it’s better to __cut back__ or __lower__ output, rather than maximize it. Despite many advances, these occur more often than we’d like to admit in modern software development.

Diseconomies of scale can take many forms, and many overlap with the underlying causes behind the mythical “man-month”:

-   ****Complexity:**** Things become disproportionately complex as they scale, as complexity increases non-linearly with size. Complexity creates overhead, making large organizations less efficient than medium-sized ones. Bureaucracy is one manifestation, but there are others.
-   ****Black Swans:**** Large systems fail in spectacular fashion. It’s why big companies create systemic risk while small businesses and startups fail in the thousands without cause for alarm. Within software, this might be a large monolithic application, prone to serious, single point failures.

Humans, being the independent and unpredictable automatons we are, are especially prone to diseconomies of scale. Coordination costs eventually overwhelm even the most thoughtful engineering leaders. Application deployments are themselves quite brittle, necessitating vastly more manpower and attention as they grow.

## We will never have enough software developers

> __Senior executives report that the lack of developer talent is one of the biggest potential threats to their businesses — [The Developer Coefficient 2018, Stripe](https://stripe.com/files/reports/the-developer-coefficient.pdf)__

Executives believe that insufficient developer talent is one of the biggest threats to their business, and yet there’s good reason to think this might never be solved.

![](/https://nnamdi.net/content/images/2021/05/image-20210405115744208.png)

As I discuss in a previous essay, [Why We Will Never Have Enough Software Developers](https://nnamdi.net/never-enough-developers/), changes in the underlying technologies of modern software development whittle away the accumulated human capital of software developers:

> __****Specific skills in software development quickly become dated.**** Programming languages and development frameworks go out of style. Hadoop is hot one year, and it’s old news the next. Like a fast, expensive car that quickly loses value as it’s driven around town, the skills and human capital of software engineers fall apart without constant, expensive maintenance__

Though young engineers can keep up with the latest programming languages, frameworks, and tooling, eventually the torrential wave of new tools becomes too much to bear, and developers either tune out or drop out:

> __At age 26, 59% of engineering and computer science grads work in occupations related to their field of study. By age 50, only 41% work in the same domain, meaning a full ****~30% drop out of the field by mid-career****__

The never-ending drumbeat of new technologies drives developers out of the field. New tooling is important and valuable, but their overall effect on developer productivity depends on how much they upend existing workflows and place additional burden on already taxed developers.

As I conclude:

> __****Growing the supply of software developers is not trivial**** because the field already sees high levels of developer dropout and turnover, and this would only increase if the field were to grow larger.__

I favor efforts to grow the software engineering talent pool, but if we’re not careful, like quicksand, our efforts will be counterproductive.

## More developers, lower productivity

As we saw in part 1, more researchers don’t necessarily lead to faster progress. In the case of Moore’s Law, it merely maintains the rate of progress, at significant expense.

But is this true of the economy overall? It turns out, the answer is yes. Cross-industry data proves that simply throwing more people at a problem doesn’t work.

The chart below plots the growth in the number of employed researchers and their productivity levels (measured in terms of revenue growth generated per researcher) for ~1,700 publicly-traded U.S.-based companies over a 20-year period. Most firms “threw more bodies at the problem” (orange histogram > 1), but ****~80% of firms saw declining research productivity**** (blue histogram < 1):

![](/https://nnamdi.net/content/images/2021/05/0_4RM_2yR4dbE_6mb5.png)
*Source*

Hiring more researchers doesn’t necessarily __cause__ productivity to decline, but regardless, growing research teams and declining productivity go hand-in-hand more often than not.

Software development could fall prey to the same trap. Most companies are growing the number of software engineers they employ, mirroring the researcher employment trends, but let’s not forget productivity:

![](/https://nnamdi.net/content/images/2021/05/0_NHviru2DykbIv-9N.png)

Research productivity, and analogously developer productivity, falls over time unless acted upon by an outside force. Like gravity, this phenomenon is pervasive and ubiquitous, a force field dragging down all idea-generative industries.

## Busy work doesn’t work

I love this quote from Stripe:

> __Despite the number of developers increasing year-over-year at most companies, ****developers working on the right things can accelerate a company’s move into new markets or product areas and help companies differentiate themselves at disproportionate rates.**** This underscores the most important point about developers as force-multipliers: ****It’s not just how many devs companies have; it’s also how they’re being leveraged**** — [The Developer Coefficient 2018, Stripe](https://stripe.com/files/reports/the-developer-coefficient.pdf)__

In my last essay I talked about the importance of measuring productivity in terms of new software output. Another reason why more engineers is not necessarily the solution is that much of the value of the marginal engineer is in their ****innovation**** activities rather than their __maintenance__ work.

We can quantify the impact of engineering time spent maintaining old code rather than writing new code. According to [one analysis](http://drock.mit.edu/sites/default/files/documents/techMV_mostrecent.pdf), an engineer engaged in purely non-innovative activity __destroys__ nearly $600K in employer market value. On the other hand, the average engineer, working on a combination of maintenance and innovation activities, ****adds**** $855K in market value to their employer.

![](/https://nnamdi.net/content/images/2021/05/image-5.png)

As the study’s author speculates:

> __… the value of the engineer is a bundled combination of ****maintenance activities which have negative value**** and ****innovation activities with positive value****__

Further, he echoes Brook’s Law:

> __It may also be the case that ****more engineers does not always make for easier problem solving**** and on the margin, removing innovative activity, \[they are\] a net drain on the firm’s value__

I want to be clear: maintenance matters too. When things break, as they inevitably do, development teams must stand at the ready to fix problems and bring systems back online. This is critical work that should not be minimized in a narrow pursuit of newer, shinier objects.

That said, mere maintenance is table stakes. It doesn’t pay the bills — an engineer’s salary, first and foremost.

## Hidden figures

Again, Stripe gets it right:

> __While many people posit that lack of developers is the primary problem, this study… found that ****businesses need to better leverage their existing software engineering talent**** if they want to move ****faster****, build ****new**** products, and tap into ****new and emerging**** trends — [****The Developer Coefficient 2018, Stripe****](https://stripe.com/files/reports/the-developer-coefficient.pdf)__

Notice the emphasis on newness and speed. We can and should grow the talent pool for software engineering, but we can also do a much better job with the engineering talent __we already have__.

Maintenance matters but so does fundamental innovation. As software projects grow, and their teams with them, thoughtful engineering managers must strike the right balance or see their most precious resource go to waste.

Industry-wide we are, unfortunately, out of balance. In my next and final piece, I’ll explore exactly how much software we’re “leaving on the table” as a result.

Ready for more? Here's [Part 3](https://nnamdi.net/leaving-software-on-the-table/).

****Follow me on**** [****Twitter****](https://twitter.com/whoisnnamdi)****, subscribe to my monthly essays**** [****here****](https://nnamdi.net/)****, and reach out to me directly via nnamdi@lsvp.com**** ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[The Developer Productivity Manifesto Part 1 — The Flywheel]]>
                        </title>
                        <description>
                            <![CDATA[Developer productivity is falling. But it doesn't have to. The solution? The Developer Productivity Flywheel]]>
                        </description>
                        <link>https://whoisnnamdi.com/the-developer-productivity-flywheel/</link>
                        <guid isPermaLink="false">the-developer-productivity-flywheel</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Developers ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Wed, 31 Mar 2021, 05:40:00 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512240140-header.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
__TL;DR: I invest in productivity-enhancing tools for software developers and other technical knowledge workers. If you’re building something along these lines, ping me at nnamdi@lsvp.com__

I’ve been obsessed with developer productivity for a long time.

As developments within software tooling, application infrastructure, and data science over the last decade have vindicated my quixotic enthusiasm, I thought I’d finally articulate my rationale in writing.

Manifestos should have mission statements. I’ve always loved [Stripe’s](https://stripe.com/about):

> __Our mission is to increase the GDP (gross domestic product) of the internet__

They say great artists steal. My artistic design skills are, frankly, terrible (especially relative to Stripe’s), but I’ll steal some inspiration anyway and lay out my mission as follows:

> __My mission is to increase total software output__

![](/content/images/2021/05/image-1.png)

Developer productivity is the best way to achieve this. This essay explains why.

The Developer Productivity Manifesto has three parts, this is part 1:

-   ****Part 1: The Developer Productivity Flywheel (you are here)****
-   [Part 2: More (Developers) Isn’t Always More](https://nnamdi.net/more-developers-isnt-always-more/)
-   [Part 3: Leaving Software on the Table](https://nnamdi.net/leaving-software-on-the-table/)

### Receive my next long-form post

Thoughtful analysis of the business and economics of tech

 

Subscribe

## Every company is becoming a software factory

![](/content/images/2021/05/0_A8QJ_-v0lvk9FBMK-1.jpg)

It’s been said before, and it’s worth repeating: [every company is becoming a software company](https://www.confluent.io/blog/every-company-is-becoming-software/).

Like Henry Ford’s assembly line 100 years ago, which revolutionized mass production of physical goods, innovation in the production of complex, intricate software products and services promise to transform modern software development.

The Software Revolution __is__ the new Industrial Revolution.

Agile, scrum, waterfall etc. — the various paradigms analogize the factory assembly line and modify it for the modern context. They pose the provocative question, ****“What can the world of bits learn from the world of atoms?”****

In this light, I modify the above: every company is becoming a software ****factory****.

But analogies to old-school industry only take us so far.

In the factories of old, labor was poorly-skilled, poorly educated, poorly treated, and poorly compensated. Today, as before, labor remains the key input to production. However, in the modern __software factory__, labor (i.e. software developers) is highly-skilled, well-educated, well-treated (at least in more progressive companies), and mostly importantly, __well-paid__.

My goal as an investor and citizen of Silicon Valley is to increase total software output. Maximizing the software output requires understanding what I call, the ****software production function**** — how we map from inputs to outputs in software development. The exact function is likely unknowable, but we know for sure developers are __the__ key input. Therefore, let’s start with this simple mapping:

![](/content/images/2021/05/0_Ol16aBbPuF5r3spw.png)

Basic enough, but we can do better. Economists often decompose variables into the __extensive__ (“how much input” / “how many software developers”) and __intensive__ (“how much per input” / “how productive”) margins:

![](/content/images/2021/05/0_NoQVuock22zuDfIb.png)

So, we can grow software output in two ways: ****more developers**** or ****higher developer productivity****.

## Why we should care about developer productivity

> __“While many people posit that lack of developers is the primary problem, this study… found that ****businesses need to better leverage their existing software engineering talent**** if they want to move faster, build new products, and tap into new and emerging trends” — [****The Developer Coefficient 2018, Stripe****](https://stripe.com/files/reports/the-developer-coefficient.pdf)__

Software developers are the scarce, precious resource of software development — extremely expensive to obtain, train, and retain. ****Anything that makes software developers more productive will itself be highly valuable.****

However, productivity in software development tends to __decline__ over time rather than increase.

This is a counterintuitive claim, so let’s unpack it.

First, it helps to delineate two different types of production — the production of tangible and intangible goods:

![](/content/images/2021/05/0_pmFjSDd8dI9N6Zya.png)

****Tangible goods**** include traditional, physical products and services like cars, televisions, clothing, etc. Importantly, due to their tangibility, replicas have value. Two cars are better than one, and so on. It therefore makes sense to talk about gross “units” of production — cars assembled, televisions manufactured, etc.

On the other hand, ****intangible goods**** constitute non-physical products like ideas, patents, and… software. Intangibles are infinitely reproducible at nearly zero marginal cost. As such, it doesn’t make sense to measure intangible output in terms of __gross__ “units’’ of output. Rather, only __net new__ output matters.

Software is one such intangible. Copying existing code is as easy as running git clone on a repository. This alone does not generate incremental value — the value was in writing the original code.

With intangibles, ****novelty**** is what matters: new ideas, new designs, and new software. There’s a reason you can’t patent something already patented — there’d be no value in doing so. Similarly, new code moves the ball forward.

Let’s edit our previous equation to emphasize novelty:

![](/content/images/2021/05/image-2.png)

It turns out, this trivial change makes a ****huge**** difference.

Why? Economic evidence suggests that, unlike physical goods, idea productivity tends to decline over time. The intensive margin of the “idea production function”, i.e. the number of new ideas generated by a given number of researchers, falls dramatically as a field progresses:

![](/content/images/2021/05/image-3.png)

​Don’t believe me? Here’s one example that should resonate with fellow technologists: ****Moore’s Law****.

## Are ideas getting harder to find?

![](/content/images/2021/05/0_xguhYnmT9lSWPdzn.png)

[Moore’s Law](https://en.wikipedia.org/wiki/Moore's_law) describes how manufacturers cram twice as many transistors onto computer chips every two years.

Sustaining this self-fulling prophecy of “2X every 2 years” requires massive research teams to churn out new ideas and insights around chip design and manufacturing. It’s clear from the data that Moore’s Law is not some __a priori__ law of the universe, but rather a ****goal**** set by chip manufacturers and researchers:

> __“Many commentators note that Moore’s Law is not a law of nature but instead results from intense research effort: doubling the transistor density is often viewed as a goal or target for research programs.” — [****Are Ideas Getting Harder to Find?****](https://web.stanford.edu/~chadj/IdeaPF.pdf)__

Transistors are tangible, but the __ability__ or __know-how__ to condense their size over time is an idea and, therefore, intangible. Intellectually, we should separate the __physical__ production of literal computer chips from the __ideas__ that enable this feat.

As it turns out, the old ideas just won’t do — what got you __here__ won’t get you __there__.

In a fascinating and landmark [paper](https://web.stanford.edu/~chadj/IdeaPF.pdf), Stanford and MIT economists Nicholas Bloom, Charles Jones (hey professor!), John Van Reenan, and Michael Webb frame the constant growth in the density of transistors as evidence of a constant flow of new ideas and innovations:

![](/content/images/2021/05/0_jP2mHx71qNSVYIJ6.png)

If idea output is constant, growth in the number of researchers implies shrinking research productivity. If the flow of new ideas in semiconductor manufacturing is constant (as evidenced by Moore’s Law roughly holding steady) and the number of chip researchers increases over time, then we know (via simple arithmetic) research or idea productivity ****must**** decline:

![](/content/images/2021/05/0_S7ptnoyf9hy99zxq.png)

​And that’s exactly what we see. Generating constant growth in the number of transistors on a chip has required many more semiconductor researchers and scientists over time, ****about 18 times as many in 2014 relative to 1971****:

![](/content/images/2021/05/0_s5vJJ4hKzJd4QmVq.png)

Measured in terms of transistor density gains, ****research productivity has declined 18X over a 45-year period****, which is to say it’s about ****5%**** of what it used to be, a dramatic, precipitous decline in idea productivity. And as presented in the paper, the same phenomenon holds true in many sectors, such as agricultural and medical research.

The evidence is clear: ****new ideas are hard to find and only getting harder****. Analogously, if we assume new software represents new ideas, it only follows that new software is increasingly difficult to create too.

The work of software developers is analogous to semiconductor R&D. ****Engineering teams are tiny idea factories****, and new ideas get harder to produce over time.

__This__ is why developer productivity falls over time. The ability of software developers to write novel applications tends to decline over time, as happens in almost any idea-centric production process. When so much code has already been written, it’s difficult to improve upon the status quo.

## The developer productivity flywheel

Breaking away from this gravitational, productivity-sucking force field won’t be easy. However, we can reach escape velocity by aggressively enhancing developer productivity rather than watching it deteriorate.

Flywheels help technology startups achieve velocity, and software development is no exception. Since no mention of flywheels in a technology essay is complete without a visualized loop, here it is:

![](/content/images/2021/05/image-4.png)

To explain:

1.  ****New developer productivity tools make software developers more productive.****

Simple enough, though we shouldn’t be so naive as to think it [always works out so cleanly](https://www.derrickreimer.com/essays/2018/03/02/the-war-on-developer-productivity.html). New tools initially impair developer productivity, as individual contributors and teams adjust to new software, interfaces, and workflows. Complex tooling can drive productivity through the ground if recklessly implemented.

This dynamic generates the familiar “J-Curve” of initially declining productivity before tangible benefits are eventually realized:

![](/content/images/2021/05/0_pTVPwZPyFQf8MbbR.png)
*Original version*

So I should be precise — new productivity tools __thoughtfully applied__ make developers more productive. That productivity may not immediately materialize, either because it’s [difficult to measure](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3346739) (Commits? Lines of code? Shipped releases?) or because adjustment takes time. This only reinforces the importance of investing sooner, as to frontload the trough.

****2\. Higher developer productivity drives companies to hire more software engineers.****

Remember your supply and demand curves from [economics 101](https://www.khanacademy.org/economics-finance-domain/ap-microeconomics/factor-markets/ap-labor-marginal-product-rev/v/shifts-in-demand-for-labor)? Higher productivity pushes out the developer demand curve. Companies take advantage of enhanced productivity by hiring more developers and paying them more too:

![](/content/images/2021/05/0__phAC3txzmraNUQN.png)

In some cases, the enhanced productivity not only makes existing developers more productive but lowers the bar enough that others can now become software developers too. This pushes out the the developer supply curve, increasing employment further:

![](/content/images/2021/05/0_f1Yu2MIu3fMkyDFN.png)

****3\. More developers working at higher productivity levels ship more software, a subset of which is itself developer productivity tooling.****

As previously discussed, if we simply increase the number of developers without maintaining productivity, we merely tread water. What’s crucial here is that developer productivity and employment __rise together__. If they do, we get more software output, and that new software will itself drive higher productivity.

****4\. Loop****

For the technically inclined, here’s how the logic goes in code form (much more compact 😅):

![](/content/images/2021/05/dev_prod_flywheel.py.png)

In other words:

![](/content/images/2021/05/0_6kZsFEsFISGZIQMY.png)

This is what I like to call, ****the developer productivity flywheel****.

The future of productive, economical software development rests on running this flywheel as quickly as possible, iteratively looping through virtuous cycles of productivity, employment, and software output growth.

However, there is “[no silver bullet](https://www.cgl.ucsf.edu/Outreach/pc204/NoSilverBullet.html).” There isn’t “one weird trick” to massively supercharge developer productivity.

Instead, we must attack the problem from __every available angle__ if we are to succeed.

Ready for more? Here's [Part 2](https://nnamdi.net/more-developers-isnt-always-more/).

****Follow me on**** [****Twitter****](https://twitter.com/whoisnnamdi)****, subscribe to my monthly essays**** [****here****](https://nnamdi.net/)****, and reach out to me directly via nnamdi@lsvp.com**** ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Four Challenges Facing Developer Productivity Startups]]>
                        </title>
                        <description>
                            <![CDATA[The biggest challenges facing developer productivity startups today]]>
                        </description>
                        <link>https://whoisnnamdi.com/developer-productivity-challenges/</link>
                        <guid isPermaLink="false">developer-productivity-challenges</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Developers ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Thu, 11 Mar 2021, 08:05:00 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512235438-header-1.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
Developer productivity is undergoing a tectonic shift. New software development paradigms and tooling have accelerated the pace and productivity of modern software teams, quickening the "shipping speed" of new software.

To better understand the strategic landscape, my good friend and colleague, [Clio Smurro](https://www.linkedin.com/in/clio-smurro-31967b9/), and I interviewed founders and executives at next-generation software and infrastructure startups pushing the developer productivity frontier to get their thoughts and insights. They shared their views on:

-   [major industry trends](/developer-productivity-trends/),
-   [top strategic priorities](/developer-productivity-strategic-priorities/), and
-   biggest challenges and pain points (you are here)

In this third and final chapter, we share our findings on the **top challenges and pain points** facing developer productivity startups, including:

-   [Defining and planning for success](#definingandplanningforsuccess)
-   [Integrating with other technologies](#integratingwithothertechnologies)
-   [Building mindshare](#buildingmindshare)
-   [Scaling developer relations](#scalingdeveloperrelations)

Subscribe below to receive a nicely formatted PDF of our research!

### Receive a report with the full results

  

Send Report ⚡

## Defining and planning for success

It's common for developer productivity startups to take an engineering-first approach to development, where engineers take the lead on deciding what to build and when to build it, often leveraging personal intuitions as consumers of the product.

> We don't have PMs. We decide what to build democratically. For each sprint, everyone comes up with an idea and we discuss — Manager, Developer Tools Startup

While this works initially, without some sort of formalized product management function, teams soon hit a wall of confusion:

> We emulated the Stripe model initially, focusing on engineering and growth. We don't have any PMs and so we don't have a clear framework for what to build — Manager, Developer Tools Startup

It's difficult to chart out long-term priorities and product roadmaps that support the business and revenue generation without product-minded individuals that can liaison between the technical and commercial needs of the company.

If not tackled early, the more haphazard approach to product development can lead to a nonsensical multi-year roadmap of features no one can rigorously justify. With no North Star, new, potentially interesting ideas get shut out and ignored, for lack of space:

> We know all the things we would like to build but our roadmap is full — Director, Data Science Startup

To where should developer productivity startups look for signals on what to build next? Customers, ideally. But it's not always that simple. Depending on the customer, signals can be more or less easy to detect. Self-serve users, for example, often silently churn, never indicating what drove the decision:

> Self-serve users don’t really tell you what they want, they just churn if they are unsatisfied — Manager, Developer Tools Startup

Telemetry can drive insight into these churn patterns. The problem, however, is that many developer productivity tools are open source, so users typically have the ability to disable usage tracking, cutting off a vital information source. Many tools launch without any tracking in the first place, later causing a riot among their community when implementing telemetry later.

When we can collect user feedback, the next question arises — what to do with it? Should we first solve the problems of our loudest, most-vocal users? Should we focus on features that will help us close revenue in the near-term, perhaps even features that were specifically asked for by prospects? These are all valid questions with no easy answer.

> This is a general problem that all dev tools companies run into... building features that specific customers need so that you can close the deal, without any holistic product management framework — Manager, Developer Tools Startup

Though it's somewhat unknowable, developer productivity companies must grapple with and come to a view on the state of the product and whether it's ready for prime-time. One could always collect more data, but eventually you reach critical mass, and decisions can be made. Knowing when you've crossed this key milestone is critical. In other words, **what does success look like?**

Before reaching this point, it can be hard to tell whether you are on your way or marching in an entirely wrong direction product-wise:

> How do you know when you know enough? How do you know when you've seen enough? Are you consensus or non-consensus? If you talk to the market and get 100 "NO"s, but 5 "YES"es, are you right about your idea or are you wrong? — Founder, Data Science Startup

> How do you know if you need to change the product geometry? Also, how do you set the right KPIs? — Founder, Data Science Startup

Once product-market fit has been confidently achieved, it's time to pour fuel on the flame and scale up the team. As the team grows, engineering processes inevitably need to change to keep up. This is a key advantages of big tech organizations like Google or Amazon — they've built development processes that work at scale and have been doing so for some time. Developer productivity startups must also set themselves up for success:

> Having an effective engineering organization will be a key advantage for us. Big companies like Google and Amazon don’t just have innovative products. They also have innovative engineering processes — Founder, Application Infrastructure Startup

## Integrating with other technologies

Few developer productivity startups begin by building an end-to-end solution. Invariably, developer productivity products integrate with other parts of the software development toolchain.

> The ML landscape is very messy; very noisy. For the different steps of the ML workflow, people will use the tools which integrate best with each other — Founder, Data Science Startup

> Tools should follow UNIX philosophy of working well individually AND integrating with other tools well — Founder, Data Science Startup

It turns out, this is a different skill set from core product building, as many quickly find out:

> Seamless integration to our partners technically is challenging. We’re trying to mitigate that by hiring very well-seasoned experts in each of the areas that we’re growing — Founder, Data Science Startup

Integrating with other technologies often involves hiring that particular skill set into the organization. Though it's possible to build integration into tooling that the team doesn't have personal experience with, teams with first-hand knowledge of the complementary technology build the best integrations.

Integration is as much a technological problem as a people problem:

> And the human aspect...if you make this huge effort, you want to make sure it’s not just you entering their community, but them really integrating with yours — Founder, Data Science Startup

> We need to go one by one in building integrations with each of these open-source communities — Founder, Data Science Startup

Communities already exist around the technologies with which developer productivity startups want to integrate. **Ingratiating** oneself with these communities is as important as **integrating** with their tooling.

> We’re open source...we have to go into their databases and make sure we get buy-in from their existing developers — Founder, Data Science Startup

Greasing the wheels in this way helps build goodwill among users of the target technologies, which can open doors. The core contributors of the target technology may make an architectural change on their end that makes your integration work much easier. Mutual trust and respect enable this kind of deep collaboration.

Further, people who've spent time in these other ecosystems bring a wealth of knowledge and insights, and can help fledgling developer productivity startups avoid the mistakes of their elders:

> Our biggest challenge...we want to learn from each of these communities, and make sure that we’re not repeating any mistakes — Founder, Data Science Startup

Developer productivity startups can't afford to integrate with everything. Resources are limited, and integration work is often not a core competency. Startups must prioritize different integration options. Like sizing up the market opportunity for an entire company, startups should evaluate the market potential of complementary technologies:

> Postgres and MySQL….you’ll cover ~70% of the market if you serve those two — Manager, Developer Tools Startup

The proliferation of developer tooling makes it harder make such decisions:

> Addressable market for applications is large, but that makes it harder to focus. We need to make sure we’re strategic in choosing which applications to prioritize — Founder, Application Infrastructure Startup

Even among the tools that teams make an affirmative decision to integrate with, quality will vary meaningfully:

> We serve like 45 different tools with integration. We do a really good job at serving a few key integrations — Manager, Developer Tools Startup

## Building mindshare

Mindshare always precedes dollar share. Potential users make important architectural decisions early, requiring that they know about your technology well ahead of project intiation. Further, up-and-coming vendors must navigate any risk aversion or reticence towards new tooling, which is easier said than done:

> Unless developers have the foresight to future proof, they’ll still be building with Postgres or some other older database. We often hear "we'll start with MySQL and the move to you guys if we run into issues" — Executive, Application Infrastructure Startup

Methods for building mindshare are many and varied. Some are subtle. One answer that surprised us but makes complete sense in retrospect? Online classes. Online courses are a common learning vehicle for developers, and the content of most courses naturally tends to skew toward more mature technologies with larger userbases.

This is a sort of network effect whereby MongoDB developers, for example, benefit from the presence of other MongoDB developers because those same developers create an ecosystem of learning content around the database, flattening the learning curve:

> An example of mindshare: online courses are teaching other databases, not ours — Executive, Application Infrastructure Startup

If the community isn't large enough, vendors themselves can help cover the gap. Tutorials and documentation help developers both learn how to use a tool but also learn of a tool in the first place. This is classic content marketing, with a developer-focused spin:

> One challenge we face is improving our content. Writing tutorials, making videos, helping people use the product better — Manager, Developer Tools Startup

Mindshare has been especially difficult to build during the era of COVID-19, which has put a damper on in-person meetups and other ways in which upstart developer productivity companies build community and evangelism. Startups have rethought more formal events like conferences:

> Evangelism has been stymied. The developer ecosystem is very meetup-driven, so that’s been cut — Executive, Application Security Startup

> Conferences used to be really important, so we’ll see how being virtual affects that — Developer Advocate, Application Infrastructure Startup

These events often serve as meaningful lead generation channels for developer productivity companies, leading to potential pipeline impacts:

> We have a strong pipeline, but we need to make sure we keep that up, especially with everything being virtual now — Developer Advocate, Application Infrastructure Startup

Mindshare, once achieved, can act as powerful social proof. Though many developers claim to be driven by first principles in their design decisions and choice of tools, like all other homo sapiens, they care about what other people think. C-suite buyers care even more about the choices of their peers, so high quality customer logos definitely matter and should be displayed prominently:

> We need more logos on our website. Getting named case studies is key for us. CIOs want to know that someone else is using this thing — Executive, Application Infrastructure Startup

## Scaling developer relations

Speaking of building mindshare, developer relations (DevRel), also known as developer evangelism, developer advocacy, developer experience, etc. has emerged as an incredible way for developer productivity startups to ramp up mindshare and market awareness. Developers may not like being sold to, but they don't mind nerding out on cool tech with someone who speaks the same language:

> Developers don't want to hear from sales folks. They don't want to be told what to do by their CIO. They want to be part of broader community, contribute, participate — Developer Advocate, Analytics Infrastructure Startup

> Developers don't like being sold to, but they trust other developers — Developer Advocate, Application Infrastructure Startup

Simple enough. But even if you're fully onboard with the idea of developer relations, doing it well is tough. To start, developer advocates are hard to come by. There just aren't many DevRel folks out there. The function is still relatively new, and career pipelines into the role have yet to fully materialize:

> There is a serious lack of talent for DevRel. The ones who are good have already been hired. And the ones who could be good often have a hard time getting a foot in the door without a Twitter presence — Developer Advocate, Application Infrastructure Startup

> DevRel is a hard spec to hire for. Technical folks are usually introverts. Extroverts are typically not technical — Developer Advocate, Analytics Infrastructure Startup

Knowing who to hire among available candidates is also tricky. Companies often use social media following as an indicator of DevRel potential. But nearly of the DevRel professionals we spoke to cautioned _against_ using social media following as an rubric for hiring developer advocates, with many saying it's entirely unnecessary for the job. Further, a large social media following is only helpful if it's backed up by **technical ability** and **credibility** with the developer community:

> You don't really need to have a following to be good at DevRel. It's much more important to have street cred \[with developers\] — Developer Advocate, Analytics Infrastructure Startup

> Social media is just one part of the toolbox. Further, social following in one area does not necessarily translate to another — Developer Advocate, Analytics Infrastructure Startup

> There are lots of people with big social media following who no one actually likes — Developer Advocate, Application Infrastructure Startup (author note: 🤣)

Once the DevRel team begins to scale, questions emerge around where it fits within the broader org chart and hierarchy. This quickly gets contentious, and rarely do DevRel teams get to decide their own fate. This leads to bad situations where DevRel ends up in a part of the organization that isn't truly aligned with the practice, or worse, DevRel ends up straddling multiple functions:

> Developer relations often gets stuck in the middle between different functions — Developer Advocate, Developer Tools Startup

> In the past people didn't really understand DevRel. This lack of understanding drove poor outcomes. Future will be DevRel as its own organization — Developer Advocate, Application Infrastructure Startup

Poor understanding of what DevRel is and its potential drives significant consternation. Again, because there are so few of these individuals and the function is so nascent, identifying enlightened executives and manager to lead these efforts is not trivial. With the right leadership, however, DevRel can thrive:

> It's much better to have someone run the team who is involved in developer relations themselves. Having adamant buy in really helps — Developer Advocate, Application Infrastructure Startup

But inevitably the question emerges — how should we measure DevRel's successes (or failures for that matter)?

> A lot of people don't know how to be effective in DevRel. You need analytics of some sort. You need to understand your funnel — Developer Advocate, Analytics Infrastructure Startup

This is another point of serious contention within the developer relations community. There are no standard metrics or KPIs in DevRel, leading to difficult conversations within the DevRel team and the rest of the company about whether and how DevRel is pulling its weight:

> Metrics are tough. The metrics you end up measuring are often defined by the function you fall under — Developer Advocate, Developer Tools Startup

> You will just be looked at as a cost center if you can't quantify this stuff. Really you are revenue generating, and you need to think of yourself that way — Developer Advocate, Analytics Infrastructure Startup

Though there may not be a perfect KPI for DevRel that works for all companies and communities, in picking a set of metrics, it helps to keep in mind a "North Star" tied to the fundamental values and purpose of the business:

> Define success by how you serve other developers. Are you giving them what they need so they can build? — Developer Advocate, Application Infrastructure Startup

## Conclusion

It was amazing to speak to so many developer productivity founders and operators about the major trends shaping their businesses, their strategic priorities, and their biggest challenges and pain points as they've scaled up. The discussions around challenges were especially humbling. With all the glitz and glamour around startups and Silicon Valley these days, it's easy to forget the fundamentals — building a company in any sector is really, really hard, and developer productivity is no exception.

We hope you've enjoyed this series on developer productivity. We'd love to continue the conversation. If you are a developer productivity founder or operator who's resonated with any of these findings, let's chat!

[Clio Smurro](https://www.linkedin.com/in/clio-smurro-31967b9/) & [Nnamdi Iregbulem](https://www.linkedin.com/in/nnamdiiregbulem/) ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Robinhood Traders are Last to the Party]]>
                        </title>
                        <description>
                            <![CDATA[Robinhood traders get fleeced not by HFTs front-running milliseconds before their order hits but by other retail investors, days earlier.]]>
                        </description>
                        <link>https://whoisnnamdi.com/robinhood-party/</link>
                        <guid isPermaLink="false">robinhood-party</guid>
                        <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Mon, 1 Mar 2021, 08:10:00 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512232204-header.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
The recent Robinhood and Gamestop / Gamestock / Gamestonk fiasco shed a light on some of the complex financial infrastructure that undergirds equity trading.

One mechanism, [payment for order flow](https://www.investopedia.com/terms/p/paymentoforderflow.asp), plays a large role in how Robinhood provides commission-free trading to its users.

Payment for order flow is often characterized as a shady practice that enables high-frequency traders (HFTs) to front-run Robinhood traders, siphoning off a sliver of profit as they do.

I tend to think such concerns are overblown. From the perspective of an individual Robinhood trader, HFTs are of almost no importance, and their impact on trading profits is imperceptible, especially if one only transacts periodically.

The truth is much more ironic.

Robinhood traders get fleeced not by HFTs front-running milliseconds before their order hits but **by other retail investors, days earlier.**

### Receive my next long-form post

Thoughtful analysis of the business and economics of tech

 

Subscribe

## The party ended \*five\* days ago

It turns out, [trading activity among Robinhood users lags the rest of the retail community by multiple days](https://papers.ssrn.com/abstract=3776874), more than enough time to squeeze out any potential profits.

A few charts demonstrate this dynamic. The graph below plots "abnormal retail trading volume" against the days before and after a stock peaks on WallStreetBets, the popular Reddit community, measured in terms of mentions. You can think of day zero as representing the day when mentions of a particular stock, say GME, peaked, with "-5" representing five days before, "5" representing five days after, and so on. "Abnormal" simply means retail trading activity relative to the prior 20-day moving average of retail trading volume for a particular stock.

Notice how retail trading volume peaks about two days before WSB mentions peak:

![image-20210222140135524](/content/images/2021/03/image-20210222140135524.png)

In other words, popularity on WallStreetBets lags the broader retail market. Stocks first get popular among non-Robinhood retail traders, and the subreddit subsequently picks this up, increasing how often the name is mentioned.

Now let's look at Robinhood volume relative to WallStreetBets mentions. Robinhood activity peaks 2-3 days _after_ WSB activity:

![image-20210222140228350](/content/images/2021/03/image-20210222140228350.png)

All in all, that's a five day delay between when a stock begins to cool off among most retail investors and when Robinhood traders finally get the memo.

By the time most Robinhood users actually trade, the party is over. And when the festivities end, so do the profits.

## Noise traders

I started the piece with my skepticism about the impact HFTs have on the profits of Robinhood traders. But if retail investors are taking advantage of market opportunities a full _five days_ ahead of Robinhood traders, I'm much more inclined to think that could have an effect.

The data proves this out. While recent research has shown that retail traders are in fact informed (measure as a positive correlation between retail trading activity and future stock returns), **Robinhood activity has no positive relationship with future returns**.

The table below shows the results of a regression of future returns at 3, 5, and 20-day intervals on Robinhood user ownership and aggregate retail trading volume, along with a number of control variables. While the coefficient on retail volume is positive and statistically significant in all cases, suggesting that _non-Robinhood_ retail trading volume predicts future returns, the coefficient on Robinhood ownership is negative and statistically insignificant, meaning that **Robinhood activity has little relationship with future returns**:

![image-20210222140349147](/content/images/2021/03/image-20210222140349147.png)

This finding led the authors to conclude the following:

> Contrasting with recent evidence that retail traders are informed, we find that Robinhood ownership changes are unrelated with future returns, suggesting that zero-commission investors behave as noise traders.

If case you don't read a ton of academic finance research, that's an extremely polite and understated way of saying: **Robinhood trading is random** (with respect to financial returns at least). In other words, there's no _there_ there.

And I should say: nothing about the app necessitates that. It's the behavior of the folks using it.

## There's no such thing as free alpha

I think it's great that Robinhood exists. I love that individuals have more ways of accessing the financial markets without coughing up exorbitant fees.

**But access isn't everything.** We all have "access" to Las Vegas, but that doesn't make it a path to wealth creation. Likewise, if Robinhood traders use the app to merely gamble on 50/50 bets that a particular stock will go up or down, little wealth or value creation will result.

Despite the app's name, Robinhood users for the most part redistribute among themselves the breadcrumbs left by the rest of the market. ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Why Developers Love Redpanda]]>
                        </title>
                        <description>
                            <![CDATA[Why Vectorized's focus on developer experience will unlock real-time streaming for the great majority of developers]]>
                        </description>
                        <link>https://whoisnnamdi.com/why-developers-love-redpanda/</link>
                        <guid isPermaLink="false">why-developers-love-redpanda</guid>
                        <category>
                                        <![CDATA[ Developers ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Wed, 10 Feb 2021, 21:02:00 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512237802-Why_Developers_Love_Redpanda_vectorized_img.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
[Red pandas](https://en.wikipedia.org/wiki/Red_panda) are cute. But for a while, that was the only thing they had going for them.

No longer. As with whales (Docker) and elephants (Hadoop, Postgres) before them, red pandas finally have the backing of a hardcore application infrastructure technology.

Meet [Redpanda](https://github.com/vectorizedio/redpanda), a real-time event streaming platform backed by [Vectorized](https://vectorized.io/). We at Lightspeed led the company’s [recently announced](https://techcrunch.com/2021/01/26/vectorized-announces-15-5m-investment-to-build-simpler-streaming-data-tool/) Seed and Series A rounds, and we couldn’t be more excited about the potential for Vectorized to revolutionize real-time streaming.

Vectorized is one of the most technical companies I’ve ever worked with. But unlike many products, where deep technical innovation often comes at the cost of incredible complexity, Redpanda bucks this trade-off. Like its animal namesake, Redpanda is approachable, combining a simple and accessible developer experience with an underlying engine that pushes streaming to never-before-seen performance levels.

Vectorized is maniacally focused on the developer experience. As a result, it “just works”:

> ___“RedPanda’s performance, simplicity and ease of operation dramatically improves next generation data applications. Switching our development environment from Kafka to Redpanda dramatically lowered development overhead, while pipelines feeding Clickhouse “just worked” and moved our I/O bottleneck back to the disks where it belongs. Recommended.” —_ ****_Eric LaBianca, CTO, The Seventh Sense_****__

In this post, I want to highlight three key aspects of the Redpanda developer experience — ****simplicity****, ****accessibility****, and ****performance**** — and discuss why we think Vectorized’s emphasis on usability will unlock real-time streaming for the great majority of developers, who are underserved by existing solutions. You’ll also hear it straight from members of the community, who I’ve quoted throughout.

### Receive my next long-form post

Thoughtful analysis of the business and economics of tech

 

Subscribe

# Simple is beautiful

Redpanda abstracts away the complexity that often prevents the typical developer from adopting real-time streaming. There’s a long list of optimizations that I won’t entirely do justice to here, but I wanted to highlight two of the most impactful: ****No Zookeeper**** and ****No JVM (Java Virtual Machine)****.

# No Zookeeper

[Apache Zookeeper](https://zookeeper.apache.org/) is a critical piece of infrastructure in Kafka and many other big data technologies. Its core purpose is to manage coordination of nodes and metadata in distributed systems, and it runs as a separate set of machines that must themselves be managed by the operator.

Zookeeper is hardened tech at this point and does its job reasonably well. However, it’s a pain to manage and requires a separate set of Zookeeper-specific expertise to deal with problems as they inevitably occur. No one asked to manage an entirely separate distributed system, yet it has historically been a hard requirement in streaming technologies like Kafka, creating additional operational overhead and burden for many Kafka users:

> ___“Running five node zookeeper clusters was pure overhead.” —_ ****_Hacker News_****__

The issue is near and dear to me — while a product manager at Confluent, I made the case for the removal of the Zookeeper dependency, eventually resulting in the landmark [KIP (Kafka Improvement Proposal)-500](https://cwiki.apache.org/confluence/display/KAFKA/KIP-500%3A+Replace+ZooKeeper+with+a+Self-Managed+Metadata+Quorum) which proposed replacing Zookeeper with a self-managed quorum. KIP-500 was met with applause from much of the Kafka community, who were sick and tired of dealing with Zookeeper. It was time for the Zookeeper to retire.

Removing a core dependency from a decade-old technology is not trivial. A year and a half after the publication of KIP-500, Zookeeper removal proceeds with incremental steps, necessary to mitigate potential migration issues. This can’t be done in a single update.

Redpanda takes a different approach. Thoughtfully architected from the start to leverage the open source [Raft consensus algorithm](https://raft.github.io/), Redpanda obviates the need for a third-party consensus system like Zookeeper. This meaningfully reduces operational complexity and has a direct, positive impact on developer productivity:

> ___“We care about reliability and performance at Zenly, so no Zookeeper and 10x faster was a no brainer.” —_ ****_Jean-Baptiste Dalido, Head of Infrastructure Engineering, Zenly_****__

# No JVM

> ___“I was avoiding Kafka for some time because of admin costs and lack of JVM expertise” —_ ****_Vectorized Community Slack_****__

Another driver of complexity in the event streaming ecosystem historically has been the Java Virtual Machine, or JVM, a hard requirement in Java-based systems. The JVM is the virtual machine that enables applications compiled to Java bytecode to run, acting as an intermediary between the source code and the system.

Unfortunately, JVM expertise is in low supply outside of the Java/Scala developers. Ironically, the JVM is often the source of issues when working with Kafka.

Developers want streaming, but they don’t necessarily want to become Java (assuming they weren’t already familiar) or distributed systems experts. There is meaningful pent-up demand among developers for a non-JVM based streaming architecture to power modern real-time applications. Sizing this up, the Python and JavaScript communities alone could be an order of magnitude greater than the existing Kafka/Java population:

![](/https://nnamdi.net/content/images/2021/05/No_JVM__Redpanda_vectorized_img.png)

Written in C++, Redpanda needs no JVM and thus users need no JVM knowledge or expertise. This finally moves streaming technology away from Java, a big deal given how many data infrastructure technologies have been built around Java or the JVM over the years:

> ___“This looks awesome! I’ve been waiting for a long time for someone to think outside the JVM, and I really hope this is a growing trend. The “big data” industry has seemingly been joined at the hip with Java ever since Hadoop came onto the scene, and the Apache community in particular has a lot of apps that are deeply unfriendly to non-Java apps” —_ ****_Hacker News_****__

# Power to the developers

Real-time event streaming technologies are not always accessible to the typical software developer. In fact, most individual developers have been left behind in the streaming revolution. Seeing this untapped opportunity, the team at Vectorized architected Redpanda in ways that expand the pool of developers that can productively operate the system.

# Kafka compatibility

![](/https://nnamdi.net/content/images/2021/05/0_rzLnKt-j8CZ6Ezd6.png)

Too often, when new technology comes around it bifurcates the existing community in an attempt to grow adoption. This leaves individual developers and teams in the ensuing crossfire.

In building the future of real-time streaming, Redpanda respects what came before it. Despite all its improvement under the hood, Redpanda maintains ****full API compatibility with Kafka****. This means that existing Kafka-based systems can be swapped over to Redpanda with no changes to existing applications, making for an easy and straightforward migration path.

More than a smart business move, maintaining Kafka compatibility makes Redpanda much more accessible to existing Kafka users. This is important because developers in fact love the Kafka __API__, though they don’t always love managing the associated infrastructure.

Further, Kafka API compatibility means that Redpanda users can continue to leverage the amazing Kafka ecosystem that has built up over the years. This can lead to interesting combinations of Kaka-related tools and Redpanda as the core streaming engine:

> ___“Very cool — I was able to use a kafka connector to get websocket fanout out of a redpanda installation no problem. I’ll be writing a blogpost about this.” —_ ****_Vectorized Community Slack_****__

# WebAssembly

![](/https://nnamdi.net/content/images/2021/05/The_future_of_streaming_Redpanda_vectorized_img.png)

[WebAssembly](https://webassembly.org/), or WASM, is one of the most exciting up-and-coming technologies in software development today. WebAssembly lets developers write code in any major language, translate that code to the compact WASM format, and run it on the web with the high performance of a native application.

Redpanda is one of the first infrastructure technologies to take advantage of WASM, enabling developers to “write and edit code in their favorite programming language to perform one-shot transformations, like guaranteeing GDPR compliance by removing personal information or to provide filtering and simple aggregation functions.” Here’s how one community member described Redpanda’s WASM engine:

> ___“Very clever and useful way to take advantage of WASM… It reminds me a little bit of JS-derived views in CouchDB, just way more powerful and performant thanks to WASM rather than plain JS interpreter”_ ****_— Vectorized Community Slack_****__

JavaScript, Python, Rust, Go — anything that compiles to WebAssembly (basically everything at this point) can be used to transform data. Again the key is accessibility — inline WASM transforms in Redpanda represent just that. WASM also unlocks interesting use cases beginning to emerge among the community:

> ___“What excites me the most is the WebAssembly feature, as it enables us to create a “Data Firewall’’, the last mile of access, transforms and policy.” —_ ****_Jean-Baptiste Dalido, Head of Infrastructure Engineering, Zenly_****__

# Gotta go fast

Performance isn’t often pitched as a productivity boost. But it is, especially at scale.

Better performance at the __infrastructure__ level leaves more room for the application itself to function as intended without running into resource constraints. This means less optimization work by the developer and also opens up streaming to other languages like JavaScript and Python, whose worse performance as high-level languages is balanced out by speed at the infrastructure level.

To better understand this, let’s talk cores and tails.

# Hold my Coors, I mean, cores

![](/https://nnamdi.net/content/images/2021/05/Hold_my_core_Redpanda_vectorized_img.png)

Hardware is moving target, continuously evolving and improving. The last 15 years have been no exception. The underlying hardware targeted by streaming and message queue systems has changed meaningfully since the advent of real-time systems, opening up new opportunities for performance enhancements that take advantage of new physical resources.

Written in lower-level C++, Redpanda’s [thread-per-core architecture](https://vectorized.io/blog/tpc-buffers/) is optimized for modern hardware and squeezes out every last bit of performance, fully exploiting the resources it runs on. Redpanda also comes with [intelligent auto-tuning](https://vectorized.io/blog/autotune-series-part-1-storage/) out-of-the-box, which automatically generates optimal settings for your specific hardware/kernel/Redpanda setup. Organizations can do more with less, and the benefits extends down to the level of the individual developer too, who now has more “breathing room” when it comes to performance due to Redpanda’s more efficient streaming engine. Developers can worry less about optimization and just write the applications they want, enhancing developer productivity.

# Don’t fail in the tail

Averages rarely tell the whole story when it comes to performance. Latency, for example, can be, on average, quite similar between two systems and yet diverge meaningfully at the 99th+ percentile. Reliable performance “in the tails” is critical for certain use cases like fraud detection, where financial institutions must return decisions ASAP at the point of sale.

Unfortunately, performance issues often “hide” in the tails of the latency distribution, only rearing their head periodically. But when they do pop up, the delays can be monstrous.

Because Vectorized built a new storage engine from scratch that removes much of the overhead in Kafka and can fully saturate the underlying device, Redpanda operates with stable tail latencies. This means architects get predictable performance in their applications and fewer unexpected spikes in latency.

![](/https://nnamdi.net/content/images/2021/05/End-to-End_Latency_Percentiles_Redpanda_vectorized_img.png)

This sort of reliability is difficult to achieve in JVM-based systems like Kafka. With a C++-based architecture and CPU-level optimizations, Redpanda achieves 19x better tail latency.

# Developer love always wins

We love Vectorized’s focus on improving the developer experience for real-time infrastructure. Redpanda represents an improvement to the status quo along multiple dimensions. Through simplicity, accessibility, and performance enhancements, Redpanda opens up streaming to a wider audience of developers who, at the end of the day, simply want to write applications — __without__ having to struggle with operational complexity.

Here’s how one engineer put it:

> Congratulations to [@emaxerrno](https://twitter.com/emaxerrno?ref_src=twsrc%5Etfw) and the [@VectorizedIO](https://twitter.com/VectorizedIO?ref_src=twsrc%5Etfw) team on their funding!  
>   
> Their mission to make a faster and more reliable Kafka alternative is a worthy one. Our industry deserves diverse data infrastructure options, and it's exciting to see so many new ones getting traction.
> 
> — Large Data Bank (@JordanALewis) [January 26, 2021](https://twitter.com/JordanALewis/status/1354149159755010056?ref_src=twsrc%5Etfw)

We at Lightspeed couldn’t agree more.

__Want to learn more, connect with the Vectorized team, and meet other Redpanda users? Join the__ [__Slack community__](https://vectorized.io/slack)__, check out the__ [__repo__](https://github.com/vectorizedio/redpanda)__, and sign up for early access to__ [__Vectorized Cloud__](https://vectorized.io/cloud)__.__ ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[There's Nothing Magical About the SaaS Magic Number]]>
                        </title>
                        <description>
                            <![CDATA[Magic number is a bad metric. Sales and marketing drives much less revenue than this not-so-magical number implies]]>
                        </description>
                        <link>https://whoisnnamdi.com/magical-magic-number/</link>
                        <guid isPermaLink="false">magical-magic-number</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Thu, 4 Feb 2021, 22:38:47 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512229971-1yocKpYAcw.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
I've never liked the SaaS "magic number."

It always struck me as somewhat contrived — a simple metric with intuitive appeal, but one that is so overused and confused at this point that it harms analysis more than it helps.

My core beef with the metric is that it takes what is fundamentally a **correlational** relationship and confuses it with **causality**, to the chagrin of many strategic finance teams on the inside and investors attempting to model and forecast businesses from the outside.

While it'd be an overstep to say that sales and marketing doesn't cause revenue growth at all, I've realized most companies and investors are totally confused about the extent to which it does.

I want to set the story straight.

First, let's get definitions out of the way. "[Magic number](https://www.thesaascfo.com/calculate-saas-magic-number/)" is defined as follows:

$$\text{Magic Number} = \frac{\text{New ARR}}{\text{Sales & Marketing}}$$

Magic number is typically calculated on a quarterly basis, often with S&M shifted back one period, better matching and reflecting that software sales typically take anywhere from 3-6 months.

Magic number is just a ratio — it's New ARR divided by S&M expense. It tells us nothing about the degree to which that spending caused the New ARR. In fact, for reasons I will illuminate, the degree to which S&M spending actually causes new revenue is almost certainly less than is implied by this not-so-magical ratio. In other words, **magic number overestimates the causal impact of S&M spending**, rendering it useless for decision making.

Here's why:

-   **Omitted / confounding variables**
-   **Reverse causality and simultaneity**
-   **Observation vs. intervention**

I'll walk through each in turn.

### Receive my next long-form post

Thoughtful analysis of the business and economics of tech

 

Subscribe

## Omitted / confounding variables

Magic number is a faulty metric because it ignores important revenue growth factors and incorrectly attributes those growth contributions to "sales and marketing efficiency."

To illustrate this, I'll start with a very simple point. Refer back to the definition of magic number — New ARR divided by sales and marketing spend:

$$\text{Magic Number} = \frac{\text{New ARR}}{\text{Sales & Marketing}}$$

The first thing to note is that New ARR has two sources — S&M and "other stuff":

$$\text{New ARR} = \text{New ARR from S&M} + \text{New ARR from Other Stuff}$$

New ARR can either come from our investments in S&M or it can come from other sources. This could include organic inbound (which may or may not be "marketing" driven depending on the situation), R&D (we release a new feature or product that the market already wants, and it effectively sells itself), etc:

> Open source and product lead growth rely heavily on R&D for GTM. Yet, sales metrics don't account for that.  
>   
> Lately I pay more attention to gross burn vs. growth rather than sales metrics. The latter simply don't provide an accurate picture of the variable costs in the business.
> 
> — martin\_casado (@martin\_casado) [January 31, 2021](https://twitter.com/martin_casado/status/1356013483305783299?ref_src=twsrc%5Etfw)

Rather than enumerate all the possible alternative sources of revenue, let's just crudely summarize them under "Other Stuff" and modify our equation to reflect this attribution:

$$\text{Magic Number} = \frac{\text{New ARR from S&M} + \text{New ARR from Other Stuff}}{\text{Sales & Marketing}}$$

If we break this fraction into two parts, the first would represent "true" magic number — the true causal impact of S&M spend — while the second would represent the degree to which the typical measure of magic number is inflated by misattribution of New ARR:

$$\text{Measured Magic Number} = \text{True Magic Number} + \underbrace{\frac{\text{New ARR from Other Stuff}}{\text{Sales & Marketing}}}\_{\text{Magic Number Inflation Factor}}$$

In other words, magic number omits important non-S&M contributors to revenue and in doing so falsely attributes the New ARR from these alternate sources to S&M. Thus, in a perverse way, we reward ourselves for S&M "efficiency" when in fact it is other factors that meaningfully drive revenue growth. For all we know, our sales and marketing expenditures could be woefully inefficient, but we fool ourselves into thinking otherwise.

Because these other sources of revenue nearly always contribute positively to ARR growth, the magic number inflation factor is almost always positive, meaning that the standard measure for magic number is typically an inflated estimate of the true causal influence of S&M spend. This has serious implications for our ability to model and forecast revenue growth based on assumed S&M inputs.

This also explains why magic number tends to decline when S&M grows rapidly and tends to increase when S&M spend is suddenly cut. It's partly because S&M has diminishing returns, i.e. "true magic number" tends to fluctuate with S&M spend itself. But the other reason is the inflation factor, which includes S&M in the denominator. This means that when S&M grows quickly (specifically, faster than "New ARR from Other Stuff"), the inflation factor decreases, so our measured magic number decreases. The reverse happens when S&M grows more slowly — magic number inflation increases.

In statistics parlance, magic number is **biased** estimate of the "true" magic number due to omitted or confounding variables:

![endogeneity](/content/images/2021/02/endogeneity.png)

In the diagram above, Y is New ARR, X is S&M, Z is a set of observed variables that influence both S&M and New ARR, and U is a set of unobserved variables that also influence our P&L. Unobservables are, by definition, not observed and therefore difficult to account for, but at a minimum we should attempt to control for those observed variables that influence S&M and/or New ARR.

Magic number does neither, guaranteeing that it won't properly measure the impact of S&M on revenue. Magic number is therefore less a measure of sales efficiency and more a measure of its correlation with new revenue. Another name for this is **spurious correlation**. You've probably seen one of these funny charts before:

![8GLG52Ye5Q](/content/images/2021/02/8GLG52Ye5Q.png)

Two variables can be totally unrelated, yet be nearly perfectly correlated. In this case, the "magic number" between margarine consumption and Maine divorces is totally meaningless. Again, the point here isn't that S&M and New ARR aren't at all related; the point is that because they tend to move together and are **plausibly** related, we tend to think they are much more related than they in fact are.

## Reverse causality and simultaneity

There's a tendency to think, "Well, S&M is in the denominator and revenue is in the numerator, so S&M must be driving revenue." That way of thinking also accords with the way sales and marketing is framed most of the time — they are the "revenue generating" part of the business, so investments in S&M must, to some extent, drive revenue.

But rarely asked is the reverse — to what degree does revenue cause S&M expense?

Now, that might at first seem absurd, but don't dismiss it so easily. Revenue almost certain does cause S&M. How? Well, in the most basic sense, aside from capital raised from the outside, the main source of funding for all expenses on the income statement is revenue. That's where the business earns the cash that it can then spend on various initiatives.

If you still don't believe me, think about this proof of concept. Does COGS (Cost of Goods Sold) cause revenue, or does revenue cause COGS? Clearly revenue causes COGS, by definition, since COGS is the cost of goods **sold**, so something has to be sold (and revenue has to be generated), in order for COGS expense to show up. So clearly, revenue can cause expenses to some degree.

What we choose to spend on S&M is dependent on the revenue we **expect** to earn. For example, if you know your S&M is highly effective at driving revenue growth, that is likely to encourage you to invest more in S&M. Conversely, if S&M is not very effective at driving revenue, you will likely spend less than you might otherwise. In that subtle way, revenue (or more accurately in this case, expected revenue) influences S&M spend.

You may not have ever considered this, but it shouldn't be too surprising. There's a reason you rarely see companies spending, say, 500% of revenue on S&M. Companies typically limit their S&M spend to some reasonable level relative to current revenue. Therefore, revenue in some way influences S&M spend. Economists would say that S&M is **[endogenously determined](https://en.wikipedia.org/wiki/Endogeneity_%28econometrics%29)** — it's not some number we pull out of thin air. Our expectations for how valuable it will be and how much revenue we have in the first place determine how much we invest in S&M. Further, our business plans and objectives ("hitting $XM in ARR by year end") influences our S&M spending decisions.

This phenomenon — where A causes B and B also causes A — is called **simultaneity** in statistics and econometrics. More colloquially, you might know this as **reverse causation**.

Reverse causation poses a serious problem to our attempts to forecast new revenue from sales and marketing expenditure. Because magic number estimates the relationship between S&M and revenue from observational data rather than some kind of controlled experiment, we don't know the degree to which one causes the other or the reverse. If a business has a magic number of 1, that could mean that $1 of S&M causes $1 of New ARR, or it could mean that businesses that **expect** to add $1 in New ARR choose to limit their S&M to roughly $1.

The answer is somewhere in the middle, and yet magic number assumes the former. Not good.

## Observation vs. intervention

The third and final problem with magic number is that is confuses **observation** and **intervention**. This is very related to the notion of correlation vs. causation but goes further by personifying causation a bit.

_Observation_ means watching something happen, making no attempts to influence the outcome or the relevant variables. As an analyst measuring the sales efficiency of a software business from the outside, magic number is an observational metric. You can't do anything to influence any of the inputs, you are merely reporting their values. Magic number is not a good measure of sales efficiency, but as long as you acknowledge this caveat, there's nothing inherently wrong with calculating its value, useless though it may be.

However, if you are an actor inside the business intending to intervene in some way, choosing how much to spend on S&M, the entire game changes. In this case, magic number is not only useless as a measure of sales efficiency, but it is in fact a **logical fallacy** to use it to forecast the revenue impact from assumed S&M spend.

The reasons behind this are somewhat subtle, but [this analogy](https://www.inference.vc/about/) should make it quite clear.

![](https://upload.wikimedia.org/wikipedia/commons/4/4b/Modern_Aneroid_Barometer.jpg)

Imagine you have a barometer measuring the air pressure around you. If the barometer is functioning properly, we should **observe** a tight relationship between the pressure reading on the barometer and the actual air pressure. In other words, air pressure and our barometer reading are highly _correlated_.

However, if we were to hack the barometer and manually change the measurement it reports, would we expect the pressure in the room to change as a result? Obviously not. **Intervening** by changing the pressure reading of the barometer does not impact actual air pressure, despite the 1:1 observational correlation between the two. The interventional or causal relationship between the barometer and air pressure is exactly zero in that direction (though there is a very strong causal influence in the other direction, air pressure to barometer reading). The observational relationship breaks down when we interfere with the system.

> "In summary, y and x are correlated or statistically dependent and therefore seeing x allows me to predict the value of y, but y is not caused by x so setting the value of x won't affect the distribution of y. Hence, p(y|x) and p(y|do(x)) behave very differently." — [ML beyond Curve Fitting](https://www.inference.vc/untitled/)

Economists know this as the [Lucas Critique](https://en.wikipedia.org/wiki/Lucas_critique), which tells us that attempting to conduct economic policy based on some macroeconomic model trained on observational data will inevitably fail, as the identified relationships disappear into the ether.

**The same logic applies to sales and marketing spending.** We should not expect magic number, which is effectively a measure of the correlation between New ARR and S&M, to hold constant if we were to purposefully increase S&M. New ARR and S&M can be tightly correlated, and yet the observational correlation between the two breaks down when we intervene. Choosing to, say, double S&M next year does not in any way guarantee that we will book twice as much New ARR. Worse, the breakdown in the relationship tends to go against us — after a significant ramp in expense, sales efficiency almost always ends up being worse than expected, rather than better.

If we are interested in forecasting new revenue based on an intentional choice of S&M spending, magic number won't do. We need to find a better magic number, one that approximates the true causal impact:

> "In applications where you ultimately want to control or choose x based on the conditional you estimated, you should seek to estimate p(y|do(x)) instead. For example, if x is a medical treatment and y is the outcome, you are not merely interested in observing a naturally occurring treatment x and predicting the outcome, we want to proactively choose the treatment x given our understanding of how it affects the outcome y." — [ML beyond Curve Fitting](https://www.inference.vc/untitled/)

## Magic multipliers

If you're a regular reader of mine, you'll know that this is the part where we transition from theory to data. Can we identify the true causal impact of S&M on revenue growth?

We can try.

Funnily enough, economists regularly tackle a very similar issue — calculating how much the economy grows when the government spends more money, otherwise known as the "fiscal multiplier." How exactly to calculate these multipliers is a constant debate among economists, but a few methods have emerged with some consensus. Here I will leverage a statistical method called "[local projections](https://sites.google.com/site/oscarjorda/home/local-projections)" which I won't explain in detail here, but I will summarize by saying it enables us to calculate the causal impact of an increase in S&M spend on New ARR over time via a special linear regression. The ratio of the increase in New ARR to the increase in S&M spend yields the "multiplier", or true magic number in our case.

I've assembled a dataset of anonymized quarterly New ARR and sales and marketing spend for a sample of early to mid-stage software companies. This is far from a representative sample of all software vendors, but with 230 data points across 26 companies we can generate insights nonetheless. As evidence of its non-representativeness, the average magic number in my dataset is **2.1**. Most companies in fact hover closer to 1, but the logic is the same regardless.

First we'll calculate the impact of an increase in S&M (relative to what it otherwise would have been) on New ARR (again, relative to the counterfactual). We'll recalculate this at various horizons — current quarter (labeled "Q1" below), one quarter out, two quarters out etc (chart on the left). To get the total effect we'll sum up the impacts across quarters (chart on the right):

![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fwhoisnnamdi%2FBYNBFHio-T.png?alt=media&token=bb38640a-7eae-4478-b4ab-a8914c006c11)

A $1M increase in S&M this quarter generates $0.38 more New ARR that same quarter, $0.39 the next quarter, and so on. Cumulatively, over four quarters this adds up to about **$2** of additional New ARR vs. if S&M spend hadn't increased.

Now if this was the entire story we'd say that the true magic number reaches two after four quarters and perhaps continues to increase after that. OK, so perhaps magic number as typically calculated doesn't measure the immediate impact of S&M spending, but it gets there eventually, right?

**But it's not the whole story.** We've cheated a bit here by only counting the initial $1 bump in S&M spend. But if we increase spending today, spending tomorrow will also be higher. Part of the effect on New ARR we measured above is due to higher spend in the current quarter, but part of it is also driven by higher spend next quarter (and the quarter after that, and so on).

To account for this, we must calculate the effect of S&M this quarter on S&M **next** quarter **AND** the quarters thereafter and include the cumulative effect in the denominator of our magic number calculation:

![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fwhoisnnamdi%2FYAnS6FgR3i.png?alt=media&token=c114258a-743d-4c16-b80f-a8924bb3fb7b)

The chart above shows the impact of S&M on **itself** into the future. A $1 increase this quarter leads to the next quarter being $0.70 higher than it would have otherwise been, $0.86 the quarter after that, and so on. Cumulatively, we spend about $3.5M more after four quarters.

Now we can compare the cumulative effect on New ARR to the cumulative increase in S&M to get a proper measure of the "true" magic number:

![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fwhoisnnamdi%2F1yocKpYAcw.png?alt=media&token=17eb3eb1-6cf8-4c6c-98f3-7a015c8704bf)

The true magic number is estimated to be only **0.6** after four quarters, this is less than a **third** of the average magic number in this dataset of 2.1. So not only is the true magic number much lower than the typical calculation, it takes many quarters to reach this much lower cumulative impact. Importantly — **we don't get nearly the bang-for-buck that the traditional magic number implies.**

To make this real — a forecast that New ARR will be $2.1 higher next quarter based on a $1 increase in S&M this quarter will be off by **80%** (per the first chart, New ARR one quarter out is only $0.39 higher) and cumulatively (including the first and second quarter impact) will be off by **78%** (based on 0.46 cumulative magic number at Q2).

## Magic isn't real

It's worth pointing out some caveats here.

I wasn't able to fully control for the "other stuff" that also drives New ARR. In my regressions I do control for the long-term trends of each variable (via a cubic polynomial time trend interacted with company fixed effects for those who care) which helps account for these unobserved factors, but this is not a perfect solution. I also don't identify a completely exogenous increase in S&M. Inherently, I assume that S&M deviations from trend imply unexpected increases in S&M, but that's also not totally right.

This is amateur econometrics, so it'll do.

My conclusions:

-   **Magic number is a bad metric.** It misleads us into thinking that by turning some knobs we can precisely control revenue growth. It commits the scandalous statistical sin of mixing correlation and causation
    
-   **S&M alone has little effect on New ARR.** Certainly not in the near term and to a much lesser degree than you might think in the longer run
    
-   **We don't really know how to calculate software sales efficiency.** This analysis leveraged 230 data points to get some level of certainty, but for any particular company it'd be difficult to replicate with only a few quarters or years of data
    

So what can we use as an alternative?

I've always liked the easy heuristic of **New ARR vs. cash burned**. This ignores specific expense line items and gives companies credit for "all-in" capital efficiency, which is arguably what matters most anyway. It avoids the perils of attributing revenue growth to particular initiatives, like sales and marketing or research and development. Of course, we still have an omitted variable problem (burning cash is not the only way to drive revenue growth), but it's much less severe.

_3,000 words later, I hope I've convinced you that magic number doesn't tell you much about sales efficiency or productivity. If you've found a better metric, I'd love to hear about it. Feel free to shoot me a note at [first.last@gmail.com](mailto:first.last@gmail.com)_ ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Product-Market Fit is Lindy]]>
                        </title>
                        <description>
                            <![CDATA[The longer you search for product-market fit, the less likely you will find it.]]>
                        </description>
                        <link>https://whoisnnamdi.com/product-market-fit-is-lindy/</link>
                        <guid isPermaLink="false">product-market-fit-is-lindy</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Thu, 17 Dec 2020, 18:20:27 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512228437-header.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
The longer you search for product-market fit, the less likely you will find it.

This phenomenon is called the Lindy effect.

To be "Lindy" means the longer something survives, the more time it has left. Remaining life _extends_, rather than contracts, with age.

Perishable objects like flesh-and-blood humans don't work this way. As we age, our remaining time on this Earth decreases — a 90 year-old has less expected time left on the clock than an 80 year-old.

But certain non-perishables follow a different rule. Per [Wikipedia](https://en.wikipedia.org/wiki/Lindy_effect):

> The Lindy effect is a theory that the future life expectancy of some non-perishable things like a technology or an idea is proportional to their current age, so that **every additional period of survival implies a longer remaining life expectancy**. Where the Lindy effect applies, **mortality rate decreases with time**.

![](/https://nnamdi.net/content/images/2020/12/image-20201217101504390.png)

Product-market fit follows the Lindy effect.

More precisely, _lack_ of product-market fit is "Lindy". **The longer you don't have it, the longer you won't have it.**

An additional year of "no product-market fit" implies a longer remaining period of "no product-market fit." The odds of achieving product-market fit with any particular idea decline with time. Thus, **if you don't achieve product-market fit quickly, you may never achieve it at all.**

Product-market fit, like the elusive "cure" for cancer, is not a fixed destination, guaranteed to be reached with enough time spent running toward it. In a weird way, moving "toward" it doesn't actually get you any closer to it — it only moves further away.

Product-market fit _escapes_ from you.

### Receive my next long-form post

Thoughtful analysis of the business and economics of tech

 

Subscribe

## Product-market fit isn't normal

I was inspired to write this essay by a recent tweet by [Matt MacInnis](https://twitter.com/stanine), COO of [Rippling](https://www.rippling.com/), where he claimed product-market fit is obvious once you have it, suggesting business performance is [lognormally distributed](https://en.wikipedia.org/wiki/Log-normal_distribution) vis-a-vis product-market fit:

> (1) It's been said, but now I get it in my gut: if you have to ask whether you’ve found product-market fit, you haven’t. Like most things, it’s lognormal: better PMF yields way, way better biz performance. At a certain point, most assumptions about how to build a co. break. 2/7
> 
> — Matt MacInnis (@stanine) [August 4, 2020](https://twitter.com/stanine/status/1290714927489880065?ref_src=twsrc%5Etfw)

No, he didn't mention the Lindy effect _directly_, but that's what instantly came to mind as I read the tweet.

Here's why.

A lognormal distribution is one which, if you take the logarithm of all its possible values, yields a normal distribution. Hence "log" "normal". Due to the exponential nature of the logarithm, lognormal distributions are skewed:

![lognormal-distribution](/https://nnamdi.net/content/images/2020/12/lognormal-distribution.jpg)

Lognormal distributions are neither normally distributed nor thin-tailed. Instead, they are [fat-tailed](https://en.wikipedia.org/wiki/Fat-tailed_distribution).

This is an important fact.

When MacInnis says performance is lognormal, or skewed, with respect to product-market fit, he's making the point that, once you have product-market fit, **you go straight to the tails**. Business performance accelerates so meaningfully that any ambiguity goes out the window. You get it "in your gut." It becomes obvious, as nearly everything about the business gets better.

## A/B testing with fat tails

In [Why Don't VCs Index Invest](https://nnamdi.net/vcs-index-invest/), I make the point that, when facing sufficiently fat-tailed distributions, it makes sense to spread your bets widely, avoiding concentration or dependence on any one investment or opportunity. Instead "index invest," placing small bets in everything reasonable without overthinking it too much.

This extends to all domains where fat-tails reign. [A fascinating paper](https://eduardomazevedo.github.io/papers/azevedo-et-al-ab.pdf) by researchers at Microsoft, HomeAway, Wharton, and Columbia University applies this logic to the domain of internet search engines and reaches similar conclusions.

Search platforms like Microsoft Bing and Google face a trade-off when A/B testing various changes:

-   run large, data-hungry experiments to get precise effect estimates for potential improvements to the platform
-   run small, "lean" experiments that do not have much statistical power but nevertheless detect effects if they are large enough

The conventional wisdom is that larger A/B tests are better because they increase statistical significance of the experimental results, building confidence that seemingly positive results are, in fact, positive.

It turns out, the optimal choice for how many users to assign to an experiment depends on the fat-tailedness of the distribution of potential returns, or what the researchers call, the "innovation value distribution":

> ... **with sufficiently fat tails, this conventional wisdom reverses and the go lean approach of trying many small experiments is preferred**. Intuitively, with sufficiently fat tails, even small experiments are sufficient to detect the largest effects; which in this case account for most total value. Larger experiments detect subtler effects, but these constitute less of the total value; **making the value of information concave**. This case also has different implications for the marginal value of information.

To translate — when most of the potential gains come from a small number of experimental changes (i.e., innovation is fat-tailed), you're better off running as many experiments as possible so as not to miss the potential "big one." Yes, this might come at the cost of statistical precision, but large effects are so large they can be detected even in small samples.

Another way to think about this is in terms of the "value" of information:

> while the production function is always concave for large numbers of assigned users, its shape with few users depends critically on the thickness of the tails of the prior. If the prior is not too fat-tailed... then the production function is convex. However, **we show that if the prior is very fat-tailed... the production function is concave.**

Again, I translate — when the value of experimentation is fat-tailed, the marginal value of additional information declines as you gather more data because any large, positive innovation is easily detectable with only a small amount of data. Thus, collecting more information doesn't help much — the value of information is _concave_.

On the other hand, if the value of experimentation is normally distributed or thin-tailed, then there are almost no mammoths out there to find. Instead, most innovations are of middling value. In that range, precision matters, as it can be the difference between accidentally implementing a slightly worse change over a somewhat better one. More information helps decipher between the two. Thus, the value of information is _convex_.

We can see this in the chart below from the Bing paper, which shows the shape of the experiment "production function" (which effectively represents the value of experimentally gained information) when graphed against experiment sample size. The different lines represent various levels of \\(\\alpha\\) , or the assumed fat-tailedness of the innovation distribution. Lower \\(\\alpha\\) means fatter tails, and vice versa.

Fat-tailedness (small \\(\\alpha\\)) leads to a concave information value curve, whereas thin-tailedness (high \\(\\alpha\\)) leads to a convex information value curve:

![image-20200906170720679](/https://nnamdi.net/content/images/2020/12/image-20200906170720679.png)

## You'll know it when you see it

So which is it?

> We present evidence—using a sample of approximately 1,505 experiments—suggesting that **innovations at Microsoft Bing have very fat tails**.

**The innovation value distribution is fat-tailed.** So small experiments can detect the largest effects, which represent most of the value of A/B testing:

> **If the distribution of innovation quality is sufficiently thick tailed, a few ideas are large outliers**, with very large negative or positive impacts. These are commonly referred to as black swans, or as big wins when they are positive. The production function is concave and has an infinite derivative at n = 0. **The optimal innovation strategy in this case is to run many small experiments, and to test all ideas.**

OK, but what does A/B testing with fat tails have to do with product-market fit?

Think about the search for product-market fit as a sort of multifaceted A/B test. You try various permutations of the product, business, and go-to-market model, looking for something that especially resonates with users or customers. The vast majority of these combinations won't work out, but a handful will. Those few will work _so well_ in fact that they will _dwarf_ the performance of all the other iterations.

If it is true that product-market fit yields much better business performance, then product-market fit should be relatively obvious, even with little data. This suggests that, if you do have some data for an idea and it _still_ isn't obvious to you whether or not it achieves product-market fit, you may have a problem, especially if you've been at it for a while.

But isn't that exactly what MacInnis said?

> **"If you have to ask whether you've found product-market fit, you haven't."**

The Bing experiments provide the mathematical and empirical justification for MacInnis' bold statement. While I won't quibble over whether business performance is lognormal or some other distribution, it's clearly quite skewed, just as in the Big experiments. In that kind of world, product-market fit loses much of its mystique — it doesn't take a genius to know whether you have it:

> Consider a startup firm that uses a lean experimentation strategy. The firm tries out many ideas in small A/B tests, in hopes of finding one idea that is a big positive outlier. Even though the A/B tests are imprecise, the firm knows that, **if a signal is several standard errors above the mean, it is likely to be an outlier.** So the firm decides to only implement ideas that are, say, 5 standard errors above the mean. **This means that the firm will almost certainly detect all outliers that are more than, say, 7 standard errors above the mean.**

"_You'll know it when you see it_" rings quite true.

Due to fat-tailed business performance, it doesn't take much data to know you've achieved product-market fit, nor does it require some complicated, made-up framework from a supposed startup guru. In the context of the Bing analysis, any A/B test that yielded a result more than several standard deviations above the baseline essentially achieved "fit," with a high degree of certainty.

## Starting over

Let's end by returning to where we began — the Lindy effect.

MacInnis' statement is effectively a rephrasing of the Lindy effect —  if it's taken long to find product-market fit, it's likely going to take _a lot_ longer.

To rephrase once more: since it doesn't take much data to know you have product-market fit (assuming you do, in fact, have it), then if you haven't achieved it yet, you probably have a long way to go.

This is brutal stuff. However, via lean experimentation, planting many flags in various places and quickly shifting gears when no treasure is found, you can, in expectation, start "closer" to the goal. Per the Microsoft paper:

> We call this the "lean experimentation" strategy, as it involves running many cheap experiments in the hopes of finding big wins (or avoiding a negative outlier). This strategy is in line with the lean startup approach, which encourages companies to **quickly iterate through many ideas, experiment, and pivot from ideas that are not resounding successes**.

I want to emphasize that these are are all probabilistic statements and say nothing about any particular circumstance.

You and your team could have been chasing product-market fit for a given idea for the past year, and, for all anyone knows, it could be just around the corner. Many ideas seemed hopeless and for the longest time didn't work... until they finally did.

But that's not the norm.

Uber, Netflix, Facebook, DoorDash. In each case, soon after launch, it became quite clear they were doing something right. Even in companies that went through many iterations, things started to work very soon after landing on the right idea.

To be clear — the chase does not _cause_ product-market fit to never be achieved. It represents growing _evidence_ it won't be achieved. The chase represents _information_, the value of which is concave, meaning it has diminishing returns. That one needs to chase at all is, strangely enough, a sign you're on the wrong path.

More viscerally, if the trail through the forest seems much longer than it should be, **you're probably lost**. Breath, regroup, and try something different.

### Receive my next long-form post

Thoughtful analysis of the business and economics of tech

 

Subscribe ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Why We Will Never Have Enough Software Developers]]>
                        </title>
                        <description>
                            <![CDATA[Developers are dropping out of the profession in large numbers]]>
                        </description>
                        <link>https://whoisnnamdi.com/never-enough-developers/</link>
                        <guid isPermaLink="false">never-enough-developers</guid>
                        <category>
                                        <![CDATA[ Developers ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Tue, 27 Oct 2020, 17:55:02 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512226770-header-v2-resized.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
We will never have enough software developers.

Developers are dropping out of the profession in large numbers despite efforts to grow the number of computer science graduates and software engineers.

Here's why.

## Developer dropout is real

Software development has a _serious_ retention problem:

-   At age 26, 59% of engineering and computer science grads work in occupations _related_ to their field of study. By age 50, only 41% work in the same domain, meaning a full **~30% drop out of the field by mid-career**
-   In contrast, engineering and computer science majors who join _unrelated_ fields upon graduation retain at much higher rates, with only 10-15% switching out after the age of 26:

![](/https://nnamdi.net/content/images/2020/10/W-kECKz1nw.png)

Engineers often leave engineering for non-STEM management roles. Graduation into management is not surprising. What's surprising is that these are **non-STEM** positions. Engineers swap technical roles for _non-technical_ roles over time.

This phenomenon, which I'll call "**developer dropout**," is a real problem. What's behind it?

### Receive my next long-form post

Thoughtful analysis of the business and economics of tech

 

Subscribe

## Out with the old skills, in with the new skills

Programming-related jobs have high rates of skill turnover. Over time, the types of skills required by companies hiring software developers change more rapidly than any other profession.

To demonstrate this, [researchers](https://academic.oup.com/qje/article/135/4/1965/5858010) analyzed job postings on more than 40,000 online job boards and company websites between 2007 and 2019, controlling for employer, location, and occupation. They defined "new" skills as those that were rare or non-existent in 2007 but prevalent in 2019 and "old" skills as those that were prevalent in 2007 but rare or extinct in 2019.

-   While only 30% of all job vacancies required at least one new skill by 2019, **47% of computer and mathematical jobs required at least one new skill** (i.e. a skill that was not common back in 2007)
-   This compares to _less than 20%_ of jobs in fields like education, law, and community and social services
-   In addition, **16% of jobs in computer and mathematical fields in 2007 required a skill that was obsolete by 2019** (i.e. a skill that was common in 2007 but relatively rare in 2019), more than double any other job category:

![](/https://nnamdi.net/content/images/2020/10/azDsA9n3rx.png)

About a third of the change in required skills in computer-related occupations is due to specific new software:

-   The fastest-growing software skills between 2007 and 2019 include **Python, R, and Apache Hadoop**
-   Software that was popular in 2007 but effectively obsolete by 2019 includes QuarkXpress, ActionScript, Solaris, IBM Websphere, and Adobe Flash (ah, finally a name I recognize)

Data science, machine learning, and AI saw big increases among technology-intensive jobs as well. For example, the number of STEM-related jobs requiring skills in machine learning and AI grew more than 4x from 2007-2017, touching more than 15% of STEM jobs:

![](/https://nnamdi.net/content/images/2020/10/Pc3AjVflhW.png)

To better compare rates of skill change across occupations, the researchers came up with a measure of skill change that tracks the absolute growth or decline of various skills within each profession from 2007 to 2019. Occupations whose required skills change rapidly in prevalence among job postings receive a high score, while jobs whose skills do not change much receive a lower score:

![](/https://nnamdi.net/content/images/2020/10/xfeKvOWxo-.png)

-   **Computer-related occupations receive the highest score by far, 4.8**. Note that the mean and standard deviation of this measure are ~3 and ~1 respectively, so computer-related jobs are **nearly two standard deviations away from the typical job in America**
-   Meanwhile, jobs in education and and those involving manual labor have very low skill change scores, typically less than 2.

We can get even more granular and look at specific job roles. This level of detail makes the difference even more stark (only showing the fastest changing roles):

![](/https://nnamdi.net/content/images/2020/10/OwoB5xsSdO.png)

Web development has the highest rate of skill change _among all jobs in the country_. Next up are sales engineers, another often technical role. Database administrators, computer network architects, sysadmins, and application developers all make the top 10, and we see many other technical roles among the top 30. The mean and standard deviation are similar here, placing web development **more than 3 standard deviations away from the typical job in America** in terms of skill change over time.

Suffice to say, **software development is a rapidly changing profession**.

You might think, however, that skill change would eventually settle down as one becomes more experienced.

_You'd be wrong._ The skills for software engineering jobs change rapidly throughout the entire career lifecycle:

![](/https://nnamdi.net/content/images/2020/10/7ItjXMaTE-.png)

-   In entry-level roles in computer and engineering occupations all the way through those requiring 12+ years of experience, **the proportion of job postings requiring at least one new skill in 2019 was effectively the same, 40-45%**
-   In contrast, **29% of entry-level non-computing and engineering roles in 2019 required at least one new skill, but this proportion declines to 24%** for jobs requiring more than four years of experience

> This means that experienced STEM workers seeking employment in 2019 are often required to possess skills that **were not required** when they entered the labor market in 2007 or earlier.

Software engineers **never** escape the skill-change vortex, even many years into their careers. Experienced engineers must learn and adopt technologies that didn't even exist when they started out. Developers must constantly retool themselves, even well after their [formal education](https://nnamdi.net/college-degrees-software-engineers/) ends.

## [Nothing's changed but my change](https://youtu.be/m1ERvlxgCD8?t=166)

**College majors associated with faster changing jobs pay more early on.**

-   In professions with one standard deviation increased skill change, pay is **~30%** higher in the first few years of one's career
-   If we exclude both the fastest and slowest-changing fields (Engineering/Computer Science at the high end, Health/Education at the low end), the early earnings premium for faster-changing roles increases to **~60%**:

![](/https://nnamdi.net/content/images/2020/10/heMo13OsG1.png)

**Fast-changing fields pay better.**

Notice however that the pay advantage declines over time. By the time one approaches the age of 50, the pay premium for working in rapidly changing fields falls dramatically to only 20-30% vs slower changing professions.

Here's another way to see the eroding pay advantage. The below chart simulates the earnings of the average worker by category of college degree from ages 23 to 50 in 2016 dollars.

-   Computer science and engineering grads start off with sizable advantage vs any other major
-   However, this premium _falls_ over time as the earnings of CS and engineering graduates plateau over time while the earnings of their peers grow _faster_ for _longer_
-   In fact, **life and physical science graduates' earnings surpass their computer and engineering classmates by the age of 40**:

![](/https://nnamdi.net/content/images/2020/10/Lifecyle-Earnings-by-Degree-Category.png)

Excluding business majors, the earnings premium of software engineering declines over time in both percentage _and_ absolute dollar terms, to the point where engineers barely out-earn social science majors:

![](/https://nnamdi.net/content/images/2020/10/Engineering-_-Computer-Science-Earnings-Premium.png)

But the focus on college major is somewhat misleading. This phenomenon has less to do with one's field of study and more to do with _choice of occupation_.

To show this, researchers plotted the earnings premium of various categories workers relative to those with a non-Engineering/Computer Science major working in a non-Engineering/Computer Science job.

-   Workers who major in Engineering or Computer Science but work in unrelated fields actually see their earnings advantage _compound_ over time, rather than decline
-   On the other hand, regardless of major, individuals who work in Engineering or Computer Science jobs see their earnings advantage erode over the years:

![](/https://nnamdi.net/content/images/2020/10/OFffH9kBKA.png)

> **Declining relative returns is a feature of STEM jobs, not majors.** The earnings premium for non-STEM majors in STEM occupations starts off near 40%, but declines to 20% within a decade. In contrast, the relative earnings advantage grows over time for computer science and engineering majors working in non-STEM occupations.

The **profession** of software development drives the declining earnings premium, **not the college major**.

In fact, computer science majors who work in non-CS fields experience the _opposite_ dynamic of their non-developer peers — their relative earnings premium rises as they advance. A CS major who eschews the profession doesn't earn much more than otherwise similar non-CS majors early on, but eventually out-earns their peers by nearly 20%.

OK, that's enough about _what_ is happening. Now let's see _why_ it's happening.

## _Human_ capital depreciates too

Imagine a simple model where workers choose their profession in order to maximize income, which is a derivative of their own skill or human capital. Over time, workers gain new skills, while the value of their existing skills depreciates somewhat due to changing times.

Some workers, endowed with superior ability, learn faster than others, picking up skills at a quicker pace. Those workers will tend to sort into high-skilled, fast-changing professions initially, maximizing their early career earnings. Less impressive workers will sort into low-skilled, slower-changing professions.

In a world where human capital never depreciated, we could imagine that high-skilled individuals like software developers would maintain a relative human capital (and earnings) advantage over other professionals, leading to consistently increasing pay and a stable relative premium:

![](/https://nnamdi.net/content/images/2020/10/Human-Capital--w_o-Depreciation-.png)

![](/https://nnamdi.net/content/images/2020/10/Software-Engineering-Human-Capital-Premium--w_o-Depreciation-.png)

But, if human capital depreciates over time and that rate of depreciation is higher in rapidly-changing fields like software development, then developers' initial advantage would erode over time, narrowing the gap vs. non-developers:

![](/https://nnamdi.net/content/images/2020/10/Human-Capital--w_-Depreciation-.png)

![](/https://nnamdi.net/content/images/2020/10/Software-Engineering-Human-Capital-Premium--w_-Depreciation-.png)

This simple model helps explain what we see in the data — the software engineering earnings advantage disappears as the _effective_ human capital gap narrows.

> Applied majors such as computer science, engineering, and business teach vintage-specific skills that become less valuable as new skills are introduced to the workplace over time.

Specific skills in software development quickly become dated. Programming languages and development frameworks go out of style. Hadoop is hot one year, and it's old news the next. Like a fast, expensive car that quickly loses value as it's driven around town, the skills and human capital of software engineers fall apart without constant, expensive maintenance:

> Intuitively, careers with high rates of obsolescence require workers to learn many new tasks each year, which **diminishes learning gains and lowers the returns to experience**.

## Quick learners are fast dropouts

The hits don't stop there. Ironically, **the quickest learners are also the quickest dropouts**.

To understand why, think back to the model we just outlined. Quick learners accumulate human capital faster than their slower peers, which means they have the most to lose when certain skills or abilities fall out of favor. In fact, **the return to being a fast learner is _higher_ in jobs with _low_ rates of skill change** because the learnings can [compound](https://nnamdi.net/you-dont-understand-compound-growth/) over time instead of dwindle in relevance.

> High-ability workers are faster learners, in all jobs. However, **the relative return to ability is higher in careers that change less, because learning gains accumulate.**

Said another way, **the opportunity cost of working in a rapidly-changing field is highest for the best learners**. This creates immense pressure to drop out of software engineering and other fast-changing careers into more stable roles and industries.

The researchers show this by regressing STEM job status on a number of other variables, including an interaction between age and score on the [Armed Forces Qualifying Test](https://www.military.com/join-armed-forces/asvab) (AFQT), a common measure of cognitive ability. The coefficient comes out negative and statistically significant, implying that the relative probability of working in STEM at any given age _declines_ with cognitive ability (column 1 and 2 below):

![](/https://nnamdi.net/content/images/2020/10/LFrqrUH7Kl.png)

> The results imply that a worker with cognitive ability one standard deviation above average is 4.9 percentage points more likely to work in STEM at age 23, but **only 1.6 percentage points more likely to be working in a STEM job by age 34**.

In fact, by age 40, the regression predicts **workers with higher cognitive ability are _less_ likely to work in STEM than those with lower cognitive ability**.

![](/https://nnamdi.net/content/images/2020/10/Relative-STEM-Probability-for-1-SD-Higher-Cognitive-Ability.png)

Simply put, **professionals with higher cognitive ability drop out of STEM careers earlier and faster**:

> ... STEM majors with higher scores on the Armed Forces Qualifying Test (AFQT)—a widely used proxy for academic aptitude—**leave STEM careers more often and at younger ages**.

## Implication: Growing the software engineering talent pool is harder than you think

Two points I want to drive home:

**First** — software development has a _turnover_ problem.

Growing the supply of software developers is not trivial because the field already sees high levels of developer dropout and turnover, and this would only increase if the field were to grow larger. A larger software development labor pool would presumably drive down wages, encouraging even more developers to shift out of the profession, especially those past their career midpoints. On that flat part of the earnings curve, the incentive to remain a developer is weak at best.

**Second** — software development also has a _selection_ problem.

The highest ability, fastest learners disproportionately leave the field over time. They have a multitude of other ways to profitably leverage their intellect and skills. **Software development carries serious opportunity cost.** Again, this is ironic because one would normally expect the best to stay and worst to leave, but that's not what we see in the data. The software development talent pool mix shifts toward _lower cognitive ability_ as any given cohort matures.

Combined, these points suggest software development may be destined for perennial labor shortages unless the pace of change slows sufficiently.

> ... **rapid technological change can lead to a short shelf life for technical skills**. This tradeoff between technology-specific and general skills is an important consideration for policymakers and colleges seeking to educate the workers of today, while also building the skills of the next generation.

To conclude, I emphasize: **highly skilled people prefer highly stable careers in the long-run**. This lets their relative ability and human capital advantage compound over time. Rapid deterioration of skills continuously levels the playing field, preventing the best from separating themselves from the pack. In such a situation, **it makes more sense to quit the race early than get caught in the pile up**.

_Thanks to [David Deming](https://scholar.harvard.edu/ddeming/home), [Kadeem Noray](https://scholar.harvard.edu/kadeem/home), the authors of [the study](https://academic.oup.com/qje/article/135/4/1965/5858010) from which much of this essay is derived._

### Receive my next long-form post

Thoughtful analysis of the business and economics of tech

 

Subscribe ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Enterprise Software Monetization is Fat-Tailed]]>
                        </title>
                        <description>
                            <![CDATA[In enterprise software, averages are meaningless. Instead, focus on the tails.]]>
                        </description>
                        <link>https://whoisnnamdi.com/software-fat-tailed/</link>
                        <guid isPermaLink="false">software-fat-tailed</guid>
                        <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Tue, 6 Oct 2020, 15:59:16 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512221769-header-resized.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
In enterprise software, **the "average" customer is a meaningless concept.**

Paying too much attention to the "average" customer leads many founders and investors astray.

Instead, **focus on the tails.**

Here's why.

## The basics

Define "monetization" as the average revenue a software vendor extracts from its customers:  
$$  
\text {monetization} = \text {total revenue / total customers}  
$$  
Wild variation in monetization across customers means most enterprise software customers contribute little to overall revenue.

For example, a late-stage software company with $50M+ in ARR and ~2,000 customers might have a small number of customers with enormous contracts, possibly greater than $500K or even $1M in ARR each, and a large number with tiny contracts in the ~$10K range.

The extreme, non-negative variation around the average produces a right or positively skewed monetization distribution:

![image-20200824082909012](/https://nnamdi.net/content/images/2020/10/image-20200824082909012.png)

Sufficiently skewed distributions are "[fat-tailed](https://en.wikipedia.org/wiki/Fat-tailed_distribution)." Large customer contracts will determine the properties of the distribution, like its mean or variance. Similarly, large customers will account for an enormous proportion of overall revenue. In other words, software monetization is a [power law](http://reactionwheel.net/2015/06/power-laws-in-venture.html).

In a [previous essay](https://nnamdi.net/vcs-index-invest/), I introduced the notion of \\(\\alpha\\), or alpha, the shape or tail parameter, which characterizes the "fat-tailedness" of a power law distribution. The smaller \\(\\alpha\\), the more skewed the distribution, the fatter the tails, with \\(\\alpha < 2\\) indicating extreme skew and fat-tailedness. For software monetization, the fatter the tail, the more common and impactful are those "whale" customers.

Evidence for skewed monetization is tough to come by without access to a company's commercial contracts. However, we can infer the fat-tailedness from the SEC filings of public software companies with a simple trick.

### Receive my next long-form post

Thoughtful analysis of the business and economics of tech

 

Subscribe

## The math

The trick? A [formula](https://www.youtube.com/watch?v=XhTHG3QmVwM) exists that calculates the concentration in the top percentiles of a power law distribution based on the \\(\\alpha\\) of the distribution:  
  
$$  
\text{s} = p^{\frac{\alpha-1}{\alpha}}  
$$  
where \\(s\\) is the share of the total and \\(p\\) is the percentile.

Plug in the \\(\\alpha\\) and the percentile \\(p\\) you are interested in to get the share of the total that the top-p% of customers represent.

Invert the formula to yield the \\(\\alpha\\) of a power law distribution given a certain percentile and share:  
$$  
\alpha = \frac{\log{p}}{\log{p}-\log{\text{s}}}  
$$  
This means we can infer \\(\\alpha\\)​ and therefore how fat-tailed the revenue distribution is if we know the share of revenue represented by the largest customers of a given software vendor.

We can estimate the shape of the customer distribution by plugging \\(\\alpha\\) back into the first equation along with some other percentile X in order to estimate the share of revenue earned from the top-X% of customers, which we can repeat for other percentiles. For example, the relationship between \\(\\alpha\\) and the revenue concentrated in the top-20% of customer looks like this:

![img](/https://nnamdi.net/content/images/2020/10/qNmFsndFgJ.png)
*Source: David Salazar*

An \\(\\alpha = 1.16\\) yields the classic [Pareto 80/20 distribution](https://www.investopedia.com/terms/1/80-20-rule.asp), where 20% of customers account for 80% of revenue.

Before we proceed, know that this method only works if we assume upfront that the distribution is in fact power law distributed, at least in the tails. We never proved this, so don't interpolate/extrapolate too far with this method.

With that caveat acknowledged, let's throw caution to the wind!

## The evidence

Public companies do their best to hide customer concentration. But at a certain point fiduciary duty requires that they disclose revenue concentration, especially if a few customers account for a large enough chunk of revenue.

Commonly, companies will state that "no customer represents more than X% of revenue." Less frequently, companies will go further, disclosing the number of customers that exceed some revenue threshold, typically $100,000, and the proportion of revenue they represent. This gets framed as a point of strength — "look how many large customers we have" — but it also indicates customer concentration since the X customers with greater than $100K contracts are by definition the X largest customers.

Here's an example from [Slack's S-1](https://www.sec.gov/Archives/edgar/data/1764925/000162828019004786/slacks-1.htm):

![image-20200905234041096](/https://nnamdi.net/content/images/2020/10/image-20200905234041096.png)
*Source: Slack's S-1*

From this sort of disclosure we can calculate the \\(\\alpha\\) and fat-tailedness of customer monetization for public software vendors via the procedure outlined above, plugging in the share of total revenue and total customers represented by these large customers. [I've done the hard work for you](https://docs.google.com/spreadsheets/d/163g-Tn9AdjF4bb4v7j8VidqeqMJvAHvb7EHUR2ctYsk/edit?usp=sharing) for a subset of public software companies:

![image-20200905234326829](/https://nnamdi.net/content/images/2020/10/image-20200905234326829.png)
*Source*

**The alphas are universally below 2**, implying a high level of skew.

A more visual representation of the alphas (with the average in black):

![alpha](/https://nnamdi.net/content/images/2020/10/alpha.png)

The implied top-20% and top-1% revenue concentration are quite large for most companies (blue = top 20%, red = top 1%):

![share](/https://nnamdi.net/content/images/2020/10/share.png)

So, **the top 20% typically represent ~70% of revenue**, while **the top 1% represent ~40%**. Not quite Pareto 80/20, but pretty close! Interestingly, many companies tied to infrastructure in some way like Datadog, Fastly, and Twilio do have 80/20 monetization distributions, at least in some years.

As I caveated, these figures will be off to some degree. I'd guess they overestimate revenue concentration somewhat.

However, I'm comforted by corroboration from a [report by Theta Equity Partners](https://www.thetaequity.com/slack-ipo) that uses a completely different methodology to estimate the monetization distribution of Slack's customer base. Based on the [S-1 filing](https://www.sec.gov/Archives/edgar/data/1764925/000162828019004786/slacks-1.htm), they found that **Slack's top 1% of customers account for 40% of total revenue in 2019**:

> As we mentioned previously, the data and the model imply a high level of variability in the “goodness” of Slack’s customers – **a small (<1%) segment of “heavy” customers accounts for 40% of company’s revenues** and generates revenue per customer which is more than 100 times larger than everyone else.

Using my methodology, I find that **the top 1% of Slack's customers in 2019 represented 43% of revenue**, which is quite similar.

## The implications

It's easy to see these results and think "Yes, sure, customer concentration is a thing. So what?" But the implications of a fat-tailed monetization distribution are profound.

### Why don't software companies index invest?

First, as I discussed previously in [Why Don't VCs Index Invest](https://nnamdi.net/vcs-index-invest/), when facing a sufficiently fat-tailed distribution of returns, it doesn't make sense to be picky or overly concentrated in one's investments. Index investing is the optimal allocation strategy.

Here, returns are synonymous with revenue and investment is synonymous with customer acquisition costs, or CAC. If the distribution of revenue is fat-tailed, vendors should be trying to insert their software into as many customers as possible, as cheaply as possible. Don't try to land large upfront, as this requires investing in a heavy and expensive enterprise sales motion that may not yield results. Instead, spend small and land small, with each customer contract acting as a potential "lottery ticket" that may unlock a much larger contract later on, similar to an early-stage startup investment.

There's some merit to this analogy between venture capital and software go-to-market strategies. One only has to look at estimates of \\(\\alpha\\) for venture capital investments to see that we are dealing with similar phenomena here (ignore the orange footnotes):

![img](/https://nnamdi.net/content/images/2020/10/zxdn9Kwr6j.png)
*Source: Reaction Wheel*

### Whale hunting

Second, as I allude to in the [aforementioned essay](https://nnamdi.net/vcs-index-invest/), finite samples of a positively skewed, fat-tailed distribution tend to underestimate the average, or mean, of the distribution. Large values are rare, so small samples will tend to miss them. Unless you have an extremely large dataset, the "true" mean is typically larger than the mean you measure from the data. So the calculated sample mean tends to increase as the sample size grows, reflecting those large, infrequent outcomes.

Said more precisely:

> An additional difficulty in the numerical estimation of moments—and, therefore, of risk—is due to the very slow convergence of estimated values to the exact values of the process, even if the associated moments are finite. This “slow Law of Large Numbers” is caused by the large weight of rare events (black swans), which take a lot of data to show up, and prevent a proper estimation of the moments of such processes through the moments of a sample. — [Fat tails and black swans: Exact results for multiplicative processes with resets](https://aip.scitation.org/doi/10.1063/1.5141837)

Further, the largest value you are likely to see in a sample of power law distribution (the expected value of the maximum value) is proportional to the sample size \\(n\\) and inversely proportional to \\(\\alpha\\):  
$$  
\mathbb{E} \[x\_{max}\] \sim n^{1/(\alpha-1)}  
$$  
In plain English — your [personal best](https://en.wikipedia.org/wiki/Personal_record) can only get better with more attempts. In the realm of venture, that looks like this:

![image-20200907200713719](/https://nnamdi.net/content/images/2020/10/image-20200907200713719.png)
*Source: Reaction Wheel*

This is why returns in venture capital tend to increase with portfolio size:

![H5qNMNZPFO](/https://nnamdi.net/content/images/2020/10/H5qNMNZPFO.png)
*Source: AngelList*

In the context of software monetization, the "true distribution" is the set of **all** potential customers while the "sample" is the set of _current_ customers. Each customer is like a draw from a random variable representing all potential customers, just like a venture capital investment is like a random draw from a fat-tailed distribution of potential returns.

**My claim is that the average monetization across your customer base — ARR/customer, revenue/customer, etc. — is an underestimate of the "true" or "potential" monetization.** As you land more customers, so the logic goes, the revenue you extract will tend to rise due to this fat-tailed phenomenon, with no change in pricing model or customer targeting.

More _is_ more, or specifically, more customers is more monetization, for the same reason that [larger venture portfolios yield higher returns](https://www.institutionalinvestor.com/article/b1nlj1gb3g3bbd/The-Pervasive-Head-Scratching-Risk-Exploding-Problem-With-Venture-Capital). Your wins get bigger the more broadly you penetrate the market.

It's natural to ask — "**how much does current monetization underestimate potential monetization?**"

[Nassim Taleb](https://arxiv.org/abs/2001.10488) has already done the math for us. Imagine that true average monetization is the sum of the monetization of customers smaller than the largest customer we've acquired thus far, which he calls \\(K\\), and the contribution of potential customers larger than our largest:  
$$  
\text{true mean} = \text{mean of existing customers}\_{<K} + \text{contribution of potential customers}\_{>K}  
$$  
In the chart below, the shaded region represents larger customers yet to be acquired:

![image-20200909125015490](/https://nnamdi.net/content/images/2020/10/image-20200909125015490.png)
*Source: Statistical Consequences of Fat Tails*

What proportion of the total does this extra bit represent? That depends both on \\(\\alpha\\), or how fat-tailed the distribution is, and on the sample size. The smaller \\(\\alpha\\) and the smaller our sample, the more we underestimate the true mean:

![image-20200909124000940](/https://nnamdi.net/content/images/2020/10/image-20200909124000940.png)
*Source: Statistical Consequences of Fat Tails*

So for an enterprise software company with \\(\\alpha = 1.3\\), which is typical in my data set, and 1000 customers, these not yet acquired customers will account for 20% of the true mean. This means the **true monetization is about 1 / 0.8 = 25% higher than current monetization**.

_That number is conservative_ — it assumes that we've already acquired every customer up to a certain size, thereby maximizing monetization of customers equal to or below that size. We haven't really done this though, so even current monetization underestimates how well we could monetize, even without acquiring larger customers. Factor that in, and the degree to which we underestimate true monetization only increases.

### The math behind land and expand

Combined, the above insights form a mathematical justification for "[land and expand](https://saasx.com/2018/09/13/how-to-execute-a-saas-land-and-expand-strategy/)"-style go-to-market strategies.

Here, land and expand is effectively an [indexing strategy](https://nnamdi.net/vcs-index-invest/) — land at as many organizations with as little investment as possible. Every once in a while you'll land a Google, a Facebook, or an Amazon (both figuratively and literally) which will drive a disproportionate share of revenue.

Even if those customers start off small, any given customer could potentially become quite large.

Further, it can make sense to overspend somewhat on establishing those small beachheads, as they likely underestimate the true average contract value. For this reason, common metrics for evaluating the efficiency of software sales like the "[magic number](https://nnamdi.net/magical-magic-number/)" may underestimate the efficiency of land and expand models, especially during the land phase.

> If we suspect right-skewness, the true mean is more likely to be underestimated by measurement of past realizations, and the total potential is likewise poorly gauged. — [The Fourth Quadrant: A Map of the Limits of Statistics](http://www.edge.org/conversation/nassim_nicholas_taleb-the-fourth-quadrant-a-map-of-the-limits-of-statistics)

In fact, one of the best software investments I ever made (that shall remain nameless) was in a company that on its face seemed quite inefficient, with a magic number well below 1. The leadership team preached the virtues of its land and expand model, but our static analysis of its sales metrics was doomed to underestimate its true efficiency, even after many hours spent (by yours truly) wrangling and analyzing the data.

Luckily, we got over our concerns and made what turned out to be a great investment.

With more mathematical context, the story is a visceral personal reminder to properly grapple with the implications and dynamics of fat-tailed software monetization.

## The end

This is just a small taste of fat tails, and I plan to write more in the coming months on their broader implications for high-growth startups. Much ink has been spilled on this topic within the context of venture investing, but not so much for operating the underlying businesses themselves.

Here's a preview of the topics:

-   Why software markets are always larger than we think
-   [Why "Weighted ACV" beats the traditional ACV metric](https://whoisnnamdi.com/weighted-acv/)
-   Why investors consistently undervalue enterprise software and overvalue consumer startups
-   Why open source is built by individuals rather than communities
-   [Why product-market fit gets harder to achieve the longer you search for it](https://nnamdi.net/product-market-fit-is-lindy/)

I've been thinking about some of these essays for the better part of a year, so I'm excited to _finally_ share these ideas. ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[An Image is Worth 16x16 Words: Transformers for Image Recognition (Paper Explained)]]>
                        </title>
                        <description>
                            <![CDATA[Large Transformer trained on large datasets outperform CNN-based architectures and achieve state of the art results on image recognition tasks]]>
                        </description>
                        <link>https://whoisnnamdi.com/transformers-image-recognition/</link>
                        <guid isPermaLink="false">transformers-image-recognition</guid>
                        <category>
                                        <![CDATA[ Research ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Machine Learning ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Developers ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Notes ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Sun, 4 Oct 2020, 23:17:13 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512218911-M_MKtM3ruT.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
-   **Title:** [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://openreview.net/forum?id=YicbFdNTTy)
-   **Authors:** Anonymous (Paper currently in blind review for [ICLR 2021](https://iclr.cc/))
-   **One sentence summary:** Large Transformer trained on large datasets outperform CNN-based architectures and achieve state of the art results on image recognition tasks
-   **Source:** [https://openreview.net/forum?id=YicbFdNTTy](https://openreview.net/forum?id=YicbFdNTTy)
-   **Compression:** 8 pages (original ex. references and appendix) -> 3.75 pages (this article)

## **Summary**

"[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://openreview.net/forum?id=YicbFdNTT)" introduces the Visual Transformer, an architecture which leverages mostly standard Transformer components from the original NLP-focused "[Attention is All You Need](https://arxiv.org/abs/1706.03762)" paper but instead applies them to computer vision, specifically image recognition. Images are transformed into sequences of image patches representing "tokens," similar to word tokens in NLP. The model is trained in supervised fashion on image classification using these patch sequences as input.

The image-based Transformer does not outperform CNNs when trained on mid-sized datasets such as [ImageNet](http://www.image-net.org/), underperforming similar-sized ResNet models. This is likely due to an inability overcome the inherent advantage of CNNs (inductive biases like translational equivariance and locality). However, when the Transformer model is pre-trained on large image datasets (specifically, [JFT](https://arxiv.org/abs/1707.02968)) and transferred to other tasks, the model achieves SOTA results.

The strong results suggest the long hoped-for convergence of architectures across NLP and computer vision may finally be here in the form of Transformers. Per [Andrej Karpathy](https://twitter.com/karpathy), Director of AI at Tesla:

> An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale [https://t.co/r5a0RuWyZE](https://t.co/r5a0RuWyZE) v cool. Further steps towards deprecating ConvNets with Transformers. Loving the increasing convergence of Vision/NLP and the much more efficient/flexible class of architectures. [pic.twitter.com/muj3cR6uGA](https://t.co/muj3cR6uGA)
> 
> — Andrej Karpathy (@karpathy) [October 3, 2020](https://twitter.com/karpathy/status/1312279279741276161?ref_src=twsrc%5Etfw)

## **Key results and takeaways**

-   **Large Vision Transformer model mapping patches of image to classification labels outperforms CNN-based architectures and achieves state of the art results** when trained on large (100M+ images) datasets
-   Transformer model **underperforms CNNs when only trained on mid-sized datasets**
-   **Early layers of the Vision Transformer are able to attend to large chunks of the image**, unlike traditional convolutional layers with are constrained to a local window
-   Self-supervised pre-training with "_masked patch prediction_" achieves decent results but **underperforms supervised pre-training**

## **Methodology**

### **Architecture**

The model effectively analogizes between words as tokens of larger sentences and groups of pixels as "tokens" of larger images. Like a sequence of word tokens makes a sentence, a sequence of pixel patches makes an image. Thus, the input image is broken up into multiple patches of \\(P^2 \\cdot C\\) dimensions representing square subsections of the original image (including all color channels \\(C\\)), forming a sequence of image patches of length \\(N\\).

Image patches \\(\\mathbf{x}\_{p}^{n}\\), typically 16x16 pixels, are embedded into \\(D\\) dimension vectors using an embedding matrix \\(\\textbf{E}\\). The sequence of "patch embeddings" is prepended with a learnable \\(\\texttt{\[class\]}\\) token, similar to [BERT](https://arxiv.org/abs/1810.04805), telling the model to classify the image, leaving us  with a \\((N+1) \\times D\\) dimension vector, \\(\\textbf{z}\\).

The representation of the first token in the output of the final Transformer encoder layer serves as the image representation. The classification head is attached to only this token. Position embeddings are added to the patch embeddings, and these vectors serve as input to the encoder.

The Transformer architecture is constructed as follows:

$$  
\begin{aligned}  
\mathbf{z}\_{0} &=\left\[\mathbf{x}\_{\text {class }} ; \mathbf{x}\_{p}^{1} \mathbf{E} ; \mathbf{x}\_{p}^{2} \mathbf{E} ; \cdots ; \mathbf{x}\_{p}^{N} \mathbf{E}\right\]+\mathbf{E}\_{p o s}, & \mathbf{E} \in \mathbb{R}^{\left(P^{2} \cdot C\right) \times D}, \mathbf{E}\_{p o s} \in \mathbb{R}^{(N+1) \times D}  
\ \mathbf{z}\_{\ell}^{\prime} &=\operatorname{MSA}\left(\mathrm{LN}\left(\mathbf{z}\_{\ell-1}\right)\right)+\mathbf{z}\_{\ell-1}, & \ell =1 \ldots L  
\ \mathbf{z}\_{\ell} &=\operatorname{MLP}\left(\mathrm{LN}\left(\mathbf{z}\_{\ell}^{\prime}\right)\right)+\mathbf{z}\_{\ell}^{\prime}, & \ell =1 \ldots L  
\ \mathbf{y} &=\operatorname{LN}\left(\mathbf{z}\_{L}^{0}\right)  
\end{aligned}  
$$

where \\(\\mathbf{z}\_{\\ell}\\) represents the patch sequence representation output at each layer \\(\\ell\\) of the network and \\(\\mathbf{z}\_{L}^{0}\\) is the first token of the final layer output, which is fed into the classification head with Layer Norm \\((\\mathrm{LN})\\) applied.

Layer representations \\(\\mathbf{z}\_{\\ell}\\) are passed through each Transformer block, where Layer Norm and multi-headed self-attention is applied \\((\\operatorname{MSA})\\), a residual skip connection to the previous layer's representation \\(\\mathbf{z}\_{\\ell-1}\\) added, followed by Layer Norm, and a feed forward layer \\((\\mathrm{MLP})\\) with a residual connection to the intermediate representation, \\(\\mathbf{z}\_{\\ell}^{\\prime}\\).

The authors construct multiple versions of the model at various scales to compare results across model size, similar to BERT. Base = "B", Large = "L", Huge = "H".

![img](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fwhoisnnamdi%2F2OSKlMdMu4.png?alt=media&token=96665b64-59c0-49e6-9620-4e2ed5c194a9)

The authors also experiment with a _hybrid_ architecture, where instead of using patches as the input sequence, the intermediate representation of a ResNet model is used, replacing the patch embedding. The rest of the architecture remains unchanged.

### **Training**

Models are (pre-)trained on multiple image datasets, including ImageNet (1K classes / 1.3M images), ImageNet-21K (21K classes / 14M images), and JFT (18K classes / 303M images). As the largest dataset, JFT-300M is the main focus of the paper, which we will see enables big performance improvements when used in the largest versions of the architecture. Here, the model is pre-trained for 1M steps. The remaining training hyperparameters can found be found in the paper.

The Vision Transformer is fine-tuned at higher resolution than pre-training, which helps performance. However, higher resolution images have more pixels, so the patch sequences are longer. Rather than create extra positional embeddings for these additional tokens, the existing embeddings are interpolated such that multiple higher resolution patches correspond to each lower resolution positional embedding. This is necessary as the additional positional embeddings would not have been seen during pre-training and hence would be meaningless if applied directly. This is the only point in which inductive bias about the structure of images enters into the Vision Transformer.

## **Experiments and results**

The authors choose a number of benchmark tasks for performance evaluation: ImageNet, ImageNet ReaL, CIFAR-10/100, Oxford-IIIT Pets, Oxford Flowers-102, and the 19-task VTAB classification suite.

### **Performance**

ViT-L/16 matches or outperforms [BiT-L](https://arxiv.org/abs/1912.11370) (large ResNet that supports supervised transfer learning) on all datasets with 4-10x fewer computational resources used during pre-training (as measured by TPUv3-days):

![img](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fwhoisnnamdi%2F48LEQrPHie.png?alt=media&token=ea615283-6f31-4f55-adfd-c56daed993cf)

This performance advantage disappears if ViT is trained on a smallest dataset, such as ImageNet. Only with the largest dataset, JFT-300M, do larger models outperform all others:

![img](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fwhoisnnamdi%2FGRzsbFb6vJ.png?alt=media&token=1aec9a71-1329-4340-92ca-de961f4e0f2c)

ViT-B/16 and ViT-B/32 do not gain as much from being trained on larger datasets. This alludes to the intuition that the convolutional inductive bias is most useful for smaller datasets. On larger datasets however, learning the patterns directly is better:

![img](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fwhoisnnamdi%2FXwJwDr5hc7.png?alt=media&token=e301e7d6-acfd-4720-adc3-41756e4d92c1)

### **Performance vs. compute cost**

The Vision Transformer outperforms ResNets in terms of the performance / compute ratio. ViT uses half as much compute to attain the same performance level (x-axis is pre-training exaFLOPs on log scale):

![img](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fwhoisnnamdi%2FNTp0HNzd1y.png?alt=media&token=fc2114ab-293b-46b5-9865-173e36146697)

Interestingly, hybrids slightly outperform the Vision Transformer with small computational budgets but not for larger ones. The authors note their surprise at this result, as it might be expected that convolutional feature maps coming from ResNet would be helpful at any scale

### **Global self-attention**

Self-attention allows the Vision Transformer to integrate information across the entire image, even in the lower Transformer layers. This is unlike CNNs, where only the later layers are able to aggregate information from different parts of the image. Experiments show that the "attention distance" of the attended area is large in the later layers, as expected, but also large in some portion of the earlier layers, demonstrating the ability to learn long-range dependencies. For example, some of the earlier layers heads attend to patches 100 pixels away from on another (right chart below):

![img](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fwhoisnnamdi%2F3WDSyMFDSj.png?alt=media&token=0df75f5f-c3bd-4f74-92a8-d6d1398bbd2b)

The model clearly attends to image regions that are most relevant for classification:

![img](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fwhoisnnamdi%2FdUY64eIB3n.png?alt=media&token=6ce00cb5-cf21-43ae-a904-6e69ac2854d4)

### **Self-supervised pre-training**

The authors experiment with self-supervised pre-training using "_masked patch prediction_," mimicking masked language modeling in the NLP context. With this training regimen, ViT-B/16 achieves 79.9% accuracy on ImageNet, a 2% improvement relative to training from scratch, but 4% lower than supervised pre-training used in the rest of the paper.

## Reflection

OK, so this is exciting stuff. Personally, I've never liked convolutions, for similar reasons that I've never liked recurrent neural networks. There just something _complicated_ about them. Something inelegant. I've always loved more linear architectures, largely composed of feedforward layers with various augmentations like normalization or residual skip connections, i.e. Transformers and attention-based networks in general (and yes I know it's possible to rewrite convolutions as affine transforms). So I think it's very cool to finally see this applied to computer vision with strong results.

I don't know if there were enough ablations to totally prove this, but it seems like insufficient data was the core blocker preventing linear transformations from achieving similar results to CNNs. The inductive biases of CNNs have always been their key advantage, but that advantage seems to wither under the weight of massive global self-attention learned on massive image datasets, at least for image recognition / classification.

The authors also note that, given performance does not yet appear to saturate with increasing model size, the Vision Transformer could potentially be scaled up even further. Nice.

A couple wrinkles to point out.

The paper is currently under double-blind review for conference submission at ICLR 2021, so the authors remain anonymous for now. That said, I'd be shocked if it wasn't Google behind this paper. There are a few tells, like the fact that they use TPUs (Google-specific hardware) for training and the JFT-300M dataset (a Google maintained dataset). As of right now, it doesn't appear the JFT-300M is publicly available -- only Google researchers have access. Therefore, even if the code was made publicly available (which I'm guessing it won't), the results are not replicable. As readers, we have no idea what architectural tricks may have been used that are not made clear by the text of the paper itself, so reproducibility is not guaranteed.

Second, as some folks on Twitter have noted, using these 16x16 patches as input to the model is likely suboptimal. It's at best a first step and the strong performance of the hybrid version of the model (which uses intermediate ResNet representations as input) suggests as much.

As noted before, convolutions seems to help most in low-data / compute regimes, helping the model perform better with less training time for all but the largest model and dataset. Future research may reveal better ways to represent the input image while still avoiding the use of convolutions. If I had to guess, as in NLP, self-supervised pre-training will be key. ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Pre-training via Paraphrasing (Paper Explained)]]>
                        </title>
                        <description>
                            <![CDATA[Transformer model pre-trained on document retrieval and reconstruction performs well on both fine-tuned and zero-shot downstream tasks]]>
                        </description>
                        <link>https://whoisnnamdi.com/pre-training-paraphrasing/</link>
                        <guid isPermaLink="false">pre-training-paraphrasing</guid>
                        <category>
                                        <![CDATA[ Research ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Machine Learning ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Developers ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Notes ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Tue, 29 Sept 2020, 21:03:16 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512216832-image-20200927143543270.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
_(I'm trying something new: summarizing and explaining technical research papers I come across. I spend a ton of free time reading these so I figure why not put some of that time spent to good use? Hoping this benefits others who are knee-deep in machine learning and econometric research.)_

**Title:** Pre-training via Paraphrasing

**Authors:** [Mike Lewis](https://ai.facebook.com/people/mike-lewis/), [Marjan Ghazvininejad](https://www.linkedin.com/in/marjan-ghazvininejad/), [Gargi Ghosh](https://www.linkedin.com/in/gargi-ghosh-5b1087b/), [Armen Aghajanyan](https://www.linkedin.com/in/armenag/), [Sida Wang](https://www.linkedin.com/in/sidaw/), [Luke Zettlemoyer](https://twitter.com/lukezettlemoyer) (all of Facebook AI)

**One sentence summary:** Transformer model pre-trained on document retrieval and reconstruction performs surprisingly well on wide range of fine-tuned and zero-shot / unsupervised downstream tasks

**Source:** [http://arxiv.org/abs/2006.15020](http://arxiv.org/abs/2006.15020)

**Compression:** 10 pages (original ex. references and appendix) -> 3.5 pages (this article)

## Summary

"[Pre-training via Paraphrasing](http://arxiv.org/abs/2006.15020)" introduces **MARGE**, a **Multilingual Autoencoder that Retrieves and Generates**. In this architecture, a _retrieval model_ is trained to score the relevancy of batches of "evidence" documents \\(z\_{1...M}\\) based on their similarity to a "target" document \\(x\\). Simultaneously, a _reconstruction model_ is trained to reconstruct the original target document conditioning on the evidence documents and their relevance scores from the retrieval model. This back-and-forth emulates the behavior of an autoencoder (or even a denoising autoencoder) whereby the mapping of target document to evidence documents serves as an _information bottleneck_ forcing the model to learn document representations that will best enable the reconstruction of the input document.

Once pre-trained on this "paraphrasing" task, MARGE can then be leveraged for downstream tasks like sentence retrieval, machine translation, summarization, paraphrasing, and question answering. Even with no fine-tuning (i.e. "zero-shot"), the model demonstrates impressive performance on these tasks. Performance improves meaningfully with task-specific fine-tuning.

## Key results and takeaways

-   Transformer-based model pre-trained on retrieval / reconstruction performs admirably across multiple downstream generative and discriminative tasks, **including state of the art (SOTA) results on some tasks**
-   **Achieves BLEU scores of up to 35.8** on zero-shot unsupervised document translation with no task-specific fine-tuning
-   Outperforms other unsupervised models on unsupervised cross-lingual sentence retrieval by large margin
-   Impressively, **model is trainable from random initialization** despite "chicken-and-egg" problem of retrieval and reconstruction models being co-dependent

## Methodology

Retrieval and reconstruction together act as an _autoencoder_. The retrieved documents act as a noisy representation of the input, and this process serves as an information bottleneck for the algorithm (an encoder). The meaning of the original input is therefore encoded in these documents via the choice of which documents to retrieve along with the relevance score assigned to each. This mapping of input to retrieved documents is the "encoder" of the autoencoder. The reconstruction of the input via the retrieved documents is effectively the decoder.

Both the encoder and decoder here have a [Transformer](https://arxiv.org/abs/1706.03762)\-like architecture with [multi-headed attention](http://jalammar.github.io/illustrated-transformer/) calculated across multiple layers.

### Retrieval model

The input to the MARGE model is a batch of "evidence" (the documents to be retrieved) and a "target" (the document to be reconstructed). Batches are created by:

-   sharding the document dataset into groups of potentially documents using heuristics like publication date
-   taking the \\(k\\) evidence documents within the shard most similar to the target document (according to \\(f(x,z)\\) below)
-   including a subset of these \\(k\\) documents in the batch, weighting documents in other languages more than same-language documents

Batches are dropped and regenerated offline every 10K training steps by re-computing the pairwise relevance scores across documents.

The retrieval model compares candidate documents by computing a pairwise relevance score \\(f(x, z)\\) between a target document \\(x\\) and evidence document \\(z\\) from the corpus. This takes the form of the cosine similarity of the documents, encoded by a function \\(g(\\cdot)\\), which takes the form of the first token of a 4-layer Transformer network:

$$  
f(x,z) = \frac{g(x) \cdot g(z)}{|g(x)| |g(z)|}  
$$

These relevance scores are used both to select documents to be included in each batch as well as push the model to pay more attention to more similar documents when reconstructing the input, as I'll cover later on.

Using the same function for both targets and evidence ensures documents with similar words are more likely to be mapped to similar representations, even if the encoding function is largely random (which it will be at initialization).

### Reconstruction model

The reconstruction model computes the likelihood of the target document tokens, conditioned on the evidence documents within the batch and associated relevance scores. The vector representations for all evidence documents within each batch are concatenated together into a single vector \\(z\_{1...M}\\) before being used for reconstruction:

$$  
L\_\theta = - \sum\_i \log{p\_\theta(x\_i|z\_{1...M}, f(x\_i,z\_1), ..., f(x\_i,z\_M))}  
$$

During decoding, attention weights are calculated for each token of the target across the set of concatenated evidence documents, meaning that the weights correspond to the attention the decoder should pay to each token of each evidence document at each time-step, capturing token-wise similarity as in standard [dot-product attention](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html). Here however, the relevance scores for each document are added to the attention scores for the tokens from that document, multiplied by a trainable scalar parameter \\(\\beta\\). These biased scores are then softmaxed, yielding the attention weights for each time-step, layer \\(l\\), and attention head \\(h\\):

$$  
\alpha = softmax\_{z\_{1...M}}(Q^{lh}(x\_i)K^{lh}(z\_{1...M}) + \beta f(x\_i,z\_j)) \in \mathbb{R}^{|x\_i| \times \sum\_j |z\_j|}  
$$

Backpropagating the reconstruction loss improves both the reconstruction model and the relevance model via this attention mechanism.

### Architecture and training

The model encoder (distinct from the encoder \\(g(\\cdot)\\) used to encode individual documents) is a 12-layer Transformer network with dimension 1024 and feedforward layers of size 4096. The decoder is similar in structure but the feedforward layers there are size 16536 and 4 additional Transformer layers are added to the base with only self-attention (i.e. attention only to other words within the same document) and feedforward layers of size 4096. These supplemental layers allow the words in the target to contextualize locally before doing so across documents. The first four layers of the encoder are used in the relevance model (this is the \\(g(\\cdot)\\) referred to above).

The model is initially pre-trained on the [CC-NEWS corpus](https://commoncrawl.org/2016/10/news-dataset-available/) for 1M total steps. At this point, the checkpointed model is referred to as MARGE-NEWS. Then, the authors further pre-train for an additional 100K steps on Wikipedia, referring to the resulting model as MARGE.

For fine-tuning, a different procedure is used for generation and classification problems. For generation (translation, summarization), the task input is fed into the encoder and the final output is generated by the decoder. For classification, the task input is fed into both the encoder and decoder

The MARGE model ends up with 963M parameters, more than most of its comparison set of "the strongest available multi-lingual pre-trained models" ([mBERT](https://arxiv.org/abs/1810.04805?source=post_page), [XLM](https://arxiv.org/abs/1901.07291), [XLM-R](https://arxiv.org/abs/1911.02116), [MMTE](https://arxiv.org/abs/1909.00437), [mBART](https://arxiv.org/abs/2001.08210)), but is trained on fewer languages and a medium-sized dataset and a medium amount of GPU pre-training days:

![image-20200927144908141](/https://nnamdi.net/content/images/2020/09/image-20200927144908141.png)

## Experiments and results

The papers show the wide applicability of MARGE and its paraphrasing pre-training technique by evaluating its performance across wide array of NLP tasks. MARGE performs well across many tasks, wider than any previous pre-trained model. This includes zero-shot document translation, and performance improves further with fine-tuning. The strong results of MARGE establish retrieval / reconstruction as a viable alternative to MLM for pre-training. The success of the method is partly driven by the higher relatedness of the pre-training task to downstream tasks.

### Document-Level Machine Translation

The authors demonstrate the models strong translation performance across a number of language pairs and within both zero-shot and fine-tuned settings, achieving 35.8 BLEU in the case of unsupervised translation from German into English on the [WMT19 dataset](http://www.statmt.org/wmt19/), the highest score ever achieved by a system trained with no bitext (as in iterative back-translation). Performance does vary significantly by language:

![image-20200927144848658](/https://nnamdi.net/content/images/2020/09/image-20200927144848658.png)

Supervised translation with labeled bitext improves performance further, achieving competitive results against mBART:

![image-20200927145026936](/https://nnamdi.net/content/images/2020/09/image-20200927145026936.png)

### Cross-lingual Sentence Retrieval

It would make sense that a model trained retrieve similar documents, sometimes in a different language, would perform well on a sentence retrieval task. Confirming this intuition, MARGE outperforms other unsupervised models by almost 10 points on the [BUCC2018 benchmark](https://comparable.limsi.fr/bucc2018/bucc2018-task.html), though the embeddings are tuned somewhat on BUCC development data:

![image-20200927144959013](/https://nnamdi.net/content/images/2020/09/image-20200927144959013.png)

### Summarization

The authors evaluate the model's performance on monolingual sequence-to-sequence generation via text summarization tasks sourced from the [MLSum](https://arxiv.org/abs/2004.14900) dataset. Performance is compared across multiple languages, and the extractive oracle performance level is shown for comparison-sake. What's impressive here is that MARGE's summaries are inherent abstractive - the model is generating summaries in its own words, not simply extracting words from the input text - and yet it manage to outperform an extractive mBERT model on a fundamentally extractive performance metric ([ROUGE-L](https://en.wikipedia.org/wiki/ROUGE_\(metric\))). This is not trivial to do:

![image-20200927145105128](/https://nnamdi.net/content/images/2020/09/image-20200927145105128.png)

### Question answering and paraphrasing

The [MLQA dataset](https://arxiv.org/abs/1910.07475) is used to test MARGE's performance on question answering. MARGE what over or underperforms XLM-R depending on the language, on average underperforming by a small margin (as measured by F1 score). Paraphrasing is tested on the [PAWS-X paraphrase detection dataset](https://arxiv.org/abs/1908.11828), where the model is trained on English and zero-shot transfer is tested on other languages. MARGE demonstrates SOTA performance relative to XLM-R:

![image-20200927145133850](/https://nnamdi.net/content/images/2020/09/image-20200927145133850.png)

## Conclusion

I think this is an interesting paper mainly due to its demonstration of a new pre-training methodology that appears to work as well as masked language modeling for NLP-related tasks. The literature around pre-training grows daily, but I think we've only really scratched the surface of potential pre-training methods. That's why I'm excited to see some new blood in NLP pre-training, which has been dominated by masked language modeling a la BERT.

That said, the paper involves _some_ hackery that seems necessary to get the model to train. This includes the heuristics that are used to group documents into relevant batches to retrieve from (which is inherently non-differentiable) as well as others tricks like encouraging cross-lingual behavior via hardcoded overweighting of cross-lingual documents in the collected batches. These tricks act as a sort of "intelligent design" mechanism which is not uncommon in deep learning but does mean that the model is not entirely trainable end-to-end via gradient descent (though it mostly is).

Additionally, these steps are outlined in the paper but due to limitations of the human language are not easily replicable solely via the paper's explanation. The authors would need to open source the underlying model code for others to replicate and verify these results.

Model-specific training and architectural hacks aside, MARGE's performance is quite impressive and adds a new feather in the pre-training quiver for NLP models. ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Top Three Strategic Priorities of Developer Productivity Startups]]>
                        </title>
                        <description>
                            <![CDATA[What's top of mind for developer productivity leaders]]>
                        </description>
                        <link>https://whoisnnamdi.com/developer-productivity-strategic-priorities/</link>
                        <guid isPermaLink="false">developer-productivity-strategic-priorities</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Developers ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Tue, 18 Aug 2020, 15:30:32 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512212692-strategic-priorities-1.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
Developer productivity is undergoing a tectonic shift. New software development paradigms and tooling have accelerated the pace and productivity of modern software teams, quickening the "shipping speed" of new software.

To better understand the strategic landscape, my good friend and colleague, [Clio Smurro](https://www.linkedin.com/in/clio-smurro-31967b9/), and I interviewed founders and executives at next-generation software and infrastructure startups pushing the developer productivity frontier to get their thoughts and insights. They shared their views on:

-   [major industry trends](https://nnamdi.net/developer-productivity-trends/),
-   **top strategic priorities (you are here)**, and
-   [biggest challenges and pain points](https://nnamdi.net/developer-productivity-challenges/)

In this second chapter, we share our findings on the **top strategic priorities** of developer productivity startups, including:

-   [Priority #1: Product-(go-to-)market fit](#priority-1-product-go-to-market-fit)
-   [Priority #2: Sow first, reap later](#priority-2-sow-first-reap-later)
-   [Priority #3: Customers must succeed... or we will fail](#priority-3-customers-must-succeed-or-we-will-fail)

Want to be notified when we publish part three of our findings? Subscribe below, and we'll also send you a nicely formatted PDF of our research!

### Receive a report with the full results

  

Send Report ⚡

## Priority #1: Product-(go-to-)market fit

Developer productivity companies work extremely hard on the initial V1 of the product. As developers themselves, these founders are targeting difficult technical problems where elegant solutions do not yet exist, hoping to bring a product to market that is fundamentally innovative. While in "heads down" mode, however, it's easy to ignore market sizing. Many developer productivity founders do not sit down to make initial estimates of their addressable market until after the technology has already been built:

> **2020 is all about product-market fit.** The last year and a half, we built a platform that solves a hard technical problem. Now we've come out of stealth. The question is — **is this a market for 5 or 5,000? Is it 5 or 10 years out?** — Founder, Data Science Startup

> **We are doubling down on product-market fit.** We need to figure out who buys our tool, what their characteristics are, and how to find more of them. — Executive, Developer Tools Startup

Successfully growing a developer productivity startup from the seed stage onward involves more than simply "knowing" what your market is. More than that, it requires forging a path from the laptops of your engineering team to the credit cards of potential users, teams, and organizations. "How to distribute" is not necessarily the same question as "how to sell," a question which confounds many developer productivity companies. This slight variation of product-market fit, which we call **product-go-to-market fit**, is tricky for developer productivity startups, who often achieve significant adoption of their tools well before revenue catches up, similar to many consumer startups.

As technically-oriented companies founded by technically-oriented people, developer productivity companies struggle with marketing, positioning, and sales, which are rarely their strong suits. For example, early teams often lack someone with product marketing or sales expertise, leading to ambiguity and uncertainty around how to initially frame the product or company:

> We need to nail down our positioning from a marketing standpoint. **We were founded by 2 ex-engineers, and we haven't had a marketing person this whole time.** What is our positioning? How do we present that in an effective way? — Developer Advocate, Application Infrastructure Startup

Many companies have aligned themselves to burgeoning trends they've presciently identified, but invariably they arrive at a solution in advance of most market demand. This puts many in the difficult spot of deciding both how to bridge the gap to where real flesh-and-blood customers are today and how much bridge is worth building. Oftentimes there's a significant distance between next-gen startups on the bleeding edge and old-school organizations where most of the revenue lies:

> How many people actually need real-time prediction vs just batch prediction? If they aren't yet ready for real-time, **do we need to start by providing them stepping stones to meet them where they’re at?** — Founder, Data Science Startup

Part of that bridge comes in the form of product improvements that enhance the applicability of the product to real customer use cases. One founder framed this as "product geometry":

> What we've built is a relatively new concept for people. **But we have to make sure that the product geometry maps well to actual customers needs for enough people.** — Founder, Data Science Startup

On the other hand, sometimes the product is fine as-is but education is required to both generate awareness and understanding of the potential value of the product or service. This doesn't always work, naturally leading to the question — _is it them or us?_:

> **If you talk to potential customers, there’s a lot of customer education required.** If you’re trying to explain it and they’re not getting it, is that their problem or yours for not explaining it right? — Founder, Data Science Startup

Another vector upon which to achieve product-go-to-market fit is in the business model itself. Pricing can be a meaningful barrier to adoption. If it's too expensive to get started with the product, most developers and small organizations will never bother to try it out, which can be a missed a opportunity:

> **We want to bring down the cost of using our technology to the point where we can give it to developers for free**, especially since Amazon and others provide a free version. — Executive, Application Infrastructure Startup

This is not simply a matter of changing the pricing on your website. For example, if your distributed database requires a node of at least a certain size to avoid performance issues, lowering the barrier to entry for prospective customers requires modifying the product. Certain technologies, especially infrastructure running clusters or nodes in the cloud may require significant architectural changes to run on smaller instances:

> **Our technology is expensive to run.** Our cheapest cluster is still $50+/month. We'd love to get it down to $10/month so **more people can get a small slice of it instead of fewer people getting a big slice of it.** — Executive, Application Infrastructure Startup

## Priority #2: Sow first, reap later

Developer productivity startups execute a very clear one-two punch — plant and nurture the seeds of grassroots adoption first, harvest revenue second. In other words, if they use it, they will (eventually) pay for it (or so the theory goes):

> Our number one goal is to increase end user adoption. Our second goal is revenue. **We believe that if they try our product, they will spend for it eventually**. — Executive, Developer Tools Startup

Oftentimes, the first step is achieved via an organic open core model where core functionality is made available in the open source edition of the product, with extra team and enterprise-focused features available as closed source or open source activated via license key. Broadening open source adoption becomes a singular goal for many:

> Our mission is straightforward — **we want to 10x our open source community**. — Founder, Data Science Startup

> **We want to make \[our open source technology\] a huge success**. We want to grow the open source community. — Founder, Application Infrastructure Startup

Self-service is the key to bottoms-up adoption. Developers want to get up to speed as quickly as possible with minimal friction. They need to see value immediately and feel like they are in control. Typically, this means having the ability to spin up a small instance or process on their laptop via command-line interface (CLI) or other tool:

> **We need more bottoms-up adoption from developers.** Building capabilities for developers to spin up their own instances is key. — Executive, Application Infrastructure Startup

But the "_if they use it, they will pay for it_" logic does not always play out. Sometimes, bottoms-up adoption works well for the free version of a tool but _doesn't_ work so well for the paid version. It's not uncommon to see technologies, especially those that are open source, struggle to convert adoption into paid usage. And when users do want to pay for more features, developer productivity startups quickly find out its much harder to onboard paid clients who now have higher expectations, need more hand-holding, and require service-level agreements (SLAs). This makes paying customers much more difficult to adequately serve than open source users who can simply download and go:

> We have tons of interest in our beta. We want to move them from beta users to paid customers. **We need to figure out a better way to onboard clients to make us more efficient and scalable**. — Developer Advocate, Application Infrastructure Startup

> **Every account is very manual right now.** We're very much a "white glove" service. — Manager, Application Infrastructure Startup

One might expect ambitious growth plans to temper in the face of the COVID-19 pandemic. Interestingly, most of the startups we spoke with have seen little measurable impact from COVID-19. Most are beginning from a small starting point with little to lose, meaning that everything is effectively upside. Yes, the pandemic is certainly generated waves, but these early-stage vendors manage to stay largely under the surf:

> **We’re not big enough for COVID to affect us.** We're starting from zero, so all growth is significant. All clients are growth for us. — Executive, Data Science Startup

> **We haven’t really been affected by COVID at all.** Some consumer and travel customers have had to pull out or pause. Otherwise, no issues from COVID. — Developer Advocate, Application Infrastructure Startup

As companies targeting developers, who are more accustomed to working remote, developer productivity companies have not seen much disruption in buying or usage patterns. In some cases, COVID-19 has actually served as a tailwind, with work from home generating a need for better, more robust tools that individuals and small teams can use independently to get productive work done:

> **If anything it’s helped us.** People are home doing data science and they need right tools. — Executive, Data Science Startup

## Priority #3: Customers must succeed... or we will fail

> We’re starting to move to a phase of our business where customer success is increasingly important. — Executive, Application Infrastructure Startup

Moving fast and breaking things makes sense in the earliest phases of development when shipping product out the door takes precedence, but early design decisions eventually catch up with you. The best things move quickly in the early days to achieve a minimum viable product that serves the more obvious use cases but pivot at some point to cleaning up the mess they made:

> **We started by shipping a lot of things really quickly, but what we shipped was buggy and broke a lot of design conventions**. We’re working on re-skinning it, improving accessibility, etc. **Fit and finish. Polish**. — Manager, Developer Tools Startup

Here again, education comes up as important process getting users up to speed and reducing time-to-value. Ironically, developer productivity tools, especially on the infrastructure side, can be complex and difficult to learn, emphasizing the importance of high-quality documentation and customer support.

> **Improving the user experience is a priority.** The process of signing up, using the product, all the different use cases. Our product is difficult and complex to learn. **We need to make it easier.** — Executive, Developer Tools Startup

Eventually, once the product is in good shape and serving initial use cases well, the focus shifts to customer success and retention. This often involves solving for issues that do not necessarily arise in casual usage but do crop up in large-scale production environments or other serious use cases. Sophisticated customers have little patience for a tool that fails when it counts most:

> For the first few years, it was about getting our first use cases. Does it demo well? Can it survive contrived failure scenarios? **Now that we have a lot of customers, we focus on retention.** — Executive, Application Infrastructure Startup

Customer success means serving all stakeholders well. This means both individual contributors and managers, the front office and back office, and so on. For example, infrastructure companies must appease not only the principal architect tasked with conceiving the target architecture that will then be implemented, but the individual developers themselves who will either implementing or maintaining the technology on an ongoing basis:

> **We need to make our technology more developer friendly.** Infrastructure architects love us, but once the architects make that decision, **they need people to build that vision**. — Executive, Application Infrastructure Startup

As developer productivity startups move up-market, customers demand more enterprise-grade features. Developer productivity becomes _organizational productivity_. In this sense, customer success becomes a source of product development feedback, informing the roadmap and guiding vendors toward new revenue opportunities and larger deals:

> **Our customers need to be able to plug our technology into the rest of their enterprise stack.** Single sign on, lightweight directory protocol (LDAP), etc. We call this enterprise "ruggedization." — Executive, Application Infrastructure Startup

In addition to enterprise features, developer productivity startups must plug into a number of popular developer and collaboration tools already in place in many of the organizations they wish to penetrate. Integrations to Slack, Atlassian, Git, and other tools becomes "table stakes" — practically required to see wide adoption given how embedded these other tools are in the workflows of most developers:

> We want to deepen our integrations. Jenkins, GitHub, Atlassian, GitLab, Microsoft. **We want first-class plugins.** — Executive, Application Security Startup

## Conclusion

As with most startups, developer productivity companies must adapt to an evolving strategic context as they scale and mature. What worked in stealth might not work after public launch; what passed muster as a scraggly startup selling to other startups might not cut it for buttoned-down enterprise clients.

This strategic evolution comes in three broad phases:

-   **First**, developer productivity startups map out and blaze a trail from the source code to the credit card numbers of user and buyers at target organizations
-   **Second**, having identified fertile soil, they seed the market via organic adoption, making bets that will hopefully bear fruit the form of revenue in the coming month and years
-   **Third** and last, the best developer productivity companies ensure their fledgling customers do not die on the vine but rather grow their usage and expand their contracts over time

Successful execution of this three part plan lays a strong foundation for developer productivity companies to thrive and survive in the face of strategic threats (the public cloud casts a long shadow) or economic disruption.

Be informed of our next and final publication in this series, the **biggest challenges and pain points of developer productivity companies**, by entering your email below: ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Six Trends Shaping Developer Productivity]]>
                        </title>
                        <description>
                            <![CDATA[We interviewed developer productivity leaders. Here's what they said.]]>
                        </description>
                        <link>https://whoisnnamdi.com/developer-productivity-trends/</link>
                        <guid isPermaLink="false">developer-productivity-trends</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Developers ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Tue, 14 Jul 2020, 19:20:44 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512207729-image-20200714021923342.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
Developer productivity is undergoing a tectonic shift. New software development paradigms and tooling have accelerated the pace and productivity of modern software teams, quickening the "shipping speed" of new software.

To dissect these trends, my good friend and colleague, [Clio Smurro](https://www.linkedin.com/in/clio-smurro-31967b9/), and I interviewed founders and executives at next-generation software and infrastructure startups pushing the developer productivity frontier to get their thoughts and insights. They shared their views on:

-   **major industry trends (you are here)**,
-   [top strategic priorities](https://nnamdi.net/developer-productivity-strategic-priorities/), and
-   [biggest challenges and pain points](https://nnamdi.net/developer-productivity-challenges/)

In this first chapter, we share our findings on the important trends shaping developer productivity, including:

-   Trend #1: [Developers have the power... and the purse](https://nnamdi.net/developer-productivity-trends/#trend-1-developers-have-the-power-and-the-purse)
-   Trend #2: [Application security is "shifting left"](https://nnamdi.net/developer-productivity-trends/#trend-2-application-security-is-shifting-left)
-   Trend #3: [The distributed cloud is having its COVID moment](https://nnamdi.net/developer-productivity-trends/#trend-3-the-distributed-cloud-is-having-its-covid-moment)
-   Trend #4: [Remote software development is here to stay](https://nnamdi.net/developer-productivity-trends/#trend-4-remote-software-development-is-here-to-stay)
-   Trend #5: [The growth of Python, Spark, and Big Data](https://nnamdi.net/developer-productivity-trends/#trend-5-the-growth-of-python-spark-and-big-data)
-   Trend #6: [Transfer learning from DevOps to data science and data engineering](https://nnamdi.net/developer-productivity-trends/#trend-6-transfer-learning-from-devops-to-data-science-and-data-engineering)

Want to be notified when we publish part two of our findings? Subscribe below, and we'll also send you a nicely formatted PDF of our research!

### Receive a report with the full results

  

Send Report ⚡

## Trend #1: Developers have the power... and the purse

![](/content/images/2020/07/dev-power.png)

> "**Buying products that save developer time is no longer an argument you need to explain**. People get it." — Executive, Developer Tools Startup

Software engineers continue to be a scarce resource in most organizations. Companies are increasingly focused on enhancing the productivity of developers. In doing so, power and autonomy flow to developers, and the dollars are quick to follow.

Vendors are reflecting this new reality in their go-to-market positioning and sales efforts:

> "In our sales conversation, **we frame things in terms of productivity and developer time saved**... You're comparing the cost of the product against engineering time saved." — Executive, Developer Tools Startup

Even before sales gets involved, developers are adopting software they need to get their work done on their own, often without the involvement or permission of procurement or upper management. Developers know what they want, the tools they love, and the technologies that enable their ideal architectures and designs.

> "**We're seeing lots of self-serve.** Developers are getting more autonomy as buyers. **Most of our sign-ups are via bottoms-up** — people signing themselves up, after which our sales team eventually reaches out to them." — Executive, Developer Tools Startup

Unlike many other domains of enterprise software, where features are built to appease higher and higher levels of management rather than end users themselves, the core developer experience remains hugely important. If anything, analogous trends in observability and monitoring are developing in a symbiotic rather than antagonistic fashion with developer productivity.

As engineering teams scale, the need for agile workflows becomes apparent, drawing many toward the burgeoning DevOps paradigm and its associated ecosystem of tools. DevOps enables ongoing operation of and rapid iteration on software via Git-based version control, continuous integration, continuous deployment, security testing, and more.

> "DevOps hasn’t been around for long, but more companies are realizing the need/value for it." — Developer Advocate, Application Infrastructure Startup

To the extent that vendors are attempting to appease management, they are doing so by building unified product ecosystems. These enable customers to purchase multiple component tools of the overall software development lifecycle in a single package. While these do carry some benefits for end users, most still prefer to purchase the best solution for each task, again reflecting in influence and clout of individual software developers:

> "GitHub, Atlassian, Microsoft... **They’re trying to get everyone to adopt a unified tool system.** But most people still go with best-of-breed, as far as tools go. The idea though is that some people will eventually go with more of a “you can’t get fired for buying IBM” approach, where you buy everything from a single vendor." — Executive, Application Security Startup

## Trend #2: Application security is "shifting left"

![](/content/images/2020/07/app-sec-left.png)

In past eras, application security was oftentimes dealt with after the fact. Software would be largely complete by the time security analysts had a chance to examine and poke holes in its defenses. In main cases, this might not even happen until after code is already running in production, where any vulnerabilities may have already been exploited by nefarious actors:

> "Once the code is built, the artifact goes into a registry. The security team wants to know...**what are the risks? Is this meeting my policies?**" — Executive, Application Security Startup

No longer. Application security is now a tier one priority in many organizations:

> "Security used to be an afterthought. In the past, someone would write code, someone would deploy code, and then someone else would handle security. **That boundary doesn’t exist anymore.**" — Executive, Developer Tools Startup

Spurred by the rash of high-profile security incidents and gaffes at major corporations around the world, organizations are challenging their development teams to take on more of the security burden upfront, well before software is even ready for production or artifacts have been built. "DevSecOps" is born:

> "**Shifting left means move everything towards the developer.** It doesn't have to be a security person's responsibility to ensure secrets are secure — the developer can do this now too. The more tooling you give, earlier on in the process of writing the application, the easier this is." — Director, Developer Tools Startup

Security vendors continue to sell mostly to security teams but realize their tools are increasingly landing directly in the hands of developers themselves:

> "We don't sell to developers, we sell to security teams...but at the end of the day, **it’s the developers who need to take more upfront responsibility for security.**" — Executive, Application Security Startup

It's easy to think these new tools are only valuable to large enterprises paranoid about breaches, hacks, and other threats to application security. Not true, say some of the security leaders we spoke to, who emphasized that the heightened focus on security is reverberating through the software development industry, at both large organization and small:

> "These security initiatives are not just for big companies...**every company needs them**." — Executive, Developer Tools Startup

Then there's the concept of "low trust" or even "no trust", where applications do not give each other the benefit of the doubt and every app must prove its credentials in order to send and receive requests and data from other apps and microservices. This adds new complexity to software development, heightening the important of thinking through security implications early on in development:

> "In low trust or no trust environments, how do you make sure applications can talk to each other?" — Executive, Developer Tools Startup

These complexities are inevitable, but vendors also know there are limits to developer patience. They are keen to insert security tooling into workflows as seamlessly as possible. Usability drives usage — if a tool is to difficult to use or increases cycle times too dramatically, developers won't use it, defeating the purpose entirely:

> "DevOps folks are often not security experts. **They’re looking for usability.** How easy is this to access? Does it fit in our existing workflows? Can it plug into [Jenkins](https://www.jenkins.io/)? I don't want my developers having to use a new tool." — Executive, Application Security Startup

As development and security increasingly merge, buying patterns and processes will incorporate the needs of both stakeholders, and vendors will need to adjust their tactics appropriately:

> "**DevOps needs to like it, SecOps needs to buy it**. Both are involved in the purchase process." — Executive, Application Security Startup

## Trend #3: The distributed cloud is having its COVID moment

![](/content/images/2020/07/distributed-cloud.png)

Demand for distributed computing is growing and that demand has only surged in the COVID era.

> "**COVID has caused a 2-3 year acceleration in everyone's journey to the cloud.** The companies who have survived and thrived are the ones which evolved sooner." — Executive, Developer Tools Startup

Elastic scaling of compute, storage, and other resources is important in a divergent set of scenarios. Some businesses (Zoom, Fastly, Amazon, Instacart, etc.) have seen demand for their services surge, requiring rapid scale up of existing deployments, assisted by prescient decisions to factor applications into microservices.

On the other hand, certain companies have seen business dry up overnight. This makes the ability to scale down equally important, allowing costs to flex proportionately with revenue:

> "Do only what's needed. That's key for dynamic shifts in needs. **You need to design software for scaling up, but the infrastructure also needs to be able to scale down**." — Executive, Developer Tools Startup

But what if your business runs on legacy applications or infrastructure technologies? You're in a much tougher position, which is prompting many organizations to ask the question — can you accelerate developer productivity within legacy application development?

> There's a lot of legacy .NET applications from 10 years ago that weren't originally built with containers. Organizations want to get those into containers. A question people are asking — **can you get developer productivity with legacy applications too?** — Executive, Application Security Startup

Increasingly, the answer is — yes. Legacy applications are being refactored from monolithic to microservices-based architectures, leveraging the power of containers and other associated advancements:

> "We are seeing a movement from cloud "greenfield", only using the cloud for new applications, to "lift and shift", getting legacy applications into containers and into the cloud. We've started working with Windows-based containers." — Executive, Application Security Startup

While old-school organizations and engineering teams play catch up, next-gen startups and technology companies aren't waiting around.

> "**The biggest trend right now is the move to serverless** — functions as a service, hiding more complexity from developers. Serverless is a way for developers to just focus on stateless applications, to just focus on what they use most directly." — Executive, Application Infrastructure Startup

Serverless has emerged as a major trend. Leveraging the "infinite" scalability of the major cloud providers, serverless promises to decompose applications into a series of function calls — without regard to the underlying infrastructure.

While the cloud providers have always had a clear incentive to push such a regime, it seems like development teams themselves are warming up to the idea, thought it's full realization is still years away:

> "At our company, we talk about nodes and clusters today. We think of elastic pools of storage and compute that you can just use. But, **a few years from now, we won't even be worried about literal nodes and clusters**." — Executive, Application Infrastructure Startup

## Trend #4: Remote software development is here to stay

![](/content/images/2020/07/remote-development.png)

In conducting our interviews, we were struck at how little COVID had affected the productivity of most software development teams. Perhaps we had a biased sample (we mostly spoke with startups), but the near-universal response was that the move to mandatory remote work had been relatively smooth. Many already had significant portions of their development teams working remotely, which helped to ease the transition.

Further, many had seen little disruption in their go-to-market efforts, given their focus on selling to developers who had themselves seen little interruption in their work.

If it is true that at least some teams and organizations can be nearly as productive working remotely as in-person, we may see some aspects of this new work paradigm stick around long after COVID subsides.

> "The shift to working remote is happening... It's month three of the new normal, and I think **there will be long-term changes as a result of this**." — Executive, Developer Tools Startup

Software developers who enjoy this new style of work may have other reasons to rejoice too. [As I've previously written about](https://nnamdi.net/remote-software-developers-earn-more/), developers who work remotely earn up to 22% more than developers who don't. Of course, as with many aspects of a _forced_ transition to remote work, it's not clear whether this result applies when organizations had no choice about the move.

## Trend #5: The growth of Python, Spark, and Big Data

![](/content/images/2020/07/python-spark-big-data-growth.png)

> "From our perspective, **Python is winning**, and we see that trend continuing." — Executive, Data Science Startup

Teams love Python for its ease-of-use and rapid time-to-value even for relatively non-technical individuals, who can quickly get up to speed with the language and generate value output:

> "**It’s easy to learn….people can learn it in 8 weeks and then be useful in a Fortune 1000 company**." — Executive, Data Science Startup

In addition to its user-friendliness, Python is revered for its ecosystem of packages for tackling difficult data science challenges and processing large data sets.

Speaking of large data sets, after much hype and suspense, Big Data has finally arrived and is a major driver of Python's massive popularity:

> "Big Data is not just trendy anymore, but **it’s actually happening now**. Not like 5 years ago when it was first hyped." — Executive, Data Science Startup

These large datasets are increasingly shifting to the cloud, where various storage offerings from S3 to Snowflake and more have proliferated, offering no shortage of options at competitive prices for various performance levels and data access frequencies. Leaders agree that Python is the preeminent language for handling data in the cloud:

> "We are seeing a shift to the cloud, and **Python is dominant for data in the cloud**." — Executive, Data Science Startup

However, having been conceived well before the microservices revolution or the cloud generally, Python out-of-the-box does not come with much distributed computing functionality, though multiple vendors and technologies have arisen in recent years to cover this gap ([Dask](https://dask.org/), [Ray](https://ray.io/), etc.):

> "Python... is the fastest growing language and very popular, but **Python has no distributed functionality**." — Founder, Application Infrastructure Startup

Have no fear though, [Spark](https://spark.apache.org/) is here!

Spark bills itself as "a unified analytics engine for large-scale data processing", which in layman's terms mean it's very, very fast — fast enough to handle large datasets at high throughput. Spark is built from the ground up with distributed computing in mind, enabling it to take advantage of the advancements in cloud computing we discussed above. Spark also offers a number of high-level operators that enable interoperability with more popular languages like Python, SQL, Java and more.

> "Big data development cycles used to take forever, not fast enough for software developers. **Spark has meaningfully sped up the cycles**." — Founder, Data Science Startup

## Trend #6: Transfer learning from DevOps to data science and data engineering

![](/content/images/2020/07/devops-dataops-mlops.png)

Historically, data science has not had the emphasis on fast iteration and development cycles that DevOps has had. When merged with traditional software engineering, this is less of a problem. But as data science as a practice gains clout, data science professionals are being carved out into their own teams. While this comes with certain benefits, it often comes at the cost of speed:

> "Data science is now being carved out of development teams, and the data-related development cycles have elongated." — Founder, Data Science Startup

Additionally, data science is only becoming more complex. Data engineering has emerged as a key component of the overall data science lifecycle and, while less sexy than building and training the latest deep learning models, is often the phase that takes the longest in any given project. Many consider it to be an entirely different skill set from core data science. ETL (Extract, Transform, Load) tooling is just one example:

> "Let's look at the problem of managing ETL pipelines. There are actually two pieces — the ETL pipeline that transforms the data, and the pipeline that manages the ETL pipeline itself." — Executive, Data Science Startup

DataOps and MLOps are the response to these complexities and speed bumps, bringing best practices from DevOps to the realm of data science and machine learning:

> "DataOps and MLOps bring DevOps principles such as agile development to data and machine learning." — Founder, Data Science Startup

Instead of being crushed under the weight of increasingly intractable datasets and data engineering puzzles, DataOps and MLOps help data scientists and engineers better wrangle the data development process itself and achieve business outcomes with the same agility as traditional software development:

> "We're big on DevOps and the empowerment of the data scientist and data engineer. **They should have control over the end-to-end process.** Whoever is the creator is also the person who’s responsible for the ongoing success of the artifact." — Executive, Data Science Startup

DataOps and MLOps are opening new possibilities for data and model version control, machine learning feature engineering and storage, and more. Think unit or regression tests, except for datasets and machine learning models:

> "In GitOps, the continuous integration / continuous delivery / continuous deployment pipeline checks the code and deploys the application to a staging environment, eventually ending up in production. Most tech-forward companies have adopted that now. **We want to bring that to the data scientist**." — Founder, Data Science Startup

The grand vision? Driving efficiencies that will enable teams and organizations to keep up with the growth of Big Data:

> "**The trend toward DataOps will be big.** It'll make organizations more efficient." — Executive, Data Science Startup

## Conclusion

Developers are increasingly considered one of the most important constituencies within organizations. Organizations both old and young are generating and consuming greater amounts of software, and developers remain the basic economic unit of software production.

As the scarce input of the software production function, software engineers don't come cheap, and it's therefore critical to maximize their productivity and output. As software grows only more powerful, valuable, and essential, anything that makes software engineers more productive will be similarly potent and relevant.

Be informed of our next publication in this series, the **top strategic priorities of developer productivity companies**, by entering your email below: ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Why Don't VCs Index Invest?]]>
                        </title>
                        <description>
                            <![CDATA[VCs are picky, not because they have so many options but because they have so few.]]>
                        </description>
                        <link>https://whoisnnamdi.com/vcs-index-invest/</link>
                        <guid isPermaLink="false">vcs-index-invest</guid>
                        <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Fri, 3 Jul 2020, 16:54:02 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512204223-header.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
Math and data say early-stage VCs should [index invest](https://www.investopedia.com/terms/i/index-investing.asp), and late-stage investors should [stock pick](https://www.investopedia.com/terms/s/stockpick.asp).

_Yet they do the opposite._

The reason is counterintuitive: VCs are picky, not because they have so many options but **because they have so few**.

## More deals, better performance

Theoretical modeling and empirical data both suggest that early-stage VCs do better when they spread the wealth wider.

Let's start with the theory.

First, we need to define the [power law distribution](http://reactionwheel.net/2015/06/power-laws-in-venture.html):

![640px-Long_tail.svg](/content/images/2020/07/640px-Long_tail.svg.png)
*Source: Wikipedia*

A power law distribution is one where large, consequential events (the Googles, Amazons, and Facebooks of the venture world) are rare but much more common than you might expect if the world were [normally distributed](https://en.wikipedia.org/wiki/Normal_distribution).

![https://i0.wp.com/reactionwheel.net/wp-content/uploads/2015/06/normal-v-pld-tail.png?resize=606%2C287](/content/images/2020/07/normal-v-pld-tail.png)
*Source: Reaction Wheel*

These events happen with some frequency, and that frequency can be characterized by a "shape" or "tail parameter" that governs the "fatness" of the tail, typically represented by the symbol \\(\\alpha\\).

The smaller \\(\\alpha\\), the "fatter" the tail of the power law distribution. The fatter the tail, the higher the frequency and size of outlier startups.

![https://i2.wp.com/reactionwheel.net/wp-content/uploads/2015/06/pld-tails.png?resize=611%2C287](/content/images/2020/07/pld-tails.png)
*Source: Reaction Wheel*

If \\(\\alpha\\) is sufficiently small, strange things begin to happen. With a small enough \\(\\alpha\\), **making investments nearly at random will increase a portfolio's expected return**.

The exact mathematical reasons for this are [somewhat esoteric](https://arxiv.org/abs/2001.10488), but intuitively, if there are enough potential Googles, Amazons, and Facebooks out there, they will more than cover the losses from the duds thanks to the shape of the power law distribution. When fat tails dominate returns it's not worth having an extremely high bar if it might cause you to miss out on one of these future juggernauts.

VCs often talk about power laws, but few have sufficient data to rigorously demonstrate them. However, [AngelList](https://angel.co/) does, and their head data scientist, Abe Othman, has [done the work](https://angel.co/blog/venture-returns) to analyze returns from the thousands of deals syndicated by AngelList, finding that:

> ... **the regret an investor could have for missing a winning seed-stage investment is theoretically infinite**, a phenomenon that does not appear to hold for later-stage investments. The implication is that **investors increase their expected return by indexing as broadly as possible at the seed stage** (i.e., by putting money into every credible deal), because **any selective policy for seed-stage investing—absent perfect foresight—will eventually be outperformed by an indexing approach**.

By creating power law-based mathematical model, fitting it to the AngelList returns data, and simulating 50,000 portfolios with 10 early-stage companies each, he created the following synthetic distribution of venture capital returns:

![Distribution of hypothetical manager returns showing the market return outperforms](/content/images/2020/07/chart_2.png)
*Source: AngelList*

The vertical black line represents the _market return_: the return you'd earn if you invested an equal amount in every AngelList deal in the sample (of which there were 1,808). Notice the long tail trailing off to the right side of the chart. In a world where most barely return their fund, some shower their LPs with 5x returns. **Notice how most 10-company portfolios underperform the simple (1,808-company) strategy.**

Technically, this is still a theoretical result. Yes, the model was fit to real data, but the outputs are still simulated. Now let's look at real data.

Here again we turn to AngelList, who analyze the relationship between portfolio size and performance among 10,000+ investors on the platform. They grouped the investors by number of companies in their portfolio and plotted the median return for each group. The chart confirms the theoretical results - larger portfolios generate greater returns:

![img](/content/images/2020/07/H5qNMNZPFO.png)
*Source: AngelList*

> The coefficient of the regression term is 9.0 [basis points](https://www.investopedia.com/terms/b/basispoint.asp)... implying that **the typical annual return of a portfolio of 100 investments is almost 9% higher than the typical annual return of a portfolio with a single investment**.

Note these are median returns, so they don't reflect a handful of 100-count portfolios simply getting lucky and pulling up the average. **Financial upside is easier to come by when you're exposed to a larger patch of the startup landscape.**

Theory and data agree - most early-stage venture investors would do better by indexing, investing a small amount in every reasonable startup they can find:

> Simulations on 10-year investing windows for seed-stage deals suggest **fewer than 10% of investors will beat the index**, even if those investors have skill in picking deals. Like Vanguard has taught us in the public markets, **individual investors could benefit from viewing the index as the default** and then overlaying individual deals that they like.

## 1/N

Behavioral finance theorists have a name for this strategy: [the 1/N heuristic](https://www.macroresilience.com/2010/07/08/heuristics-and-robustness-in-asset-allocation/).

It goes like this: take every investable asset of a group of N assets and invest 1/Nth of your capital in each one, with no regard to the fundamentals, mean return, or volatility of returns.

This might sound like a dumb, simplistic strategy. While it's certainly simple, it's definitely not dumb.

Ironically, 1990 Nobel Laureate and famed inventor of mean-variance portfolio optimization, Harry Markowitz, reportedly used this exact heuristic for investing his own money, eschewing his own complicated theory in favor of a simpler approach:

> I should have computed the historical covariance of the asset classes and drawn an efficient frontier…I split my contributions 50/50 between bonds and equities. - [Harry Markowitz](https://alphaarchitect.com/2014/10/17/harry-markowitz-an-equal-weight-investor/)

![image-20200624134947087](/content/images/2020/07/image-20200624134947087.png)
*Source: Your Money and Your Brain*

In fact, [one study](http://faculty.london.edu/avmiguel/DeMiguel-Garlappi-Uppal-RFS.pdf) found that the 1/N strategy dominates numerous others:

> Of the 14 models we evaluate across seven empirical datasets, **none is consistently better than the 1/N rule** in terms of Sharpe ratio, certainty-equivalent return, or turnover, which indicates that, out of sample, **the gain from optimal diversification is more than offset by estimation error**...  That is, the effect of estimation error is so large that **it erodes completely the gains from optimal diversification**.

Which is a fancy way of saying "_don't overcomplicate it._"

A _slightly_ more sophisticated reading of the results might be, "_unless you are **very** sure about the structure of the world, don't overcomplicate your investing strategy._"

The study found a negative return to overthinking and overfitting yourself to the past data, to what you _think_ you know about these assets. In venture, similar overintellectualization has been the root cause of numerous "misses" over the years - career, fund, and firm-defining deals that put certain VCs on the map and left others in desperate obscurity.

In some _subtle_ way, trying to pick the exact right startup based on a complex model of the world is like trying to buy a certain lottery ticket with a certain serial number because you think it's a "lucky ticket." One _seems_ much more superstitious, but are they closer than they appear?

## How the other half lives

Late stage investors on the other hand, _can_ afford to stock pick:

> ... our results also suggest that the opportunity cost for missing a winning investment in these later rounds is bounded... Consequently, **it is entirely appropriate that later-stage investors should reject the “spray and pray” idea and be thoughtful and discerning when they participate**...

Translation: late-stage deals have lower opportunity cost than early-stage investments, as their returns are capped much lower. To channel Jeff Bezos' [regret minimization framework](https://www.youtube.com/watch?v=jwG_qR6XmDQ): there isn't much regret to minimize when it comes to late-stage deals. So feel free to be picky.

Another take from the 1/N study:

> ... we conclude that portfolio strategies from the optimizing models are expected to outperform the 1/N benchmark if: **(i) the estimation window is long**; (ii) the ex ante (true) Sharpe ratio of the mean-variance efficient portfolio is substantially higher than that of the 1/N portfolio; and **(iii) the number of assets is small**.

In other words, it makes sense to be picky and particular when you have a large amount of data collected over a long period of time and when the number of choices is small. Both are more true of later-stage investments relative to early-stage. By definition, late-stage companies have a longer track record by which to evaluate them, and there are fewer of them. Thus, it makes sense to be a discerning capital allocator at the late-stage.

And yet we've seen the exact _opposite_ in recent years.

Though few would publicly characterize themselves as such, I know of at least a few firms who have explicit goals of creating, effectively, "indices of late-stage venture" via their large portfolios. Late-stage funds are raising and deploying capital at unprecedented rates. Many appear principally focused on putting "dollars to work," rather than earning the maximum return on their capital. They've taken to heart the approach discussed earlier, investing in every company that meets some minimum threshold.

This has led to serious perversion at the late-stage, as there are not nearly enough investable assets to support the capital inflows at reasonable valuations. **Asset values grow not simply due to improvement in fundamental startup value but also because late-stage funds need somewhere to park their money**.

In this model, late-stage startups become "_capital vehicles_," absorbing meaningful capital and earning a "_capital storage_" premium for their efforts.

[Party rounds](https://blog.samaltman.com/party-rounds), long known to greatly [inflate early-stage valuations](http://blog.eladgil.com/2010/09/party-rounds-how-to-get-high-valuation.html), have reached the late-stage.

But all parties must come to end eventually...

## _Caveat emptor indexor_

So that's the theory, but there are some real practical hurdles to creating an early-stage index:

-   Determining the right minimum threshold
-   No one sees every deal
-   Deals must be won
-   Fund and check size restrictions

### How high should the bar be?

AngelList recommends investing in every _credible_ early-stage deal, where the word "credible" does a lot of work. They add:

> We worry that **an investor promising to blindly fund every whisper of a new company would fundamentally alter the investment universe they are exposed to by introducing a huge number of new money losing investments** that otherwise would not have been created but for the investor’s universal funding policy. Consequently, our results suggest that at the seed stage investors should put money into every investment that **clears some minimum threshold**.

But how to set that threshold? It's not obvious.

There must be some bar, even if very low, otherwise you'll do a series of bad deals and get fleeced.

[Peter Thiel](https://blakemasters.com/post/21869934240/peter-thiels-cs183-startup-class-7-notes-essay) puts it this way:

> There just aren’t that many businesses that you can have the requisite high degree of conviction about. A better model is to invest in maybe 7 or 8 promising companies from which you think you can get a 10x return.

Part of the reason indexing even works in say, public markets, is that there is some minimum threshold a company has to reach to even go public. Additionally, the AngelList data from which this theory comes only includes deals that got done, by definition. We don't know if the indexing strategy carries over to deals that wouldn't have otherwise got done or to companies with lower quality than the typical AngelList deal.

Since the AngelList data comes from deals that successfully completed, you could proxy for this by setting the bar such that you would do any deal that some other reasonable person would do.

### You must see the deal to do the deal

You can't hit a target you can't see, and [you can't do a deal you can't see either](https://nnamdi.net/people-matching/).

The AngelList recommendation assumes the investor could invest in any deal they want. If you don't have access to all the deals, this logic breaks down.

Unless you are a top tier fund or individual investor, the specific subset of deals you see is almost certainly poor relative to the full distribution. You are likely missing some serious outliers.

This is why so many VCs spend so much time burnishing their public and private reputations. [VC Twitter says hi.](https://marginalrevolution.com/marginalrevolution/2020/02/why-is-vc-twitter-so-peculiar.html)

### You must also win

Let's say you set the right threshold and see all the deals that meet it. You still then must [win the deal](https://nnamdi.net/people-matching/), which is far from guaranteed.

Further, the greater extent to which a deal exceeds any given threshold, the more competitive the deal will be. Assuming there's signal there and not simply noise, your portfolio will probabilistically skew lower quality simply due to losing out on those winners, even though you recognized them as such.

This completely screws up the indexing strategy, which implicitly relies on free and open access to all deals across the quality spectrum.

### A fund cannot be sliced into a thousand pieces

Even if the above caveats don't apply, a VC still runs up against fundamental limits of the venture universe.

Like the [Planck length](https://en.wikipedia.org/wiki/Planck_length) represents the smallest distance at which conventional physics applies, micro-checks become infeasible below a certain size, both for the startups themselves and the would-be index investor.

The complement of this is that, assuming some minimum check size, you might not be able to raise a fund large enough to write all those checks.

## Masters of the universe?

VCs are picky, not because they have so many options but **because they have so few**.

The 1/N study tells us that stock picking makes sense when there are few options to choose from.

The AngelList data tells us that, in a hypothetical world of perfect access to all investments, you'd do better to spread your bets widely.

In other words, the picky behavior of early-stage venture capitalists in light of the data and theory around power laws can only be justified by their _lack_ of options. They only have a few shots on goal and must use them wisely.

People often assume this pickiness comes from the fact that most startups fail, and VCs obviously want to avoid the numerous duds. **But this isn't really true.** Even in a world where most startups fail it can make sense to invest in many more than the typical investor does currently, as shown above.

Far from masters of the universe, selectively picking among desperate founders looking for funding, **VCs are themselves starved of options**, making them extremely cautious with the few they have.

In a future article, I'll extend this idea and demonstrate why it shows venture capitalists aren't funding nearly enough startups. ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[The Value of College May Be Negative for the COVID Generation]]>
                        </title>
                        <description>
                            <![CDATA[You might think the value of a college degree has been increasing. You would be wrong.]]>
                        </description>
                        <link>https://whoisnnamdi.com/the-value-of-college/</link>
                        <guid isPermaLink="false">the-value-of-college</guid>
                        <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Wed, 17 Jun 2020, 17:26:43 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512202251-black-college-wealth.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
The value of a college degree is crashing toward zero and may even be negative for today's COVID-graduates.

This is the stunning implication of [recent research](https://research.stlouisfed.org/publications/review/2019/10/15/is-college-still-worth-it-the-new-calculus-of-falling-returns) by the Federal Reserve into the precipitously falling financial returns of higher education.

You might be thinking — “_but I thought the benefit of a college degree has been increasing all this time?_”

You would be wrong, though understandably so.

Here’s a sketch of the basic argument:

1.  College graduates continue to earn significantly more than non-college graduates
2.  However, the _causal_ (key word) income effect of college has **not** increased and, if anything, has declined
3.  Meanwhile, the cost of attending college has ballooned
4.  Therefore, the wealth-generating power of a college degree has evaporated

Let’s walk through these points, one by one.

## College graduates earn more

**Point one** represents the common (and, it should be said, _true_) belief that college graduates earn more than non-graduates. Another way of saying this is college education is _correlated_ with higher income. We can compare the simple average (or median) earnings of college degree holders and non-holders. Lo and behold, one is higher than the other, therefore, correlation.

That’s really all there is to this point, so let’s get it out of the way first. I’ll drop this chart in here for good measure, which makes the point visually:

![](/https://nnamdi.net/content/images/2020/06/CleanShot-2020-05-23-at-01.21.24@2x.png)

In words — the median family with a bachelor degree-holding head of household earns ~100% more income than the median family without a college degree.

Unfortunately, this is where most mainstream analysis of higher education ends.

Let’s go further.

### Receive my next long-form post

Thoughtful analysis of the business and economics of tech

 

Subscribe

## A degree is only one of many factors

**Point two** is where things get interesting. As I’ve discussed in [prior analyses](https://nnamdi.net/remote-software-developers-earn-more/), _correlation is not causation_. But that’s not very interesting. What’s interesting is exactly how much correlation is in fact causation vs. not. It turns out, college degrees do raise incomes but not as much as simple correlations like above would suggest.

I’ll spend some time on this point, as it’s the most important one to grasp.

To quote the authors of this same study:

> The choice to attend and subsequently complete college **is not random or arbitrary**; it is instead related to numerous financial and non-financial considerations, among them parents’ wealth, intelligence, socio-cognitive skills, race, financial acumen, and parents’ education… Although terminal college graduates and postgraduates enjoy significant income and wealth advantages over non-grads, **attributing these premiums solely to the effect of college would be a mistake**. (Emphasis mine)

In general, people who choose to attend college have led different lives than those who don’t and will experience different lives after the fact, even if they make the same choice at this juncture. These pre-existing differences interact with the effect of college itself, leading to different outcomes.

College is not the “[great leveler](https://opinionator.blogs.nytimes.com/2014/03/01/college-the-great-unleveler/)” it is often advertised as. Different inputs do not lead to the same output. [GIGO](https://en.wikipedia.org/wiki/Garbage_in,_garbage_out), meet DIDO: different input, different output.

What are some examples of these different inputs? One is parent’s education and financial acumen. Individuals with college educated and financially savvy parents do better than those without and are also more likely to attend in the first place. This confounds any simple analysis of the benefit of a college education, as some of that benefit comes not from college itself but from one’s parents.

The authors of the previously cited analysis explore this idea and conclude:

> Failing to account for parents’ education **overinflated** the college and post-graduate income and wealth premiums… Clearly, **parents’ education and financial acumen** were important variables previously omitted in estimations of the college and post-graduate premiums… Part of the effect of college would be **transmitting the effect of parents’ education**. (Emphasis mine)

Said more colorfully, rather than simply being a “leveler,” college is increasingly the vehicle through which the _already advantaged_ **express** and **[compound](https://nnamdi.net/you-dont-understand-compound-growth/)** their advantages. Mathematically, this accounts for some of the correlation I identified earlier, leaving less to be explained by true “causality” or, as the authors put it, “the true college premium.”

So how do we identify this “true” premium? There are multiple methods, but the most straightforward is to run a large regression of income on college attendance, controlling for all the appropriate variables that may correlate with college attendance or income. This is harder than it sounds, both because the choice of controls is subjective and because those variables might not be available to you.

So the author’s take a simpler approach, essentially controlling only for age. Surprisingly, that alone is enough to meaningfully reduce the supposed income benefit of college from 100% to only 50% for whites born in the 1980s:

![](/https://nnamdi.net/content/images/2020/06/CleanShot-2020-05-23-at-01.22.20@2x.png)

Two things to note here.

First, notice that, contrary to popular belief, the premium has _not_ grown over time and has in fact slightly decreased. It is _not_ the case that college is increasingly important from an income generation standpoint.

Second, while 50% is still large number, remember that this _only controlled for age_. There are numerous other variables that aren’t being accounted for, like parental education or income, which we know affect future earnings. Adding these in would very likely reduce the income premium even further, but these data are hard to come by.

As the authors put it:

> … **these figures may exaggerate the true—that is, causal—college premium.** Other variables, chief among them one’s parents’ education, may play a role in potential earnings and wealth accumulation… **Those premiums may be upwardly biased estimates of the true income and wealth premiums.** (Emphasis mine)

## College is only getting more expensive

**Point three** like point one, is relatively uncontroversial. The real cost of college in the United States has increased meaningfully over time, as tuition growth continues to outpace overall inflation.

College is increasingly “priced to perfection.”

Again, in case there are any doubts, here’s some data showing the growth in the cost of college vs. broader prices in the economy since 1978. You don’t need a legend to know which line is which:

![](/https://nnamdi.net/content/images/2020/06/CleanShot-2020-05-23-at-01.41.03@2x.png)

These surging costs are fueling an explosion of consumer debt. We can get some sense of this from the following table, which shows the median debt-to-income ratios among college degree holders across multiple birth decades at various years of age:

![](/https://nnamdi.net/content/images/2020/06/CleanShot-2020-05-25-at-02.04.33@2x.png)

So for example, the median American born in the 1960s owed debt equal to **34%** of their annual income at the age of 26, while Americans born in the 1980s owed debt of **109%** of income, **a tripling of the typical debt burden for young adults**. While much of this relates to the broader availability of mortgage debt, at the age of 26, one has to assume that much of it comes college borrowings yet to be paid off. In fact, the authors note that “debt ratios generally are higher among college grads than non-grads.”

That debt levies a pernicious tax on the debt holder in the form of regular interest payments and often leaves them with negative net worth until the debt can be sufficiently paid down.

### Receive my next long-form post

Thoughtful analysis of the business and economics of tech

 

Subscribe

## The grand finale

Add all that up and you arrive at the sobering conclusion that, **college may no longer be beneficial from the standpoint of individual wealth generation**. This doesn’t mean that it doesn’t have all sorts of _other_ value (such as non-monetary or “social” value), but on this important dimension the returns seem to have fallen to zero or potentially even lower.

Unconvinced? Here is possibly the most important chart in higher education, showing the expected impact of a bachelor’s degree on the net worth of graduates born across various decades, again controlling for age:

![](/https://nnamdi.net/content/images/2020/06/DraggedImage.png)

![](/https://nnamdi.net/content/images/2020/06/DraggedImage-1.png)

What does this say? A white college graduate born in the 1930s could have expected to achieve **247%** higher net worth than their non-college educated peers, while one born in the 1980s can only expect to own **42%** more.

For black grads the story is even worse.

While the 1930s cohort (an extremely small group it should be noted) could be expected to own 509% more wealth compared to similarly aged blacks without a degree, those born in the 1980s only achieve 6% more wealth as a result of their degree, an effect “statistically indistinguishable from zero,” as the researchers put it.

This is a **big** deal.

The collapse in the financial value of a college degree can be traced directly to the explosive growth in the cost of attending college, financed by ever-growing debt burdens.

Notice that the chart ends at the 1980s birth cohort. If these trends have continued, recent graduates born in the 90s may do even worse relative to non-graduates, reflecting further erosion in the wealth effect of a college education.

If you’ve read this far, you should be disturbed and alarmed.

Why aren’t these sobering facts receiving more attention? It’s a good question —and I don’t have an answer — but I hope to at least point the public in the right direction by shining a light on the growing problem of ineffectual “degreeism.”

But there might be a counterintuitive “silver lining” to this situation.

## COVID College Online

College graduates do better in recessions than non-graduates. Their incomes do not fall as significantly, nor do their unemployment rates surge to the same extent, as we can see in [data from the Great Recession](https://www.pewtrusts.org/~/media/legacy/uploadedfiles/pcs_assets/2013/pewcollegegradsrecessionreportpdf.pdf):

![](/https://nnamdi.net/content/images/2020/06/CleanShot-2020-05-29-at-23.47.16@2x.png)

![](/https://nnamdi.net/content/images/2020/06/CleanShot-2020-05-23-at-01.16.34@2x.png)

In this sense, a college degree serves as downside protection or insurance against the worst outcomes. Thus, in tough times the relative value of a college degree increases. Given the way COVID disproportionately impacts low-wage workers, this is even truer right now.

Many of this year’s graduates are rightfully disappointed about their curtailed college experiences and graduating into a major recession. It’s important to remember though that, ironically, degrees are in fact _more_ valuable during recessions.

## There’s no such thing as a free lunch

College is not a free lunch. It’s the furthest thing from free, and its impact on individual wealth generation is dubious.

College is no longer a “sure thing.” We’re paying more for it and getting less. How much of this shift is being driven by colleges themselves vs. changes in the underlying economy is unclear, so we shouldn’t point fingers just yet.

What’s clear is that the calculus of college must change.

_Many kudos to [William Emmons](https://www.stlouisfed.org/household-financial-stability/staff-profiles/william-r-emmons/bio), [Ana Kent](https://www.stlouisfed.org/household-financial-stability/staff-profiles/ana-hernandez-kent/bio), and [Lowell Ricketts](https://www.stlouisfed.org/household-financial-stability/staff-profiles/lowell-r-ricketts/bio), the authors of [the study](https://research.stlouisfed.org/publications/review/2019/10/15/is-college-still-worth-it-the-new-calculus-of-falling-returns) from which much of this post is derived._ ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Do College Degrees Matter for Software Engineers? Maybe]]>
                        </title>
                        <description>
                            <![CDATA[Do college-educated developers earn more? Yes, but less than you might think]]>
                        </description>
                        <link>https://whoisnnamdi.com/college-degrees-software-engineers/</link>
                        <guid isPermaLink="false">college-degrees-software-engineers</guid>
                        <category>
                                        <![CDATA[ Developers ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Wed, 20 May 2020, 15:39:14 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512200903-EdLevel_header-3.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
_This post is an excerpt from my [2020 software developer pay analysis](https://nnamdi.net/highest-paid-software-engineers-2020/). Read the rest of the analysis [here](https://nnamdi.net/highest-paid-software-engineers-2020/)._

Coding bootcamps say [the traditional model of education is broken](https://lambdaschool.com/about) and that anyone can become a software developer with the proper effort.

Further, they claim that traditional institutions aren’t incentivized to ensure their graduates get hired post-school.

Regardless of your views, these claims ignore an important question: let’s say you get the job — **will you earn less as a software developer without a college degree**?

## Yes, but it’s not as bad as you might think

![](/content/images/2020/05/EdLevel-1.png)

Developers with college degrees earn **7.5%** more than developers with no college education.

As in [previous analyses](https://nnamdi.net/age-race-gender-software-engineering-pay/), I additionally control for various observable factors like `age`, `experience`, `hours worked`, `size of employer`, `programming languages` [and more](https://nnamdi.net/highest-paid-software-engineers-2020/). Interestingly, accounting for these factors doesn’t change the college wage premium.

Advanced degrees carry greater benefits — master’s degree holders earn **13.3%** more than college degree holders, while those with doctoral degrees (i.e. PhDs) earn **20.5%** more. These numbers decrease meaningfully though once I add in the controls I mentioned above, falling to **4.1%** and **7.8%** respectively.

### Receive a report with the full results

 

Send Report ⚡

## Is this a big deal?

Now, a 7.5% college degree advantage is nothing to scoff at.

But 7.5% is _tiny_ relative to the college wage premium earned by the public at large.

A review of the evidence around higher education shows that the college wage premium is much higher for the overall population. In [my favorite study](https://research.stlouisfed.org/publications/review/2019/10/15/is-college-still-worth-it-the-new-calculus-of-falling-returns) on this topic, researchers at the Federal Reserve Bank of St. Louis show that, controlling for age and level of degree (college vs. graduate), college graduates earn **~50%** more than non-graduates (the data is similar for other ethnicities):

![](/content/images/2020/05/DraggedImage.png)

So the college degree wage premium for software developers is much smaller than the broader population benefit. This is not that surprising — income variation within careers is typically much narrower than across careers.

## Getting the gig

Of course, this all assumes you get the job in the first place.

It doesn’t matter how much a college degree impacts one’s earnings as a software developer if not having one effectively locks you out of the career.

It’s hard to judge how true this is. If you believe the hype around [Lambda School](https://lambdaschool.com/) and other bootcamp programs, a college degree is not necessarily a barrier to becoming a software developer. Realistically, few people without a college degree bother to pursue software engineering, but that doesn’t mean they couldn’t if they put their mind to it.

![](/content/images/2020/05/DraggedImage-1.png)

If bootcamps and other alternative forms of education can successfully train and place students in legitimate software engineering roles, then a college degree might not be required.

I think the jury is still out. [Recent media reports](https://www.theinformation.com/articles/lambda-schools-growing-pains-big-buzz-student-complaints) have not been kind to coding bootcamps like Lambda. Then again, anecdotes may not represent the full story. For now, only the schools themselves have the data to prove their own efficacy.

That said, we can finally put some numbers behind the claim that college degrees don’t matter for software engineers. They matter, but not much, assuming you can get hired as a developer. Once in the seat, your degree matters much less than other factors, like on the job performance, competence, etc. ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Pandemiconomics: Viral Volatility]]>
                        </title>
                        <description>
                            <![CDATA[How the virus rocked stocks]]>
                        </description>
                        <link>https://whoisnnamdi.com/viral-volatility/</link>
                        <guid isPermaLink="false">viral-volatility</guid>
                        <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Wed, 15 Apr 2020, 21:22:57 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512198504-header.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
The sky fell. Then it bounced off the ground and rose a bit. Then it fell some more, and then it rose again. Now we’re not really sure what it’s doing.

Stock analysts and market watchers are understandably baffled and bewitched by the multi-month corona-coaster:

![](/content/images/2020/04/uCUJlflCUA.png)

**The volatility generated by the coronavirus pandemic beats out even the Great Depression and the global financial crisis of 2008.** Never before has an infectious disease generated such volatility in the markets. This includes every previous pandemic on record, which typically have only tiny effects on the U.S. stock market. Even the Spanish Flu of 1919-1920, which killed 2% of the world's population, did not generate such market movement.

## The views of the news

We can quantify this differential impact by counting how often journalists attributed market volatility to ongoing pandemics or the accompanied policy responses. [Researchers did exactly this](https://www.nber.org/papers/w26945), and what they found was striking:

![](/content/images/2020/04/MIAEpsbozj.png)

Of the 1,116 times since 1900 the stock market moved more than 2.5% in a single day prior to the Coronavirus, not once was the jump attributed in any way to a pandemic. Not once.

In contrast, of the 18 days between February 24 and March 24, 2020 where the market moved more than 2.5%, journalists attributed **7 days** to the coronavirus and **8** to the federal government's response to the virus. In other words, **15 of the 18 episodes of high volatility were virus-related**. This suggests updated information about the coronavirus caused nearly all outsized market movements.

In the words of the researchers, **"no previous infectious disease episode led to daily stock market swings that even remotely resemble the response in the past month to the COVID-19 developments."**

Let’s stay on this theme of news coverage and look at this another way. Take the percentage of news articles talking about volatility, and subset those that mention an ongoing infectious disease pandemic. Here's what the same researchers found across multiple disease episodes:

![](/content/images/2020/04/LRy9A11nIY.png)

Coronavirus reigns supreme. Focus on the column “_(2) % of EMV Articles with Infectious Disease Terms_”, where you can see the ramp in volatility coverage mentioning the coronavirus. **91% of volatility-related news articles in March 2020 mentioned the coronavirus**. This compares to peak news share of 5% for the Bird Flu of 1997-1998, 8% for SARS, 4% for Swine Flu / H1N1, and 11% for Ebola and MERS.

### Receive my next long-form post

Thoughtful analysis of the business and economics of tech

 

Subscribe

## Can we trade on this?

Ok, so the virus clearly drives volatility.

Let's take the next logical step — **can we predict stock market movements using updated information on the virus?** Even further, **can we use predictions of the virus' spread to predict market movements?**

According to one of my old economics professors, we can. Sort of.

[Peter Schott of Yale and a team of fellow researchers](https://www.nber.org/papers/w26950) explore this idea with a simple exponential growth model of COVID-19. They trained the model on actual data, updating its forecasts for every additional daily data point, noting how the model's predictions change day-to-day. These updates represent the predictive value of the daily stream of new information about the spread of the virus.

They then plotted these prediction updates against the daily changes of the Wilshire 5000 index, which you can see below. They found a noisy but negative relationship between projection updates and the stock market:

![](/content/images/2020/04/c1Qu9XtY6N.png)

They go further and estimate linear regressions of this relationship, summarized below:

![](/content/images/2020/04/iN6ihzWBPg.png)

Focus on column **(2)**. The coefficient of -0.086 implies that a doubling of predicted cases leads to a 8.6% lower closing price for the Wilshire 5000 index on that same day. Scaling this down, a 10% increase in predicted COVID-19 cases yields a 0.86% drop in the market.

Per columns **(5)** and **(6)**, the relationship holds even after controlling for the government’s response to the virus, showing that we can separate the impact of the virus itself from the impact of our response to the virus.

Let's not get ahead of ourselves. Though the relationship is statistically significant, the R^2 of the regression is only 0.105, which simply means you can't predict the market very well using only an exponential viral growth model as your only input. The sample is also small, containing only 41 days of data. That said, there is clear informational value contained in these updated forecasts, which suggests they drive market movements.

## The upshot

**If epidemiological uncertainty drives stock volatility, volatility should taper as uncertainty declines.**

This is happening right now, in real-time.

Though the crisis is far from over, our forecasts are improving. This is good news in a time of mostly bad.

Meanwhile, a [paper out of the NY Fed](https://www.newyorkfed.org/medialibrary/media/research/staff_reports/sr920.pdf) this month shows the economic impact of the "sudden stop" in economic activity. This reflects not so much the virus itself but policy responses to the virus, which have been meaningful, if not always swift:

![](/content/images/2020/04/gWelVoIq73.png)

People often blame the severity of the Great Depression on [misguided policies of the Federal Reserve](https://www.hoover.org/research/feds-depression-and-birth-new-deal), but this is a man-made recession like nothing we've ever seen. ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Byron Deeter and Jason Lemkin on the State of VC and the Cloud]]>
                        </title>
                        <description>
                            <![CDATA[When to raise capital, what to expect from VCs, and the "new normal"]]>
                        </description>
                        <link>https://whoisnnamdi.com/deeter-lemkin-state-of-vc-and-cloud/</link>
                        <guid isPermaLink="false">deeter-lemkin-state-of-vc-and-cloud</guid>
                        <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Mon, 30 Mar 2020, 08:42:02 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512195620-cover.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
[Byron Deeter](https://www.bvp.com/team/byron-deeter/), Partner at [Bessemer Venture Partners](https://www.bvp.com/), and [Jason Lemkin](https://twitter.com/jasonlk) of [SaaStr](https://www.saastr.com/) hosted a live Zoom discussion last week on the current state of venture capital and cloud software in the wake of the coronavirus turmoil.

They covered:

-   What's happening in venture today
-   When founders should raise capital and what to expect
-   The "new normal" for VCs and founders
-   Changes in cloud software / SaaS

The call was so fantastic that I wanted to share my summarized notes and takeaways with you.

You can find:

-   the Zoom recording [here](http://go2.saastr.com/n0Rv0yf03jo30TS0Za001ta)
-   the slides [here](http://go2.saastr.com/Y01at0gov00jZ0033SR0zTa)
-   my notes below

Enjoy!

### Receive my next thought in your inbox

 

Go ⚡

* * *

## What’s happening in venture today

### Things are not “business as usual”, even for VCs who have capital to deploy

![](/content/images/2020/03/usual.png)

VCs are going through an internal triage of portfolio companies, figuring out where the most damage has been done and what they can do to support their companies.

The best firms have plenty of reserves and will stand by and support their companies, especially the shining stars. The portfolio companies that aren’t doing so well, who are tight on cash… those are going to be the difficult situations.

![](/content/images/2020/03/fund.png)

If a VC has just raised a fund, or it’s only their first fund, and they haven’t yet called the capital from Limited Partners, there _may_ be some risk there.

We may see LPs pull back on commitments they’ve made to certain funds in order to handle their commitments to other funds. As there are tiers of venture firms, there are tiers of LPs, and the **lower tier LPs are the ones more likely to pull out**.

With all the turmoil on both the portfolio and LP side, VCs are spending less time on new deals. Net new checks are definitely going to decline:

![](/content/images/2020/03/vc-survey.png)

> As a firm, we are trying to be careful with our time. Bessemer is taking no courtesy meetings right now. Anyone we meet with is someone we think could actually be an interesting investment — Byron Deeter

If a VC calls, **you should still take it**, but don’t have too many expectations. Many “false signals” (i.e. mild, but not serious interest) right now.

## When founders should raise capital and what to expect

### Go or no go?

Driving net new conversations is going to be hard right now. VCs are simply busy right now, and there’s not much you can do about it.

It’s very difficult to start a deal process right now unless it’s being lead by one of your existing investors or someone you’ve already met and gotten to know well pre-crisis.

If a VC ghosts on you, don’t take it personally—reach back out in a few weeks once things have settled down.

> If you already deep in negotiation and have most of the terms locked down, get the deal done immediately. Emphasize speed over greed — Bryon Deeter

For companies that were right about to kick off a financing and don’t have a big cash balance in reserve, that’s where the question of bridge financing comes up.

Given current conditions, bridge financings will be smaller in dollar terms than they typically are, but those dialogues are definitely happening.

> Founders should definitely ask their VCs for bridges, don’t be afraid to ask — Jason Lemkin

If you don’t already have discussions underway, don’t be afraid to hit pause. People will become more deal-minded in the coming months and will be looking to the broader market to inform pricing.

### Term sheets are a slippery concept

Some people may try to take advantage of the current situation and retract term sheets.

Don’t freak out—these things happen.

See it as more of a temporary dynamic based off the current market situation than any judgement of your startup or idea. It’s not a death knell.

On re-pricing:

> If an investor comes to you and wants to re-discuss pricing, I would encourage you to be open-minded to that. Try to close the current deal if you can, but if they want to reprice, that’s probably OK. Meeting somewhere in the middle is good, focus on getting the deal closed — Byron Deeter

### Alternative financing options

Consider venture debt as an option:

> If you’ve already locked down a venture debt line and think you might need it in the next 12 months, I would encourage you to draw it down right now — Byron Deeter

_Careful though_—in 2008 several debt providers withdrew venture lines they had extended to certain startups. In some cases, **this ended up taking down the company**, as they couldn’t survive without the debt financing.

This is less of risk with the tier one debt providers, but be careful with others.

### Receive my next thought in your inbox

 

Go ⚡

## The “new normal” for VCs and founders

### Not all is lost

![](/content/images/2020/03/raise.png)

The fundamentals of most startup businesses are _much_ better now than they used to be. Bookings will slow, churn will go up, but startup fundamentals are still much better than the 2000 or 2008 period where many companies went to zero.

Startups will get a little bit of a pass right now on financial performance from investors. Everyone understands that things are crazy right now. That free pass should last at least a few quarters.

The disruption may end up actually helping companies build a better foundation for the long-term by focusing on product and other critical areas and while deprioritizing go-to-market for the time being.

Even more reason to take a pause on go-to-market: some companies are reporting a spike in requests for demos right now, but **this is another false signal**. People are at home and bored, so they are signing up for demos on a whim. The conversion rate to actual deals will be _much_ lower than is typical.

Great entrepreneurship happens across all cycles, but your ability to scale will be moderated during this recessionary period.

If you have made a ton of progress since your last round, **up-rounds can still happen**. But if you have any worries, take what you can get as far as capital.

You have to accept that multiples before were simply fantastic. Now they may just be OK to good.

If you are in a structured program or accelerator like Y Combinator, don’t be surprised if you don’t find an investor as you exit the program. Typically, completing one of these programs without receiving funding is a bit of a black mark. The current situation is an exception to that.

### Batten down the hatches

That said, times are grim in more ways than one.

**Flat is the new up**—if revenue doesn’t decline you are doing something right.

Get your burn under control.

![](/content/images/2020/03/burn.png)

Every late-stage CEO should be striving to get to the point of infinite runway, i.e. **breakeven profitability**. Early-stage CEOs should be targeting **18-24 months of runway**. Careful though—18 months of runway is really 12 months of runway because it takes time to fundraise, especially in bad times.

**Sensitize all your projections.** If bookings falls off, burn goes up, and that’s really tough on your cash runway.

If you’re approaching an exit, know that the M&A market is likely heat up later this year. Many companies have been stockpiling cash, waiting for this moment. (The others are getting bailed out).

Companies that have been waiting to be aggressive will soon jump in. That should be true for transactions large and small.

Start building relationships and partnerships with potential strategic buyers _now_.

And if you were planning a public offering, be warned—the IPO market is _unlikely_ to recover in the same way.

## Changes in cloud software / SaaS

### SaaS valuations remain above long-term averages

![](/content/images/2020/03/contractions.png)

We have just seen one of the fastest 30% declines in market history. It’s almost unprecedented.

The good news is that it’s driven by an external shock that didn’t have to do with economic fundamentals. Therefore, optimistically we should see a solid rebound once things settle down, a vaccine is prepared, etc.

SaaS is fetching 8-10x multiples on revenue in the public markets, which is still much better than any other industry. Many of the well-known SaaS companies today grew up in an era where 8-10x revenue was the norm, and they did fine.

In hindsight these will seems like very reasonable numbers given what’s happening right now.

![](/content/images/2020/03/bvp.png)

Many people thought ’08-09 was horrible, that we were never going to recover from it, but we did:

> Citibank told me to take my startup’s cash and put it under a mattress. One of the biggest banks in the world had no confidence there would be an improvement in the market — Jason Lemkin

SaaS is particularly resilient relative to the overall economy:

![](/content/images/2020/03/SaaS-08-09.png)

### Some industries and verticals will be harder hit than others

![](/content/images/2020/03/cnbc.png)

While the macro picture for SaaS overall is far from dire, certain sub-sectors will see significant near-term pain.

There will be complete dislocations in certain industry verticals like travel. If your product serves SMBs, **be ready to take a hit**. Small businesses are seeing the worst disruption in the current environment.

**Recruiting solutions** will probably have a tough go of it for the next few quarters, as will **marketing tech**.

The more core your product is to your customer’s systems, the more resilient your revenue will probably be.

That said, prepare for some economic hit, no matter what industry you are in. Things can change very fast—companies that were reporting no impact last week are now seeing **30-40%** declines in new business.

**Get ahead of this**—figure out what your leading indicators are for potential problems. Poor customer health, will ultimately lead to churn, which will lead to revenue and business impact. ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Remote Software Developers Earn 22% More Than Non-Remote Developers]]>
                        </title>
                        <description>
                            <![CDATA[Working remote even just a few days per month leads to higher pay]]>
                        </description>
                        <link>https://whoisnnamdi.com/remote-software-developers-earn-more/</link>
                        <guid isPermaLink="false">remote-software-developers-earn-more</guid>
                        <category>
                                        <![CDATA[ Developers ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Mon, 23 Mar 2020, 16:07:39 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512193250-WorkRemote-5.jpg" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
_This post is an excerpt from my [2020 software developer pay analysis](https://nnamdi.net/highest-paid-software-engineers-2020/). Read the rest of the analysis [here](https://nnamdi.net/highest-paid-software-engineers-2020/)._

Organizations around the world have pivoted to remote work to shield themselves against the [novel coronavirus / COVID-19](https://www.cdc.gov/coronavirus/2019-ncov/index.html) threat.

The sudden change prompts numerous questions. To start, how will this new (for some [but not all](https://about.gitlab.com/company/culture/all-remote/)) paradigm affect the way we work and collaborate?

One piece of good news among all the dread—remote work could lead to higher take-home pay, at least for software engineers.

## Remote work pays

[![Remote Work](https://plot.ly/~whoisnnamdi/82.png?share_key=AeS6BZEpGR5FXWfIFa29gc)](https://plot.ly/~whoisnnamdi/82/?share_key=A4rtkRSqRGar7d19Zq3Xf6 "Remote Work")

Fully-remote software developers earn **21.9%** more than developers who never or rarely work remote.

Even once I [control](https://en.wikipedia.org/wiki/Controlling_for_a_variable) for various observable factors (including `age`, `experience`, `hours worked`, `size of employer`, `programming languages`, and [more](https://nnamdi.net/highest-paid-software-engineers-2020/)), fully-remote software developers earn **9.4%** more than developers who never or only rarely work remotely.

Further, the remote work pay advantage increases proportionately with the time spent working remotely. Therefore, fully-remote developers earn more than

-   those who work remotely more than half the time, who earn more than
-   those who work remotely roughly half the time, who earn more than
-   those who work remotely less than half the time.

Even working only a few days each month yields a substantial pay advantage of **15.5%** on average and **4.9%** controlling for observable factors, and the estimates are quite precise.

In other words, **half or more of the benefit comes from being able to work remotely at least a small portion of the time** (vs. not at all), while the remainder comes from increased remote time, maxing out at fully-remote.

This is an impressive result.

While I can’t be sure this is [causal](https://towardsdatascience.com/causal-vs-statistical-inference-3f2c3e617220), the **large apparent effects** (even after controlling for other variables) paired with the **clear upward trend** as remote work increases gives me confidence there is something real here.

### Receive a report with the full results

 

Send Report ⚡

## Wait but why?

The obvious next question is… _why_?

Hard to say—by definition the adjusted pay premium already controls for any other data I had access to.

However, I can show how the other factors contribute to the explainable premium (i.e. the gap between the unadjusted and adjusted pay premium):

[![Explaining Remote Work](https://plot.ly/~whoisnnamdi/84.png?share_key=AeS6BZEpGR5FXWfIFa29gc)](https://plot.ly/~whoisnnamdi/84/?share_key=A4rtkRSqRGar7d19Zq3Xf6 "Explaining  Remote Work")

Here we can see some of the other factors contributing to the pay advantage for fully-remote developers:

-   `years of professional coding experience`,
-   `age`,
-   and a variable that proxies for the `influence` a developer has within their organization (their decision making power over new technology purchases)

Combined with other [confounding variables](https://www.theanalysisfactor.com/what-is-a-confounding-variable/), these account for **12.5 percentage points** of the 21.9 percentage point pay advantage.

Much of the apparent premium earned by remote developers is in fact driven by seniority and tenure. These are older, more experienced developers who either prefer to work remote or whose organizations grant them that privilege.

[![Years of Professional Coding Experience vs. Remote Work](https://plot.ly/~whoisnnamdi/80.png?share_key=AeS6BZEpGR5FXWfIFa29gc)](https://plot.ly/~whoisnnamdi/80/?share_key=A4rtkRSqRGar7d19Zq3Xf6 "Years of Professional Coding Experience vs. Remote Work")

That said, the remaining **9.4%** adjusted pay advantage for fully-remote developers is nothing to scoff at. The other variables cannot explain this meaningful earnings bump.

## This one weird trick

I find an economically and statistically significant pay premium for remote developers, even those who are only away from the office a few days each month. In fact, _most of the earnings gains_ come from this initial foray into remote work, while the rest come with additional hours spent away from the office.

Would this hold in a world where remote work is more the norm than the exception? No idea.

Does this apply to non-software developers? Again, can’t say.

But it’s an interesting result nonetheless, and one that deserves further investigation as more knowledge workers shift their work online and away from the corporate office—[virus](https://www.cdc.gov/coronavirus/2019-ncov/index.html) or no virus. ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[How Age, Race, and Gender Affect Software Engineering Pay]]>
                        </title>
                        <description>
                            <![CDATA[Progress on narrowing pay gaps among software developers]]>
                        </description>
                        <link>https://whoisnnamdi.com/age-race-gender-software-engineering-pay/</link>
                        <guid isPermaLink="false">age-race-gender-software-engineering-pay</guid>
                        <category>
                                        <![CDATA[ Developers ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Mon, 24 Feb 2020, 18:11:43 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512190908-Ethnicity-10.jpg" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
_This is part 1 of my 2020 analysis of software developer earnings. You can find the rest of the results and methodology [here](https://nnamdi.net/highest-paid-software-engineers-2020)._

Age. Race. Gender. Sexual orientation.

In an ideal world, none of these factors would matter for what a software engineer earns. As characteristics, they shouldn’t necessarily influence the productivity or value of a developer, and hence shouldn’t affect pay.

Turns out, however, they do — though not always in the way or to the degree you might expect.

In this first part of my 2020 analysis of software engineering pay, I explore how these demographic characteristics match with developer pay, tease apart correlation from causation, and explain the confounding factors driving some of the more surprising results.

**Key findings:**

-   Earnings peak around 45-50 years of `age`
-   `Racial minorities` make up both the highest and lowest-paid developers
-   `Female` software engineers earn ~10% lower salaries than their male counterparts, but `professional experience` explains most of the gap
-   `Gay and lesbian` engineers earn more than `straight` engineers after adjusting for observable factors
-   `Parents` earn significantly more that `non-parents`, but this is explained by other factors

If you haven’t already, please check out the [methodology behind the analysis](https://nnamdi.net/highest-paid-software-engineers-2020), otherwise the numbers below might be difficult to interpret.

### Receive a report with the full results

 

Send Report ⚡

## Age

[![Age](https://plot.ly/~whoisnnamdi/49.png?share_key=AeS6BZEpGR5FXWfIFa29gc)](https://plot.ly/~whoisnnamdi/49/?share_key=A4rtkRSqRGar7d19Zq3Xf6 "Age")

### Earnings peak in the late 40s

Developer earnings rise fairly steadily from the early 20s through the late 40s. **The late 40s represent the highest earnings of a developer’s life**, where the average developer earns **28.7%** more than the typical 26-30 year old (the most common age range in the data), after which pay stabilizes before finally beginning to decline in the early 60s.

### Age doesn’t matter much after 35

However, adjusting for other characteristics, `age` is most “advantageous” in the 31-35 range, when a developer can expect to earn **4.7%** more than an equivalent developer five years their junior. This advantage is highly statistically significant.

The pay bump quickly dissipates with additional years however, losing statistical significance and even turning somewhat negative by the 51 to 55 range, though this is not precisely estimated.

The key point is that the additional income earned by older developers is entirely explained by factors unrelated to age. When we control for other factors, **pay does not vary much by `age` after 35.**

### Little causal impact of age

Why does the correlation between `age` and income disappear after controlling for other factors? Let’s dig in:

[![45_50](https://plot.ly/~whoisnnamdi/63.png?share_key=zIzwrGKK43zEpTMXUKUt9A)](https://plot.ly/~whoisnnamdi/63/?share_key=zIzwrGKK43zEpTMXUKUt9A "45_50")

The analytics demonstrate that `years of professional coding experience` matters much more than age itself, which is what you’d hope to see. These additional `years of professional experience` effectively explain the entire earnings premium for older software engineers.

Additionally important variables include `self-rated competence`, having `influence over technology purchases` in their organization, and `working remotely` (which older workers are more likely to do), and having dependents.

## Race

[![Ethnicity](https://plot.ly/~whoisnnamdi/42.png?share_key=AeS6BZEpGR5FXWfIFa29gc)](https://plot.ly/~whoisnnamdi/42/?share_key=AeS6BZEpGR5FXWfIFa29gc "Ethnicity")

### Minorities are both the highest and lowest-paid software developers

As in [last year’s analysis](https://nnamdi.net/highest-paid-software-developer/), the largest pay gaps are between minority groups, which make up both the highest and lowest-paid developers.

`East` and `South Asians` see the most statistically significant pay premiums relative to white developers with or without controls in the regression. In the case of `East Asians`, their pay premium _increases_ after controlling for various factors.

These premiums are large and meaningful — `East Asians` earn **7.4%** more than white developers and **13.9%** more controlling for observable characteristics, while `South Asians` earn **13.1%** unadjusted and **8.1%** adjusted more than `whites` respectively.

### Examining the East Asian pay advantage

Breaking down the explainable `East Asian` earnings premium yields some interesting findings:

[![East Asian](https://plot.ly/~whoisnnamdi/53.png?share_key=u6QFIdd8N6y0GRHdUzflZT)](https://plot.ly/~whoisnnamdi/53/?share_key=u6QFIdd8N6y0GRHdUzflZT "East Asian")

First — `years of professional coding experience` is far and away the biggest factor holding down the pay of `East Asian` software engineers. `East Asian` developers typically have less work experience than whites, which holds down their earnings.

-   My calculations suggest that `East Asian` developer would earn **8.0%** more if they had similar amounts of professional experience as whites
-   This rises to **9.0%** if we add in additional, non-professional, coding experience.

`Age` also holds back `East Asian` developers, as they are typically younger than white developers. This amounts to a **2.0%** pay disadvantage.

Lastly, the sheer magnitude of the earnings premium enjoyed by `Asian` developers should be noted. That the premium remains so large even after controlling for various factors is puzzling.

### Good and bad news for black software developers

Let’s look at one more — the pay gap for `black` developers. The unadjusted gap — which again simply compares average earnings of `black` and `white` developers — is **\-7.6%**, while the adjusted gap contracts meaningfully to **\-0.3%**, which is not statistically significant:

[![Black](https://plot.ly/~whoisnnamdi/55.png?share_key=uWaEpRLPDnUZtrlzKx31LP)](https://plot.ly/~whoisnnamdi/55/?share_key=uWaEpRLPDnUZtrlzKx31LP "Black")

Breaking down the explainable gap for `blacks` reveals similar drivers as we saw in the `East Asian` case. `Years of professional coding experience` is the main contributor to lower pay for black software engineers relative to `whites`, in total driving **5.8** percentage points of the overall 7.6% gap. Nothing else matters nearly as much.

In a sense, this is heartening. Assuming a `black` engineer gets as much out of an additional year of work experience as anyone else, purely closing the gap there would bring black pay nearly in line with white pay.

On the other hand, that the unadjusted gap is explainable via `years of experience` also means that the gap is unlikely to close anytime soon.

Why? The `age` structure of the workplace is slow to change — it takes decades to see sizable shifts

-   Additionally, as the industry diversifies, by definition most `black` professionals entering the software development career track start off at the bottom rung of the ladder
-   Thus, the diversification of the industry in fact depresses average `black` earnings, as fresh out of bootcamp developers don’t earn nearly as much as seasoned veterans

This is not a bad thing — but it does mean I can’t conclude something nefarious is behind the slow convergence of `black` and `white` wages without other evidence.

The other factor worth touching on is `ImpSyn` which is a variable representing a respondent’s own `confidence` in their skills as a software developer. More `confident` developers earn more, and there appears to be a confidence gap between black and white developers driving **1.1%** of the earnings gap.

## Gender

[![Ethnicity](https://plot.ly/~whoisnnamdi/51.png?share_key=D2GlnNRYyS41CEMS6rzhXy)](https://plot.ly/~whoisnnamdi/51/?share_key=D2GlnNRYyS41CEMS6rzhXy "Gender")

### Young women entering the software development workforce pull down average female earnings

`Women` earn **10.0%** less than male software engineers on average, a sizable difference. However, this gap is effectively eliminated once we adjust for controllable factors, falling to only **1.4%**, which is not statistically significant.

In diagnosing the unadjusted 10.0% pay gap for `women`, `years of experience` pops up once again as a dominant factor explaining most of the gap:

[![Woman](https://plot.ly/~whoisnnamdi/59.png?share_key=QGU59riYZCzp45iDEoc7uJ)](https://plot.ly/~whoisnnamdi/59/?share_key=QGU59riYZCzp45iDEoc7uJ "Woman")

**5.7** percentage points of the `gender` pay gap can be explained by the fact that `female` developers have less professional experience than `male` developers on average. Adding in overall `coding experience` explains **7.1** total percentage points of the overall gap.

[![Sexuality](https://plot.ly/~whoisnnamdi/76.png?share_key=9uXXMY4gKT4Ri9XCd2uQrx)](https://plot.ly/~whoisnnamdi/76/?share_key=9uXXMY4gKT4Ri9XCd2uQrx "Sexuality")

While `women` are only 1.7 years younger than `men` on average in the dataset, they have **3.3** fewer `years of professional coding experience` (7.1 years for `women` vs. 10.4 for `men`). We can see this in the histogram / [kernel density estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation) comparing the respective distributions of `years of professional coding experience` of `men` and `women`, where the distribution for `women` is shifted and more clustered to the left. This meaningful difference explains why `professional experience` is such as major driver of the gender wage gap.

`Confidence` (`ImpSyn`) comes up again as a factor pulling down `female` wages. Here, the `confidence` gap explains **1.0%** of the overall `female-male` `gender` gap, very similar in magnitude to that of `black` developers vs. `white` developers.

### Small contribution from other factors

In line with other research, `women` are also less confident about their own programming skills than `men` are (who for all we know might be overconfident), which explains another **1.0%** of the total gap (because higher confidence leads to higher pay, as I cover in a later post).

Experience and confidence collectively explain **8.1%** of the gender pay gap among software engineers, leaving only a 1.9% gap, including the 1.4 percentage point difference that we cannot explain.

### Receive a report with the full results

 

Send Report ⚡

## Sexual orientation

[![Sexuality](https://plot.ly/~whoisnnamdi/65.png?share_key=t51SFgyvKOWIwEl2u1Ld6N)](https://plot.ly/~whoisnnamdi/65/?share_key=t51SFgyvKOWIwEl2u1Ld6N "Sexuality")

### Earnings penalty for non-straight developers disappears after controlling for other factors

Unadjusted pay gaps among non-straight software engineers ranges from **2.5%** for `gay` and `lesbian` developers to **9.6%** for `bisexual` developers, which simply means these individuals earn less on average than `straight` engineers.

In the case of `gay` and `lesbian` developers however, this gap closes and actually reverses once I add controls. The gap becomes a pay _advantage_ of **3.4%**.

[![gay_lesbian](https://plot.ly/~whoisnnamdi/67.png?share_key=Sr50IIjPqYHtxWDS80yymI)](https://plot.ly/~whoisnnamdi/67/?share_key=Sr50IIjPqYHtxWDS80yymI "gay_lesbian")

Starting at the adjusted / unexplained 3.4% premium, the lower average unadjusted earnings is largely accounted for by `years of professional coding experience` (a recurring theme) and `age`, suggesting that `gay` and `lesbian` developers are simply earlier in their careers than `straight` developers, on average.

The decomposition of the difference in adjusted and unadjusted pay gaps is quite similar for `bisexual` developers. `Professional experience` and `age` continue to the do the most explanatory work here:

[![bisexual](https://plot.ly/~whoisnnamdi/69.png?share_key=QxSeWndRHlY67kVOkXIxiF)](https://plot.ly/~whoisnnamdi/69/?share_key=QxSeWndRHlY67kVOkXIxiF "bisexual")

## Parenthood

[![Dependents](https://plot.ly/~whoisnnamdi/71.png?share_key=dWo5kDpYg4FllIVKqQa5Ae)](https://plot.ly/~whoisnnamdi/71/?share_key=dWo5kDpYg4FllIVKqQa5Ae "Dependents")

### Parents earn more, but this is largely explained by other factors

Software engineers with `dependents` (typically children) earn **17.2%** more than those without, but this earnings premium can be almost entirely accounted for via the other factors described in this analysis.

[![yes](https://plot.ly/~whoisnnamdi/73.png?share_key=r1C05eXGR0rC3Qa0ZQja0q)](https://plot.ly/~whoisnnamdi/73/?share_key=r1C05eXGR0rC3Qa0ZQja0q "yes")

Controlling for these variables reduces the parenthood earnings premium to **1.5%**, which is small but statistically significant.

-   Perhaps the data reflects that these developers earn extra income in order to take care of their `dependents`
-   Such a small bump in earnings like does not cover the additional expense of a child or other `dependent`, however

Unsurprisingly, `years of software development experience` and `age` account for most of the earnings premium among parents. These are largely mid and late career engineers.

`Working remote` explains another **1.1%** of the pay premium among parents, as they are more likely to work from home, which makes sense given they may have a child to take care of.

## Conclusion: Significant progress on pay

Not to editorialize, but I was encouraged by many of the results here. In general, along most dimensions, discrimination in software developer earnings appears small once various factors are controlled for. In most cases, the biggest factors were some combination of `years of coding experience` and `age` which are both “problems” that will largely fix themselves as the industry diversifies.

With the exception of `race`, most of the gaps are no more than a few percentage points in magnitude. In the case of race, the gaps are meaningful in some cases but difference is in fact _in favor_ of minority groups like `East Asian`, `South Asian`, and `Middle Eastern` software developers. Their pay advantages are substantial, and the data from the [Stack Overflow](https://insights.stackoverflow.com/survey/2019) survey fail to fully explain these gaps.

The usual caveats to pay analyses apply here as well. Finding little discrimination on pay, after controlling for factors such as `job title`, does not disprove discrimination in its entirety.

-   For example, if `female` software engineers face discrimination in the form of reduced upward career mobility, that will not show up in this analysis, even though it depresses their earnings
-   The same would apply to older software developers, who may be pressured out of their organizations to make room for cheaper, younger developers.

Analogous statistical caveats abound.

That said, the data suggest the software development industry is well on its way to pay parity across the important dimensions of age, race, gender, and sexuality.

_Thanks for reading part 1 of my 2020 analysis of software developer pay. You can find the rest of the analysis and methodology [here](https://nnamdi.net/highest-paid-software-engineers-2020)._ ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[The Highest-Paid Software Engineers: 2020 Edition]]>
                        </title>
                        <description>
                            <![CDATA[Engineers are the basic economic unit of modern software development.

The software production function depends critically on developer productivity and compensation.

And yet software engineering pay remains poorly understood.]]>
                        </description>
                        <link>https://whoisnnamdi.com/highest-paid-software-engineers-2020/</link>
                        <guid isPermaLink="false">highest-paid-software-engineers-2020</guid>
                        <category>
                                        <![CDATA[ Developers ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Mon, 24 Feb 2020, 17:15:07 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512192005-clip-programming.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
Engineers are the basic economic unit of modern software development.

The software production function depends critically on developer productivity and compensation. No developers, no software.

Yes “[no code](https://medium.com/@rrhoover/the-rise-of-no-code-e733d7c0944d)” is a thing — but even the no code systems themselves are developer-built. You can’t get around it.

And yet software engineering pay remains poorly understood. Different employers pay differently for the same engineering talent. Engineers with similar resumes are paid varying salaries by the same employer.

This guide explains why.

Developer compensation is a critical piece of technology's economic impact. Awareness of this data makes you a better informed citizen of the industry.

**I am dedicated to enhancing the careers of software developers and the functioning of the organizations that employ them.** Compiling this report [every year](https://nnamdi.net/highest-paid-software-developer/) is one way I do this.

If you’re a…

-   developer,
-   founder,
-   manager,
-   or executive

... then this guide is for you.

### Receive a report with the full results

 

Send Report ⚡

## Table of contents

-   [Part 1: How Age, Race, and Gender Affect Software Engineering Pay](/age-race-gender-software-engineering-pay)
    -   How `age`, `race`, `gender` and other characteristics affect pay and the extent of pay discrimination in the software development industry
-   [Part 2: Do College Degrees Matter for Software Engineers?](/college-degrees-software-engineers/)
    -   Does `education` matter for how software engineers get paid? The answer: maybe
-   Part 3: The Characteristics of the Best-Paid Software Engineers and Best-Paying Companies (coming soon)
    -   The highest and lowest-paid `engineering roles`, how much big Big Tech pays, the `experience` advantage, and more
-   Part 4: The Highest-Paid Programming Languages, Databases, and Frameworks (coming soon)
    -   Does `React.js` pay better than `AngularJS`? How do `operating systems` affect developer earnings? How important is fluency with `cloud infrastructure`?

## About this guide

### The data

As with [last year’s analysis](https://nnamdi.net/highest-paid-software-developer/), the data in this report is based on a subset of [Stack Overflow’s annual developer survey](https://insights.stackoverflow.com/survey/2019). They have run this survey for a while now and each year graciously open source the responses in an easy work with CSV file.

The survey is global, but here I focus on 10,355 U.S. based individuals employed as software engineers on either a part-time, full-time, or independent basis.

The data is entirely self-reported, so I implicitly assume respondents make accurate claims as to their income, personal characteristics, qualifications, etc. To the extent there are obviously false responses, I have attempted to remove them.

I used Python for the analysis, and if you’d like to reproduce the results, I’ll be releasing the code I wrote after publishing the full results on my [GitHub](https://github.com/whoisnnamdi/).

### Why it’s important to adjust pay gaps for observable factors

Two possible statements comparing the pay of different groups of software developers:

1.  Developers of `type A` make X% more than developers of `type B`, on average
2.  Developers of `type A` make Y% more than developers of `type B`, all else equal

X and Y are rarely the same number. X compares the average earnings of the group A and B. Y compares hypothetical As and Bs who are similar in all dimensions except one, allowing us to attribute the difference to that single trait.

Most analyses of pay gaps stop at statement #1 and call it a day. **This is lazy and misleading.**

Though we’d love to know both X and Y, it is Y that corresponds better to our intuitive meaning of “pay gaps” — the difference between the earnings of two groups who are equivalent except for a single trait of interest (`age`, `gender`, `years of experience`, etc.).

Identifying Y requires additional data on characteristics that may correlate with earnings.

### Receive a report with the full results

 

Send Report ⚡

### We can’t explain everything

Using the above methodology, I’ll use the following terms in this report:

-   **Total / Unadjusted gap:** X, as above, the average difference between groups (e.g. pay difference between white and black engineers)
-   **Unexplained / Adjusted gap:** Y, as above, the difference between groups who vary on some dimension that can’t be explained by that variance (e.g. pay difference between similar white and black engineers)
-   **Explained gap:** X - Y, the portion of the total gap that is explainable by factors other than the trait of interest (e.g. pay difference between white and black engineers explained by factors unrelated to race)

Note — the unexplained gap is exactly that, unexplained. We cannot say for certain that the entire unexplained gap between, say, white and black software engineers is due to discrimination on race, for example. If we use different controls, the “unexplained” gap would change. At best, the unexplained gap provides an upper bound estimate of the gap attributable to that trait.

How do we know what to control for? The Stack Overflow survey upon which this analysis is based provides a rich set of data on each developer based on their answers to various questions. It’s too complicated to cover here, but I do principled covariate selection using Double Lasso per ([Chernozhukov, Hansen, Urminsky 2016](http://home.uchicago.edu/ourminsky/Variable_Selection)) for find the best set of controls for each gap I examine.

All references to statistical significance are at the p < 0.05 level. Confidence intervals are upward skewed because the original regressions used log-transformed income as the dependent variable.

### It gets better every year

This year’s new and improved analysis comes with the following enhancements:

-   **Interactive charts via Plot.ly**
    -   Hover over the charts for additional data points
-   **Uncontrolled and controlled differences**
    -   Uncontrolled effects correspond to the statement: “Developers of type X earn Y% more than developers of type Z”
    -   Controlled effects correspond to the statement: “All else equal (that we can control for) developers of type X earn Y% more than developers of type Z”
-   **Deep dives into the drivers behind unadjusted pay gaps**
    -   Disaggregation of the explainable pay gaps
    -   Decomposition follows methodology of ([Gelbach 2016](https://www.journals.uchicago.edu/doi/abs/10.1086/683668))

I am also releasing a developer earnings calculator (coming soon). Answer a few questions, and the calculator will output a pay estimate and confidence range based on the same data in this analysis.

Check out [last year’s report](https://nnamdi.net/highest-paid-software-developer/) to see how the numbers changed year-over-year.

## Knowledge (of money) is power

I do this analysis every year because I think it’s vital to understand how developers, **the basic economic unit of software development**, are compensated and rewarded for their efforts.

By the end of this analysis, you will:

-   Enhance your knowledge and understanding of the factors driving software developer earnings
-   See why some software engineers are paid more than others
-   Understand progress on eliminating discrimination in engineering pay
-   Develop an appreciation for how various factors intermingle and interact

I hope this 2020 guide to software engineering pay is valuable to you. The many hours spent conducting and assembling this analysis were certainly valuable to me! ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[How Uber and Lyft Dominated Ridesharing By Seeing Red Where Others Saw Blue]]>
                        </title>
                        <description>
                            <![CDATA[A quick visit to the Red Sea]]>
                        </description>
                        <link>https://whoisnnamdi.com/red-ocean/</link>
                        <guid isPermaLink="false">red-ocean</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Wed, 29 Jan 2020, 19:21:20 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512188626-91YCWH4jFdL.jpg" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
Over a decade ago, INSEAD business strategy professors W. Chan Kim and Renée Mauborgne popularized the terms “[**blue ocean**](https://www.blueoceanstrategy.com/)” — untapped markets where “demand is created rather than fought over”, enabling profitable growth — and “**red ocean**” — zero-sum markets, where every fluid ounce of demand is bitterly fought over, like sharks fighting over a whale carcass.

**The early ridesharing market was a deeply red ocean, plagued by price wars, rife with cutthroat tactics, and littered with failed startups.**

![](/content/images/2020/01/red-vs-blue.png)
*Source: BlueOceanStrategy.com*

However, many entrepreneurs and investors initially saw a blue ocean in the early ridesharing market. Ridesharing was a new, unexplored medium that promised to revolutionize and expand the ground transportation market wherever mobile devices were ubiquitous by “unlocking” pent-up demand for one-tap transportation.

As a young investor, I balked at this characterization. While I agreed on the long-run impact of these next-generation transportation networks, I thought this view was dead wrong in the short-term.

Here’s why.

### Receive my next thought in your inbox

 

Go ⚡

## Demand-driven market growth

In the simple Economics 101 model of supply and demand, market growth comes in two flavors: demand-driven and supply-driven, characterized by increased willingness to buy or sell respectively at a given price.

![](/content/images/2020/01/Screen-Shot-2019-03-30-at-3.39.22-PM.png)
*(Shaded area represents size of market in dollar terms, i.e. price x quantity = market size)*

Many people miss the subtlety that the cause (supply or demand) behind the effect (growth) impacts how attractive a market is to new and existing entrants.

More demand translates into both higher prices and volume transacted. Hence, markets growing due to a buildup of demand are highly attractive to new entrants, as businesses can grow without necessarily taking share from others and avoid contentious, competitive battles over market share.

![](/content/images/2020/01/Screen-Shot-2019-03-30-at-3.37.56-PM.png)

Even with the arrival of new competitors, as long as the increased in demand outstrips the increase in supply, prices, and therefore profits, will be stable or increasing, and we’ll see higher volume of transactions regardless.

This is both exciting to new market entrants and encouraging for existing suppliers.

Demand-driven growth is intuitive. It's what most people associate with a growing market. It’s easy to assume that market growth always comes from consumers demanding more.

## Supply-driven market growth

However, the sudden growth of ridesharing is better explained by activity on the supply-side — the founding of Uber, Lyft, and an [eclectic](https://www.inc.com/magazine/201307/christine-lagorio/companies-that-are-like-uber.html) [cast](http://www.businessofapps.com/news/eight-uber-competitors-reinventing-taxi-apps/) of [competing](https://www.nytimes.com/2016/04/14/business/smallbusiness/ride-sharing-start-ups-compete-in-uber-for-children-niche.html) [startups](https://www.fastcompany.com/3065089/the-ride-share-startup-thats-competing-with-uber-and-lyft-by-charging-1), all focused on the same market, the same core idea.

**Ridesharing was not a demand-side phenomenon.** The market was itself _created_ by the supply-side, which then generated supply-driven growth.

Supply-driven market growth causes prices to decline rather than increase. These price cuts eat away the profitability of suppliers and bar others from entering the market who cannot viably operate at these lower prices — like a sea wall breaking waves before they hit the shoreline.

![](/content/images/2020/01/Screen-Shot-2019-03-30-at-3.38.08-PM.png)

Further, gaining share requires either fierce competition along the product or marketing dimension or [aggressive price cuts](https://www.forbes.com/sites/briansolomon/2016/01/25/is-uber-trying-to-kill-lyft-with-a-price-war/). Market share must either be taken from other suppliers, who originated the market growth in the first place, or created by offering the same or better product at lower prices.

The ridesharing market spawned out of the emergence of these new services — new sources of supply.

What funded this [Cambrian explosion](https://en.wikipedia.org/wiki/Cambrian_explosion) of ridesharing? **Venture capital.**

Capital is a key competitive advantage due to its ability to fund continuous price subsidies. These artificially deflated prices both undercut [capital-constrained](https://nnamdi.net/entrepreneurs-ruin/) competitors and drive consumer usage.

Prices down, volume up.

The flood of new customers impresses other investors and attracts more funding, enabling the early winners to cut prices further. Additionally, the increased ridership attracts and retains drivers, whose earning power is enhanced via two-sided network effects about which much has [previously been written](https://andrewchen.co/marketplace-startups-best-essays/).

This completes the virtuous cycle:

Initial innovation → venture capital → lower prices → more usage → venture capital

## Swimming with sharks

**Lyft and Uber correctly saw the nascent ridesharing market as a red ocean.** Despite dreams of a future where spend would shift from personal car purchases to ridesharing, they could not afford to wait for demand to arrive. In the near-term, supply would outpace demand, making the market much more competitive.

Appropriately, Uber and Lyft armed themselves for the fight, raising “[war chests](https://www.theverge.com/2013/5/23/4357666/lyft-international-expansion-60-million-andreesen-horowitz)” of capital to fund a ridesharing “[arms race](https://www.forbes.com/sites/briansolomon/2016/06/02/uber-is-breaking-all-the-rules-in-its-25-billion-arms-race/#41f2b0e22646)”.

Uber and Lyft expanded aggressively into new cities at a breakneck pace. They [cut prices](http://nymag.com/intelligencer/2014/07/uberx-cutting-prices-by-20-percent-in-new-york.html), then [cut them again](https://www.vox.com/2015/1/9/7519237/the-48-cities-where-uber-is-cutting-prices), and just [kept on cutting](http://time.com/4175148/uber-price-drops/). Already market leaders, they spent generously on [cash bonuses](https://www.cnbc.com/2014/07/29/lyfts-sacrifice-for-the-sake-of-its-nyc-launch.html) and other [driver incentives](https://www.uber.com/drive/atlanta/resources/driver-partner-incentive-guarantee-faq-questions/).

In contrast, their early competitors bought into the eventually-true-but-not-yet promise of a [world transformed](https://medium.com/ipg-media-lab/the-shift-towards-multimodal-transportation-the-future-of-mobility-d0c7c25d4a06) by app-based transportation, hoping to surf the seemingly blue [ridesharing wave](https://www.huffpost.com/entry/uber-google-waze-ride-hailing-lyft_n_57c8558fe4b0a22de09485cc).

Nothing wrong with being a small fish in a big, blue pond, right?

These hapless ride-surfers didn’t realize supply-side sharks like Uber and Lyft were the entire reason for the wave in the first place.

How did this tidal wave impact other ridesharing startups?

-   Unable to match the subsidized prices, other players failed to attract users
-   Fewer customers meant lower driver earnings, leading to an exodus of drivers
-   Sparse supply on the driver-side lengthened wait times, degrading the rider experience, pushing more of them away
-   Declining usage scared off new investors
-   Incapable of raising fresh funds, the other fledging ridesharing startups, metaphorically, drowned.

The “blue ocean” turned red; the virtuous cycle turned vicious.

Acting with haste and fervor, Uber and Lyft established an insurmountable lead, effectively eliminating their U.S. competition before changing norms and consumer behavior on the demand-side had a chance to catch up.

In other words, they saw red where others saw blue.

## Lessons learned

A few key takeaways:

> **1\. Supply is red, demand is blue**

-   A merely growing market is not necessarily an attractive one
-   The drivers of growth matter more than the level of growth

> **2\. The deep sea requires deep pockets**

-   Capital can often be converted directly into market share
-   Red ocean markets demand significant capital, as [growth is unprofitable](https://www.reuters.com/article/us-uber-usa/uber-boss-says-u-s-market-unprofitable-amid-tough-competition-from-lyft-idUSKBN1D9348) in the short-run

> **3\. Cheap capital and disruptive innovation create supply-side tsunamis**

-   When capital is inexpensive, supply reacts to innovation much faster than demand

That the ridesharing market could consolidate so early in its existence is remarkable but also perhaps predictable if one understands it’s supply-side origins.

And despite an evolution of norms around getting into a stranger’s car, Uber and Lyft are [betting big](https://mashable.com/article/lyft-ipo-self-driving-cars-investors/) on another supply-side innovation: [autonomous vehicles](https://www.businessinsider.com/uber-lyft-self-driving-taxis-may-not-help-profitability-mit-2019-5).

Why? Again: to push out the supply curve, drive down prices, expand usage, attract capital, and reinvest in more robotaxis.

In other words, it’s the same supply-driven growth story all over again.

That the two ridesharing leaders continue to operate unprofitably at scale only further drives the point home. **This is not a demand-side story and never has been.**

This is a cautionary tale for entrepreneurs and investors clamoring to capitalize on the latest “wave”.

Sometimes, the right kind of rose-tinted glasses isn’t such a bad thing. ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Pattern Matching is Dead. Long Live People Matching]]>
                        </title>
                        <description>
                            <![CDATA[Venture capital is changing — for the better]]>
                        </description>
                        <link>https://whoisnnamdi.com/people-matching/</link>
                        <guid isPermaLink="false">people-matching</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Tue, 10 Dec 2019, 17:35:27 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512185992-people-matching-2-cropped-2.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
An investor once shared with me a simple framework for the fundamental skills of venture capital:

-   👁 See (the deal)
-   🧠 Analyze (the deal)
-   ❤️ Win (the deal)

Success in VC demands all three. But that doesn’t mean they're equally important.

## 👁 Out of sight, out of mind

![](/content/images/2019/12/see-2.jpg)

You can’t analyze a deal you can't see. You also can’t win a deal you can't see.

“Seeing the deal,” also referred to as “deal flow,” continues to be an important differentiator between the best funds and the rest of the pack.

Networks and access are a potent currency in Silicon Valley. As I like to say: **you can be the smartest person in the room, but if you aren’t even in the room, it doesn’t matter.**

As such, VC firms want to know about every deal being done, _even if they have little interest in doing the deal_. They want the option to say no. As in many domains, that optionality is incredibly valuable.

But simple awareness of a fundraise does not do much for you as a VC. These days, it’s often common knowledge that a particular company is fundraising.

Even if you know a deal is going down, there’s no guarantee you will be able to insert yourself into the consideration set of potential partners. I’ve had entrepreneurs directly deny to me they were fundraising when they in fact were, only to admit it after the fundraise announced shortly thereafter.

The concept of an “[anti-portfolio](https://www.fool.com/investing/general/2014/02/20/whats-in-your-anti-portfolio.aspx)” proves that even if you see a deal you can still drop the ball.

Many VCs “saw” the Facebook deal. Many VCs “saw” the Google deal. And yet they missed the most explosive growth companies the world has ever seen.

**“Seeing” is necessary but not sufficient.**

> VC's have always relied on the strength of their networks for dealflow.  
>   
> As an increasing number of founders come from more diverse backgrounds, VC's will have to change to stay relevant.  
>   
> More diverse partnerships, more openness towards cold emails, less pattern matching.
> 
> — Boris Wertz (@bwertz) [August 20, 2019](https://twitter.com/bwertz/status/1163885146048569344?ref_src=twsrc%5Etfw)

I don’t mean to deny or discount the value of deal flow. Deal flow is a valuable resource, but it’s exactly that — a resource. A resource must be extracted, enriched, and processed. Converting that resource into useful energy requires investment and effort. Otherwise, it remains inert.

### Receive my next thought in your inbox

Go ⚡

## 🧠 Signal in the noise

![](/content/images/2019/12/analyze-2.jpg)

Venture capitalists often talk about “pattern matching.” After years of investing and watching startups succeed or fail, VCs supposedly develop a keen sense of what drives success, detecting signals in the noise. These signals can take many forms, but the important claims are:

-   Patterns exist
-   Patterns are discoverable
-   Patterns are valuable

Pattern matching epitomizes the “analyze” skill.

There are three issues with basing your competitive advantage on pattern matching.

First, pattern matching is rarely differentiated. The important metrics and signs of traction are well-known these days. Increasingly, the “good” founders are obvious — those with the best pedigrees or past successes.

Like in public equities, obvious patterns are priced in. If it’s not priced in — most often — it’s not a real pattern.

This dilutes the value of pattern matching. Suddenly, the well-worn patterns VCs rely on so heavily begin to work against them:

-   The founder’s a Stanford grad? Everyone knows that, it will get priced in.
-   Previously a Google product manager? Yep, the valuation will reflect that.
-   Big, fragmented market ripe for consolidation? You bet the final valuation will incorporate this.

![](/content/images/2019/12/demand-shift-2.png)

In a competitive environment, patterns carry the seeds of their own impotence.

Second, it’s not at all obvious that pattern matching even helps much at the earliest stage. The drivers of success are indeterminate, and stories abound about ideas that seemed dumb at first but became winners. In some cases, even the VCs who funded the best companies had serious doubts going in.

If even the “smartest guys in the room” are not _that smart_, we know we have a problem.

> VC pattern matching and startup folklore are the path to mediocrity. Focus on commercial success and let data tell the story. You’ll be the next pattern people try to match. [https://t.co/ggDZJmJWbV](https://t.co/ggDZJmJWbV)
> 
> — bradford cross (@bradfordcross) [September 22, 2018](https://twitter.com/bradfordcross/status/1043351318566293504?ref_src=twsrc%5Etfw)

Third, this all assumes the patterns are _correct_ — that they contain some useful information.

_This too is suspect._

Data in VC is notoriously spotty, unreliable, and often unavailable entirely. Tying cause to effect is quite difficult, and success or failure is often over-determined.

In this environment, our natural tendency as humans is to fall back on heuristics. Some of these are helpful, but many are problematic.

Another word for a problematic heuristic is **bias**.

> _"There’s quite substantial unconscious bias that most of my male investment colleagues don’t see, where if a guy looks and smells the part with the pattern matching, (he) just (gets) such a break" – Linden Rhoads, General Manager of the W Fund ([Source](https://www.bizjournals.com/seattle/blog/techflash/2016/03/how-pattern-matching-by-investors-puts-female.html))_

“Pattern matching” seems increasingly tenuous. Either it’s easy and reliable, in which case it gets priced in, or it’s difficult to execute on in practice, making it easy to fool oneself into believing non-existent patterns, almost superstitiously.

Either way, it’s fraught.

## ❤️ All I do is win

![](/content/images/2019/12/win-2.jpg)

Venture capital is a match making service.

Winning in modern venture investing fundamentally concerns what I call **“people matching”** — bringing together the right combination of founders and investors to partner on a particular startup or idea.

It’s about the people, not the capital. Yes, capital is also matched or allocated to specific startups, but this function is being commoditized.

Showing up with a checkbook today is no longer cool or particularly notable.

**Not all funds can show up with the right partner**, however. A partner the entrepreneur wants to work with. Someone who will make both the founder and the startup better by their presence and involvement.

In this sense, the coveted “partner” title is actually quite fitting.

**Partners partner.**

In this model, venture capital firms are less allocators of capital — which is currently cheap, plentiful, and most importantly, homogeneous — and more **allocators of people**, who remain differentiated even in this hot house environment.

People are the truly limited and differentiated resource of VC firms. General Partners seek to maximize the capacity of the fund to generate returns for its Limited Partners, but ironically, **it is the General Partners who are “limited”** — there’s only so many of them, and they must be deployed judiciously. And with all the capital flowing around these days, Limited Partners are increasingly _unlimited_.

Unlike a publicly traded company, which can effectively absorb any amount of capital the market could realistically desire to invest, with prices regulating this flow, the typical startup cannot and does not want to take on every possible dollar from every willing investor. The best deals are often oversubscribed multiple times over. Financing rounds are [rivalrous](https://en.wikipedia.org/wiki/Rivalry_\(economics\)) — **we can’t all do the deal.**

The VC who can most quickly convince a founder that they are the right person to partner with will win the deal. And by winning the deal, others, by definition, lose the deal.

> Talking to 43+ funds is a massive waste of time of everyone’s time  
>   
> Now that access is completely commoditized, closing is the most valuable skill  
>   
> Founders, find your lead first to remove noise & get back to work  
>   
> If it’s unclear what value a participating firm offers, move on. [https://t.co/o4zbodpENL](https://t.co/o4zbodpENL)
> 
> — Brianne Kimmel (@briannekimmel) [December 3, 2019](https://twitter.com/briannekimmel/status/1201966954090655744?ref_src=twsrc%5Etfw)

Winning is therefore the fulcrum upon which success in venture turns. Two comparable firms can both see the deal, analyze the deal “correctly”, and yet only one may win the deal in the end. The firm that does a better job of **people matching** will, in the end, triumph.

This is why winning matters so much.

## 👥 More skin in a new game

Venture capital is evolving from a game of picking winners to a game of _being picked by winners_.

Authentic relationships and credible ability to add value matter more than ever.

Venture capital is no longer where one goes to retire quietly after an illustrious operating career. It’s where one goes to _hustle_.

> Pushing to win an investment I really want. Might lose it.  
>   
> Founder in driver's seat. Some investors complain, I love it. Some call this a bubble, but everyone is being rational.  
>   
> Founder/investor dynamic is reaching equilibrium and this is how it should be.
> 
> — Paul Murphy (@paulbz) [November 19, 2019](https://twitter.com/paulbz/status/1196914235059691521?ref_src=twsrc%5Etfw)

I think this is a positive trend, encouraging deeper empathy between investors and founders. If anything, bringing VCs down a peg might humanize them — the gilded kings and queens of Silicon Valley become the humble servants of entrepreneurs.

There’s some moral justice in this. I’ve always been irked by the very different lives lived by founders and their investors:

-   One is concentrated, the other is diversified
-   One is expected to fail, the other is expected to at least breakeven
-   One is a beggar, the other is a chooser

This is out of balance. Entrepreneurs hustle, **why shouldn’t VCs?** ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[High Retention = High Volatility]]>
                        </title>
                        <description>
                            <![CDATA[Why SaaS revenue retention is a double-edge sword]]>
                        </description>
                        <link>https://whoisnnamdi.com/high-retention-high-volatility/</link>
                        <guid isPermaLink="false">high-retention-high-volatility</guid>
                        <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Mon, 4 Nov 2019, 18:39:50 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512184302-volatility.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
> "With great retention comes great volatility" – Uncle Ben in Spiderman

OK – that's not _exactly_ what Uncle Ben said, but it makes an important point: **retention is a double-edged sword.**

Higher retention means customers stick around and pay you longer. Customers paying you more over time is even better.

Improving retention increases the value of your business, all else equal.

However, **higher retention also drives higher valuation volatility.**

The reason is clear to anyone who’s ever traded bonds.

## Cohort Math = Bond Math

![](https://images.unsplash.com/photo-1535320903710-d993d3d77d29?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ)

A cohort of customers can be likened to financial security – a bond paying you some amount periodically, perhaps indefinitely.

This “interest rate” fluctuates – it changes over time. Some customers cancel, churning out, and others maintain or even increase the size of their subscription, paying more over time. Positive churn implies these payments decline over time, while negative churn implies the opposite.

A bond has some value associated with it, which is roughly equal to the [present discounted value](https://www.investopedia.com/terms/p/presentvalue.asp) of all the future payments from the bond.

This value [varies inversely](https://www.thebalance.com/why-do-bond-prices-and-yields-move-in-opposite-directions-417082) with the effective interest rate or yield of the bond. Similarly the value of a stream of cash flows varies inversely with the discount rate applied, or in the case of a company, its [cost of capital](https://hbr.org/2015/04/a-refresher-on-cost-of-capital).

This sensitivity to interest rates increases with the tenure of the bond. In others words, the value of a 30-year bond is more sensitive to interest rates than that of a 10-year bond.

In finance, this concept is called [**duration**](https://www.blackrock.com/us/individual/education/understanding-duration). Duration measures the sensitivity of the value of a bond to a change in interest rates, which is tied to the lifetime of the bond. Bonds with longer tenure or back-loaded cash flows are more sensitive to changes in interest rates.

Due to the [multiplicative nature](https://nnamdi.net/you-dont-understand-compound-growth/) of discounting, the present value of far-away payments is more sensitive to a change in interest rates than the value of soon-to-come payments.

This is exactly the situation companies with high retention face.

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div>

## Cohort Lifetime = Bond Duration

**With higher retention, more and more of the value of an acquired customer comes from its later years of life.**

Again, the present value of these later years is more sensitive to changes in the discount rate applied to the cash flows, due to the [compounding of discounting](https://nnamdi.net/you-dont-understand-compound-growth/).

Since all companies are on some level simply a collection of various customer cohorts with variable and fixed costs layered on, the summed value of the individual cohorts determines the overall company's valuation.

Hence, the better your retention, the more sensitive and volatile your valuation is to changes in your discount rate or cost of capital. This contrasts with a high-churn company where customers last only a few months or years.

**Let’s break out the spreadsheet and prove this to ourselves:**

-   Imagine we acquire a cohort of customers who pay an annual amount growing or contracting at some constant yearly rate, which represents our revenue retention
-   Let’s assume this expansion or contraction continues for 10 years, after which the revenue collected flattens out
-   Lastly, we assume some discount rate or cost of capital, which we will use to discount the value of those payments back to the present

![](/https://nnamdi.net/content/images/2019/10/DraggedImage-1.png)

Now we ask the question: assuming a base discount rate of 10%, how would an increase to 15% or 20% impact the present value of the cohort’s revenue stream?

The answer is quite dramatic:

![](/https://nnamdi.net/content/images/2019/10/DraggedImage-2.png)

**For a company with strong net retention of 130% (-30% revenue churn), an increase in discount rate from 10 to 15% cuts the company’s valuation in half.**

Notice how a company with 30% annual revenue churn only sees a 10% impact from a similar change in discount rate. This reflects the non-linear impact of the discount rate with respect to retention described earlier:

![](/https://nnamdi.net/content/images/2019/10/DraggedImage-3.png)

## Theory → Reality

Many SaaS companies have gone public this year with strong net retention metrics of 100%+.

In recent months, however, these same companies have seen their valuations tank, often without any meaningful changes in operational performance.

![](/https://nnamdi.net/content/images/2019/10/image.png)
*Source: Jon Ma @ Public Comps*

While some point to this as an example of the “[WeWork Effect](https://theconversation.com/fallout-from-weworks-failed-ipo-shows-the-folly-of-excessive-valuations-125014)”, these business are so fundamentally different in both business models and business performance that WeWork’s troubles cannot possibly explain these haircuts.

Let’s go a level deeper. If lower churn leads to higher volatility we should be able to see it in the data.

I assembled the following set of SaaS stocks, finding their latest revenue growth and net retention data, along with the price decline they saw from 9/21 to 10/21.

![](/https://nnamdi.net/content/images/2019/10/DraggedImage-4.png)

If the theory is true, we should see companies with higher net revenue retention declined further in the recent valuation pullback.

That’s exactly what we see:

![](/https://nnamdi.net/content/images/2019/10/DraggedImage-5.png)

**Higher revenue retention, bigger valuation cut.** On average, 10% higher retention lead to an additional 3.4% decline in price. The relationship is statistically significant at p < 0.05 (in case you were wondering), and the "low" R-squared of 0.36 simply means you can't explain all of the variation in or predict the level of price change using only this single variable, despite the strong relationship. Given public stock movements are notoriously difficult to predict, this is not surprising.

And for good measure, if we decompose overall growth and examine the component reflecting only the addition of new customers (1 + overall growth - net retention), we see little correlation with the recent movement in SaaS stocks:

![](/https://nnamdi.net/content/images/2019/11/image-1.png)

## It’s all about retention

Both in theory and in practice, **better retention drives higher volatility.**

Why does this matter?

The Federal Reserve recently [cut interest rates](https://www.wsj.com/articles/fed-cuts-rates-by-quarter-point-11572458556) for the third time this year.

**The era of low interest rates won't last.** If interest rates or the cost of capital spike, SaaS valuations will necessarily decline.

Keep this is mind when evaluating high-flying SaaS investments or the value of your company's equity: **those with the best retention metrics will see the biggest drops.**

* * *

If you want to comment, like or share this post you can use this tweet:

> With great retention... comes great volatility 💣  
>   
> Volatility increases as SaaS revenue retention improves 📈  
>   
> Retention is therefore a double-edge sword ⚔️  
>   
> How and why companies with strong revenue retention see the greatest volatility 📉:[https://t.co/RIPIjngWPs](https://t.co/RIPIjngWPs)
> 
> — Nnamdi Iregbulem (@whoisnnamdi) [November 4, 2019](https://twitter.com/whoisnnamdi/status/1191487947528269827?ref_src=twsrc%5Etfw)

<div class="subscribe-form">
<h3 class="subscribe-form-title">Receive my new long-form essays</h3>
<p>Thoughtful analysis of the business and economics of tech</p>
<form action="/api/subscribe" method="POST">
<input type="email" name="email" placeholder="Enter your email" required />
<button type="submit">Go ⚡</button>
</form>
</div> ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Waitlists are a Vanity Metric]]>
                        </title>
                        <description>
                            <![CDATA[Waitlists, registered users, and other cumulative measures better measure the size of your pride than the size of your business]]>
                        </description>
                        <link>https://whoisnnamdi.com/waitlists-are-a-vanity-metric/</link>
                        <guid isPermaLink="false">waitlists-are-a-vanity-metric</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Sun, 28 Jul 2019, 19:33:48 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512178757-vanity-4.jpg" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
Waitlists, registered users, and other cumulative measures are vanity metrics.

By vanity – I mean they better measure the size of your _pride_ than the size of your _business_.

First-year MBA operations courses emphasize that long lines should be avoided and certainly not optimized for.

Unless you are in fact a vanity or “prestige” product, bragging about these metrics destroys your credibility in the eyes of potential investors.

Most importantly, in paying them too much attention, you fool, distract, and mislead your most gullible stakeholder – yourself.

> The first principle is that you must not fool yourself – and you are the easiest person to fool – Richard Feynman

Be kind to your potential customers, your investors, and yourself – **ignore these misleading numbers**.

Cumulative metrics are problematic for many reasons. The main two are:

-   They almost never decrease – _by design_
-   They have _zero_ intrinsic correlation to customer value

### Receive my next thought in your inbox

Go ⚡

## If it can’t decline, it’s not worth your time

> To have weight, a metric should be capable of moving in either direction – up or down.

An easily gamed number that can only move in one direction is problematic. Most cumulative metrics have this feature.

A measure that moves up or down with the health of the business is a valuable signal – both in terms of its _level_ and its _change_ (or rather rate of change).

That its level is X, instead of Y or Z, tells us something. That it grew by X% last month also tells us something. That "something" is much more informative when X can move up or down and X% can be positive or negative.

Classic examples include monthly revenue, paying users, monthly expenses per employee, or annual profits.

On the other hand, total registered users – people who’ve created an account for a website or app but don’t necessarily pay for it – never declines unless users un-register themselves. This rarely happens.

In practice, the registered user count grows and grows, with no regard to whether the business is improving. Hence, both its level and its change provide little information.

Waitlists are functionally similar. They exist to signal attractiveness to third parties but are completely manipulable.

The size of a waitlist never declines unless you want it to. The outflow is entirely in your control.

If you never want your waitlist to shorten, _stop accepting new users off the waitlist_. Mission complete.

As such, waitlists are mostly a rebrand of "cumulative registered users".

Having a long waitlist _sounds_ cool, but a waitlist alone is little more than a list of opt-in email addresses saved in a CSV file on your desktop or, better yet, “in the cloud” somewhere.

## A true business imposes costs on its customers and delivers value in return

> The cost customers willfully bear to interact with your business implies value delivered. Those costs historically came in the form of dollars but increasingly take the form of attention or time.

If a customer isn’t giving you their money, attention, or time, **they’re not a customer.**

You can call them registered users, opt-ins, friendly acquaintances – it doesn’t really matter.

But they’re not customers.

Adding oneself to a digital waitlist imposes almost no cost. If anything, it’s a free option – maybe I’ll sign up once the service is available, maybe I won’t.

When a human being accepts a free option, we call that person “rational”. We don’t call them a “customer”.

Like selling dollars for 75 cents, there is no signal in handing out free options.

Flaunting the length of your waitlist is like bragging about raising $10M from investors with no plans to do anything with the money. This is why bankers subtract **cash** from **equity value** when calculating **enterprise value** – quite literally, how much your business is worth.

In the preceding example, your _equity value_ might be $10M, but your _enterprise value_ would be a flat $0. You’ve delivered zero additional value to your investors.

Adding 10,000 people to your waitlist but not doing anything with those users is similarly worthless.

## How to make your waitlist valuable

I know what you’re thinking – “but what if I’m…"

-   Fundraising and want to give investors every possible reason to write a check
-   A fledgling startup without many options for measuring progress
-   Bored and just like staring at numbers (guilty as charged)

Not all is lost – paired with other metrics waitlists can in fact be a signal of business value.

-   **Example:** the proportion of people taken off the waitlist who subscribe or purchase the product

This provides some directional sense of future value – if we let X people off the waitlist we’ll gain Y customers. That Y/X ratio is a **conversion rate**.

Who knows if that conversion rate will hold across the entire waitlist – it probably won’t – but at least we know the waitlist has some value and isn’t pure vanity.

-   **Another example:** charge people for reserving a spot in line like Tesla did for the Model 3

That hundreds of thousands of people were willing to put down a deposit before ever touching, let alone driving, the car was a huge signal.

That spot in line was valuable to customers, and they manifested that value by forking over thousands of dollars years in advance of the launch. Investors, too, saw value, rewarding Tesla with a higher valuation.

Notice a pattern? To make waitlists useful, **link them to customer value**.

## Don’t wait

That slide in your pitch deck where you mention how large your waitlist is? Unless you can tie it to customer value, **delete it**.

Heard about some hot startup with a waitlist 10X as long as yours? **Ignore it**.

By distancing yourself from the raucous noise of waitlists and other cumulative measures – you refocus your attention on the true signal of entrepreneurial success – **delivering value to customers**.

Ship them a product. Provide them a service.

But please, don’t just hand them a ticket with a number on it and call it a day – or a business.

_This post has been published on [www.productschool.com](https://www.productschool.com) communities._ ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[How to Conquer Cohort Analysis With a Powerful Clinical Research Tool]]>
                        </title>
                        <description>
                            <![CDATA[]]>
                        </description>
                        <link>https://whoisnnamdi.com/how-to-conquer-cohort-analysis/</link>
                        <guid isPermaLink="false">how-to-conquer-cohort-analysis</guid>
                        <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Mon, 3 Jun 2019, 12:00:00 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512172481-lucas-vasques-453684-unsplash.jpg" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
**Understanding cohort performance is critical for modern, high-velocity businesses, especially those with subscription or recurring revenue models.**

In SaaS or consumer subscription settings, small changes in churn can [radically impact revenue growth](https://www.forentrepreneurs.com/why-churn-is-critical-in-saas/?fbclid=IwAR3RmfnV8ek0lDyyWwEXdkHNJhJxXymIN-hJeuo32c3cYekTnQOgLvAryi4).

Product managers, growth hackers, marketers, data scientists, and investors all need to understand how business decisions impact user retention.

With so many recurring revenue businesses going public, Silicon Valley _should_ get the picture by now.

Believe it or not, however, **medical researchers measure customer retention better than you do.**

What?

Sounds bold, but it’s not. Over decades, clinical researchers have refined precise and rigorous ways of measuring retention—except instead of _customer_ retention, they measure _patient survival_.

The gravity of life and death means researchers take great care in measuring treatment efficacy.

To do this, clinical researchers use a statistical method called the [**Kaplan-Meier estimator**](https://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator). The formula elegantly solves a frequent issue that pops up in cohort retention analysis: **making valid comparisons within and across groups of cohorts of different lifespans**:

$$ \widehat S(t) = \prod\limits\_{i:\ t\_i\le t} \left(1 - \frac{d\_i}{n\_i}\right) $$

Despite the fancy formula, survival analysis using Kaplan-Meier (KM) is actually quite simple and delivers much better results than other methods:

![4](/content/images/2019/05/4.png)

In this post I'll explain these results, breakdown the KM estimator in simple terms, and convince you to use it for retention analysis.

The bottom-line: **if you are an operator or investor who wants to properly measure customer cohort retention, Kaplan-Meier is the way to do it.**

### Receive my next post in your inbox

Subscribe

## Two inevitabilities: Death and Churn

The core problem the KM estimator helps us deal with is **missing data**.

**Cohort data is inherently flawed in that more recent cohorts have fewer data points to compare against older cohorts**. For example, a five-month-old cohort can only be compared with the first five months of a ten-month-old cohort. The retention rates of a cohort of customers acquired seven months ago can only reasonably be compared to the first seven month retention of older cohorts.

Imagine you had the full retention history of the previous 12 monthly cohorts and you wanted to predict the 12-month retention curve of a newly acquired customer. It’s not at all obvious how to do this.

To understand this better, let’s visualize a simpler example with only five cohorts:

![1](/content/images/2019/05/1.png)

You might first try to calculate average retention across cohorts. This is problematic for two reasons:

-   The simple average will not be representative if our cohorts differ in size
-   For any given month we can only average over cohorts that have been alive at least that long, so we effectively average over fewer and fewer cohorts over time

We can see the second issue below. With both the simple and weighted average, we get strange results when performance oscillates across cohorts:

![2-1](/content/images/2019/05/2-1.png)

Assuming we don’t re-add returning users who previously churned into their original cohort, retention cannot possibly tick up after declining—it’s a one way street. This is an artifact of our flawed method, as 5-month retention cannot exceed 4-month retention by definition.

A third, related problem arises when comparing groups of cohorts to other groups, for example, comparing 2016’s group of monthly cohorts to 2017’s. As we’ve just shown, using averages to estimate retention curves for each group doesn’t work, which means we also cannot compare one group to another.

## Questions? Ask your doctor

![ani-kolleshi-684082-unsplash](/content/images/2019/05/ani-kolleshi-684082-unsplash.jpg)

Believe it or not, clinical researchers deal with this same issue all the time.

Customer cohorts are analogous to groups of patients starting treatment at different times. Here the “treatment” is the time of customer acquisition and “death” is simply churn.

Or, imagine if the “2016 cohorts” and “2017 cohorts”, rather than being year-grouped cohorts, were groups receiving different treatments in a clinical trial. We want to quantify differences in patient survival rates (customer retention) between the two groups.

Pharmaceutical companies and other research outfits regularly contend with this. Patients start treatment at different times. Patients drop out of studies, by dying, but also by moving locations or deciding to stop taking the medication.

This creates a host of missing data issues at the beginning, middle, and end of any patient’s clinical test record, complicating analysis of effectiveness and safety.

To solve this problem, in 1958, a mathematician, [Edward Kaplan](https://en.wikipedia.org/wiki/Edward_L._Kaplan), and statistician, [Paul Meier](https://www.chicagotribune.com/news/ct-xpm-2011-08-18-ct-met-meier-obit-20110818-story.html), jointly created the [Kaplan-Meier estimator](https://www.tandfonline.com/doi/abs/10.1080/01621459.1958.10501452). Also called the _product-limit estimator_, the method effectively deals with the missing data issue, providing a more precise estimate of the probability of survival up to any point.

[The core idea behind Kaplan-Meier](http://biostat.mc.vanderbilt.edu/wiki/pub/Main/ClinStat/km.lam.pdf):

> The estimated probability of surviving up to any point is the cumulative probability of surviving each preceding time interval, calculated as the product of the preceding survival probabilities

That strange formula above is simply multiplying a bunch of probabilities against one another to find the cumulative probability of survival at a certain point.

Where do these probabilities come from? **Directly from the data**.

KM says our best estimate of the probability of survival from one month to the next is exactly the weighted average retention rate for that month in our dataset (also called the [_maximum likelihood estimator_](https://towardsdatascience.com/a-gentle-introduction-to-maximum-likelihood-estimation-9fbff27ea12f) in statistics parlance). So if in a group of cohorts we have 1000 customers from month one, of which 600 survive until month two, our best guess of the “true” probability of survival from month 1 to 2 is 60%.

We do the same for the next month. Divide the number of customers that survived through month 3 by the number of customers who survived through month 2 to get the estimated probability of survival from month 2 to 3. If we don’t have month 3 data for a cohort because it’s only two months old, we exclude those customers from our calculations for month 3 survival.

Repeat for as many cohorts / months as you have, excluding in each calculation any cohorts missing data for the current period. Then, to calculate the probability of survival through any given month, multiply the individual monthly ([conditional](https://www.khanacademy.org/math/statistics-probability/probability-library/conditional-probability-independence/v/calculating-conditional-probability)) probabilities up through that month.

Though a morbid thought, measuring patient survival is functionally equivalent to measuring customer retention, so we can easily transfer KM to customer cohort analysis!

## Putting Kaplan-Meier to the test

Let’s make this clearer by applying the Kaplan-Meier estimator to our previous example.

![3-1](/content/images/2019/05/3-1.png)

The probability of surviving month 1 is **69%** (total customers alive in month 1 divided by total in month 0). The probability of surviving month 2, given a customer survived month 1, is **72%** (total customers alive in month 2 divided by total in month 1, excluding the last cohort which is missing month 2 data). So the cumulative probability of surviving at least two months is 69% x 72% = **50%**. Rinse, wash, and repeat for each subsequent month.

Side-by-side comparison reveals the superiority of KM:

![4](/content/images/2019/05/4.png)

What’s great about KM is it leverages all the data we have, even the younger cohorts for whom we have fewer observations. For example, while the average of all the available cohorts at month 3 only uses the data for cohorts 1-3, due to its cumulative nature, the KM estimator effectively incorporates the improved early retention of the newer cohorts. This yields a 3-month retention estimate of 38%, which is higher than any of the cohorts we can actually measure at month 3.

**This is exactly what we want**—cohorts 4 and 5 are both larger and better retaining than 1-3. Hence, it is likely that the 3-month retention rate for a random customer picked among these cohorts will exceed the historical average, as the customer will likely be in cohorts 4 or 5.

Using all the data is also nice because it makes our estimates of the tail probabilities much more precise than if we could only rely on the data of customers who we retained that long.

Kaplan-Meier curves also fixes the wonky behavior in the right tail of the retention curve by respecting a [fundamental law of probability](https://www.investopedia.com/terms/c/compound-probability.asp): cumulative probabilities can only decline as you multiply more numbers.

## Recommended by 95% of doctors

This analysis could easily be extended. Let’s go back to the 2016 vs 2017 example—we could run the Kaplan-Meier calculation on each respective group of cohorts and then compare the resulting survival curves, highlighting differences in expected retention between the two groups.

While I won’t cover it here, you can also calculate [p-values, confidence intervals, and statistical significance tests](https://math.unm.edu/~james/w4.pdf) for Kaplan-Meier curves. This lets you to make rigorous statements like “the improvement of cohort retention in 2018 relative to 2017 was statistically significant (at the 5% level)”—cool stuff:

![5](/content/images/2019/05/5.png)

**Kaplan-Meier is a powerful tool for anyone who spends time analyzing customer cohort data.** KM has been battle-tested in rigorous clinical trials—if anything it’s surprising it hasn’t caught on more among technology operators and investors.

If you’re a product manager, growth hacker, marketer, data scientist, investor, or anyone else who understands the deep importance of customer retention analysis, the Kaplan-Meier estimator should be a valuable weapon in your analytics arsenal. ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Entrepreneur's Ruin, or How Not to Go Bust]]>
                        </title>
                        <description>
                            <![CDATA[]]>
                        </description>
                        <link>https://whoisnnamdi.com/entrepreneurs-ruin/</link>
                        <guid isPermaLink="false">entrepreneurs-ruin</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Thu, 28 Feb 2019, 00:09:39 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512167670-jonathan-petersson-614702-unsplash.jpg" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
# Managing risk is hard work.

In evaluating potential opportunities, leaders must consider both _probability_ (the odds of success) and _criticality_ (the importance of success).

We can arrange these opportunities on a matrix, with “necessity for survival” on the x-axis and “probability of success” on the y-axis:

![](/content/images/2020/04/matrix-3.png)

In industry, we often talk about “critical processes”—the things that must go right, otherwise chaos ensues (we go bust, our customers abandon us, we lose our best employees en masse, etc).

Business leaders often fall for the seductive fallacy that “if we only maximize the probability of success” of these critical processes, all will be right in the world:

![](/content/images/2020/04/prob-up-3.png)

**This is wrong.**

Why?

Because over long periods, **the only certainty in life is uncertainty.**

Even if we achieve a 99% probability of success in a particular critical activity—with time—the 1% outcome will be realized, and we will go bust.

Take banking for example. Prior to the advent of [deposit insurance](https://www.fdic.gov/deposit/), retail banks went bust with regularity. Any given bank was stable and solvent 99% of the time. However, 1% of the time, some internal or external forcing function would sweep numerous banks into the trash heap overnight.

It didn’t matter that things “worked out” most of the time. Deposits are a capital constraint for the bank. A single run on the bank was enough to bankrupt the firm.

99.9% of days, the banker wakes up at 6am, looks out the window at the bright, rising sun, and goes on with their day.

But inevitably, one evening the banker is rudely roused in the dead of night by the rancorous roar of a mob demanding their money back.

## Winning the battle but losing the war

The fundamental error is in seeing the world as:

-   _static_ (a single game, never to be repeated -> probability of success dominates -> win the battle, even if [pyrrhic](https://en.wikipedia.org/wiki/Pyrrhic_victory)) rather than
-   **dynamic** (repeated games, you need to still be alive in order to play -> survival dominates -> survive the battle, win the war)

This is why it’s foolish to focus solely on maximizing the probability of success of any given “critical” initiative.

At best, maximizing the probability of success clips your potential upside. If you already know something will work, you can never be positively surprised:

![](/content/images/2019/02/ruin-probability-2.png)

Instead, _reduce_ the number of activities that your business is fragile to or dependent on. We need to _cut down_ the number of critical processes—not maximize the odds of their successful completion.

_Survival_ is the only critical process.

Only by reducing the surface area and impact of potential failure, rather than the odds of failure, do we ensure survival.

How?

**Aggressive elimination of single points of failure or vulnerability.**

## Don’t hate the game, hate the constraint

Commentators describe Silicon Valley and entrepreneurship as a “lottery” or “casino” with stacked odds disfavoring any individual entrepreneur or startup.

The issue is not the game itself, however. The problem is ruin—specifically, [gambler’s ruin](https://en.wikipedia.org/wiki/Gambler%27s_ruin).

A gambler facing a series of risky bets under a capital constraint will eventually go broke, with certainty. With enough time, the gambler will hit a bad streak, cratering to zero. This can be true even if each bet has a positive expected value.

Like the banks referenced above, the gambler “needs” capital. If the bank account ever hits zero, it’s game over.

The same is true in business, especially early-stage entrepreneurship.

The reason so many startups go bust is not, as is commonly believed, because they have “low odds of success”.

The reason so many startups fail is because they have too many _needs_.

In other words, startups are **[too needy](https://psychcentral.com/blog/what-does-it-really-mean-to-be-needy/)**.

Many startups today need most or all the following to succeed:

-   Product-market fit
-   Great founding team
-   A+ players
-   Positive unit economics
-   No economic recession
-   Large and growing addressable market
-   Low competition
-   Venture capital
-   Big, marquee, “anchor” customers
-   …

The list goes on.

See why startups are so fragile, why most are doomed to fail?

Companies are constrained by their needs. If your company, startup or otherwise, needs every single one, or even most, of the above items to go well, _it will not be long for this world_.

Eventually, you _will_ fail in some way, and unless you are robust (or even better, [antifragile](https://amzn.to/2D52mHo)) to these risks, you will go bust **with certainty**.

I repeat: **robust, or go bust**.

Hence, **“Entrepreneur's Ruin”** is the natural bias among business leaders to:

-   **overweight maximization of probability and predictability** and
-   **underweight rigorous elimination of single points of failure**

It's the tendency to see business as:

-   **“_one and done_”** rather than
-   **“_one times one times one..._”** [compounding](https://nnamdi.net/you-dont-understand-compound-growth/) ad infinitum

In this multiplicative world, a single zero wipes you out. Prior success is irrelevant. A priori probability or “confidence” of success is also irrelevant.

Like the banker, all that matters is sticking around long enough to see another sunrise.

## Needs vs. nice-to-haves

The companies that survive and thrive over the long-term are the ones that keep this “needs list" as short as possible.

The “best” companies do not have the best team, the largest market, the highest profitability etc.

Ironically, the best companies are the ones that get by:

-   **EVEN THOUGH** they don’t have the best team…
-   **IN SPITE OF** a difficult economic environment…
-   **WITHOUT** access to free-flowing venture capital…

And so on.

By turning “needs” into “nice-to-haves”, exceptional companies cap their downside while retaining exposure to positive surprises, i.e. upside.

Interviewers love to ask executives about their “keys to success”. In response, CEOs proclaim “XYZ is _critical_ to our success” (insert “culture”, “people”, “operational excellence”, etc).

No. **“As little as possible”** should be critical to your success.

If you are asked “what are the keys to your success”, the best answer is—paradoxically—**“nothing”**.

When business leaders, via subtle [humble brag](https://www.merriam-webster.com/dictionary/humblebrag), share their “critical strengths”, they aren’t revealing the keys to their success. **They are writing their own obituary**:

> “**Bankruptly** was a darling Silicon Valley startup admired by all. For most of its tragically short life, Bankruptly had amazing growth, a huge market, and ample capital. But only months after raising a massive round of financing Bankruptly began running into issues. Soon afterward, Bankruptly suddenly…”

You can finish the story.

## Don't be needy

One of the primary functions of modern civilization is to guarantee human needs (air, food, water, shelter, community) and in turn create an unlimited number of nice-to-haves ([oxygen bars](https://en.wikipedia.org/wiki/Oxygen_bar), avocado toast, [LaCroix](https://www.lacroixwater.com/), seasonal homes in Tahoe and the Hamptons, secret societies—you get the point).

With such a multitude of **options**, life goes from nasty, brutish, and short to bougie, fabulous, and, frankly, _too long_ between seasons of Game of Thrones. Wouldn’t it be great to shorten that bit?

Analogously, great business leaders cut down the list of things that “must go right” and expand the list of things that feel great if they happen but are not particularly painful if they don’t.

In other words, **they cap downside while maintaining positive optionality**:

![](/content/images/2019/02/ruin-survival-2.png)

In doing so, they increase the effective “money in the bank” or “reserve capital” of the company (the “gambler”), extending its lifespan and paving a longer runway for the kinds of experimentation and random positive luck (“jackpots”) that so often drive business success.

So do what you want in your personal life, but in managing a business, don't be needy. It'll ruin you! ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[Meet Dev, the Highest-Paid Software Developer in America]]>
                        </title>
                        <description>
                            <![CDATA[]]>
                        </description>
                        <link>https://whoisnnamdi.com/highest-paid-software-developer/</link>
                        <guid isPermaLink="false">highest-paid-software-developer</guid>
                        <category>
                                        <![CDATA[ Developers ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Tue, 15 Jan 2019, 11:48:50 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512164914-luke-porter-107784-unsplash.jpg" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
_**Note: You can find this analysis updated for 2020 [here](https://nnamdi.net/highest-paid-software-engineers-2020/).**_

Dev fell in love with code at a young age.

He graduated from college with an engineering degree and then joined the Navy as a [cyber warfare engineer](https://www.navy.com/careers/cyber-warfare-engineer). Pressured by his highly educated Persian parents, Dev returned to school after military service to complete a [master's in computer science](https://online.stanford.edu/programs/computer-science-ms-degree). He has spent his entire career since in software development, mostly coding in [C++](https://en.wikipedia.org/wiki/C%2B%2B).

He realized early on that workloads were moving to the cloud and spent years retooling and upgrading his skill set, familiarizing himself with each of the major cloud vendors. He championed his team's transition from an on-premise Oracle database to a next-gen database hosted in the public cloud.

Always on the cutting edge, Dev regularly attends employer-sponsored training sessions and hackathons.

Dev worked with all of the major programming methodologies over the years but eventually settled on [Agile](https://www.agilealliance.org/agile101/), becoming a big fan.

Now a manager at a large, 15,000+ employee [technology giant](http://www.piedpiper.com/), he takes time to work out 3-4 times per week at the free company gym. Even after completing an [agile sprint](https://www.atlassian.com/agile/scrum/sprints) he keeps moving - jogging home after work most days.

After promotion, Dev gave his second monitor to one of his younger direct reports. He wasn't spending as many hours furiously coding as he used to, so the extra real estate felt unnecessary.

A big proponent of healthy work habits, last year he procured a full set of standing desks for his entire team and required that all team members eat lunch away from their workstations. Rarely rushed himself, he never skips meals.

During the week, Dev wakes up just after 11am. Every. Single. Day.

Must be nice right?

However, at 50 years of age, Dev worries if his career has already peaked. He has no plans to retire any time soon, but whispers of [ageism in tech](http://blog.indeed.com/2017/10/19/tech-ageism-report/) and seeing his older colleagues being laid off have him wondering if he's next.

That said, he knows he could make up lost income working independently as a freelancer. He has a hunch he'd make even more running solo.

A proud, loving father, Dev hopes his daughter follows in his footsteps. He sees a bright, better future for her. As far as he's concerned, the world is her oyster.

## Who is this guy?

If it's not clear already, Dev's not a real person.

But his personal and professional characteristics really do correlate with higher income.

Age. Gender. Race.

Education. Professional experience. Programming languages.

Methodology. Hackathons. Even how many monitors he uses.

The traits that correspond with higher pay are not always what we would expect - nor necessarily what we'd hope.

The economics of developer pay is an important topic for me. Developers are **the** key input in the production of software. You cannot understand tech unless you understand how developers get paid—**you just can't**.

However, the publicly available analysis of this important issue is quite poor.

## The problem with most pay analyses

[As covered in my last post](https://nnamdi.net/the-growth-share-matrix-of-software-development/), Stack Overflow conducts an annual survey of software developers, asking about various aspects of their careers, like income, job title, etc.

One disappointing feature of most pay analyses is that they show the data without controlling for any other variables. Avoiding any math more complex than simple averages, **the typical analyses of pay have no way to understand a multivariate world**.

[Stack Overflow’s own write-up](https://insights.stackoverflow.com/survey/2018/) of the survey results does this at times, taking the simple average of income across all full-stack developers, for example, and comparing the same metric for DevOps specialists:

![](/content/images/2019/01/stack_overflow_salary-1.png)

Statements like "Engineering managers, DevOps specialists, and data scientists command the highest salaries” are then made, which, though _technically true_, tend to mislead readers who are not well-versed in statistics into thinking that these are _[ceteris paribus](https://en.wikipedia.org/wiki/Ceteris_paribus)_ (all things equal) comparisons ("X makes more than Y, all else equal"), **which they are not**.

To their credit, Stack Overflow at least takes the additional step of comparing these salaries against years of professional experience, but that is just one of many possible controls. In fact, the entire survey provides a rich set of potential controls to “hold equal” and thereby generate more intuitively accurate statements about potential relationships (“X developers make more than Ys who are otherwise similar”, which is a qualitatively and likely quantitatively different statement than “Xs make more than Ys").

Unfortunately, most public discussion of pay does not control for _even a single variable_. This isn’t lying with statistics so much as it is misleading with them.

Said simply—we don’t only want to know “how much more or less do 45-year-old developers make compared to 25-year-old developers.” More importantly:

> “How much does a 45-year-old developer earn relative to a 25-year-old developer who is otherwise equivalent (in skills, experience, age, company size, country etc.)?”

Both are important questions, but to me, the latter is much more interesting and intuitive. It also happens to be much more difficult to answer—hence it is rarely attempted.

This is an attempt do better. Not perfect, but much better.

## Results

I break down the results by survey question, with a chart displaying the controlled effect of each trait on income, in addition to [95% confidence intervals](https://www.khanacademy.org/math/ap-statistics/estimating-confidence-ap/introduction-confidence-intervals/v/confidence-intervals-and-margin-of-error). Correspondingly, any references to statistical significance represent [p-values](https://www.process.st/p-value/) < 0.05.

The results represent a subset of approximately 11,000 U.S-based developers from the Stack Overflow survey.

If you’d like a nicely formatted report with the full set of results, enter your email below and you’ll receive an email with a download link when ready.

### Receive a report with the full results

Send Report

For more detail on the methodology, please see the appendix at the end.

![](/content/images/2019/01/annie-spratt-608001-unsplash-2.jpg)

### Demographics

![](/content/images/2019/01/age.png)

Initially, age increases earnings. That said, the annualized [compound](https://nnamdi.net/you-dont-understand-compound-growth/) gain is quite meager—about 0.2% through 45-54 years of age—with most of that happening before a developer turns 25.

More interesting is how the impact of age declines and turns negative (relative to a developer of 18-24 years of age) as a developer approaches 65:

-   At this level of precision, **I cannot distinguish the pay of a developer 55 or older from an otherwise similar one younger than 25**
-   I can say with reasonable certainty that **a developer over the age of 65 makes less than one of 18-25 years of age** who is otherwise similar.

Ageism could be playing a role. Tech companies, especially startups, are perceived to have a [preference for younger employees](https://www.techrepublic.com/article/tech-companies-admit-to-actively-targeting-younger-workers-for-jobs/). Evidence of ageism is often anecdotal, but this data is at least suggestive that there may be something real behind this concern.

Unfortunately, **ageism is notoriously difficult to prove or disprove**, exactly because it is exceedingly rare to find a 25-year-old who is, in every way other than age, the same as a 65-year-old.

-   For starters, a 25-year-old developer cannot possibly have much more than 5 years of professional development experience, while a 65-year-old developer almost certainly does. And that is just one variable.

This is certainly an issue worth exploring further.

![](/content/images/2019/01/race.png)

Asian and middle eastern developers are paid much more than similar white developers, and the pay _premium_ is likely larger than the pay _discount_ faced by other minorities.

Black developers appear to earn 0.8% less than white developers with similar traits, though with a wide enough confidence interval to lose statistical significance.

-   The lack of precision (plus or minus 5%) is frustrating but inevitable given the low proportion of black software developers in the population and the dataset
-   **Note: Only 1.5% of developers in my dataset are black** (not significantly different than the overall developer population)

Hispanic or latino/a developers earn 1.5% less than similar whites. Again, we see reasonably large confidence intervals due to lack of sufficient data.

That there is such a large pay premium for asian and middle eastern developers is an interesting factoid in itself and one that warrants further exploration

-   The estimates range from 5% to 10%, and all are statistically significant.

The size of these effects are especially impressive given the small proportion of the dataset these minority groups respectively represent:

-   **East Asian—2.8%**
-   **Middle Eastern—0.6%**
-   **South Asian—3.6%**

That they are statistically significant even with few data points suggests these effects are quite real.

[Pay discrimination is a serious issue](https://www.eeoc.gov/eeoc/publications/fs-epa.cfm) that warrants rigorous analysis, much more than what I’ve done here.

-   All in all, the results suggest that attempts to level the “paying” field should also focus on equalizing pay across various racial minority groups, not simply between minority groups and the majority

![](/content/images/2019/01/gender-1.png)

The pay gap for female software developers is similar in magnitude and low statistical significance to that of black developers—roughly 1.3% plus or minus 2.4%.

Gender non-binary / non-conforming developers face a large pay discount of 10% relative to male developers.

Given very few observations in the dataset, the pay effect for transgender developers cannot be estimated precisely enough to conclude anything meaningful.

![](/content/images/2019/01/sexual_orientation.png)

Gay and lesbian software developers appear to make 2.5% more than straight / heterosexual developers, but this estimate is not statistically significant.

In fact, given the confidence intervals above, none of the categories of sexual orientation are statistically significantly different from heterosexual.

-   Again, this is in part due to limited data—only 2.4% of developers in the sample are gay or lesbian, for example

![](/content/images/2019/01/dependents.png)

Good news for parents—developers with dependents (children, grandparents etc.) earn 3.7% more than those who don’t, controlling for other factors.

There are a number of potential explanations for this

-   Workers with dependents likely want / need the extra income and hence seek out jobs that pay more
-   I do not have evidence to conclude that employers are specifically choosing to pay those with dependents more than others, though this could also be the case

![](/content/images/2019/01/military_us.png)

Former and current U.S. military service members earn 3% more than those without prior military service.

As with other traits, there are any number of potential drivers of the veteran pay premium, and it's inherently difficult to pinpoint the most likely explanation.

#### Takeaways

-   Drawing justifiable conclusions around the impact of demographic characteristics on income is necessarily tricky given the highly imbalanced nature of the developer population
-   Caution is advisable when positing causal connections, and these should ideally be accompanied by some theoretical mechanism
-   That said, the results are interesting and should hopefully serve as a starting point for deeper and more rigorous analysis.

![](/content/images/2019/01/vasily-koloda-620886-unsplash.jpg)

### Education

![](/content/images/2019/01/formal_education.png)

Advanced, non-professional degrees drive higher earnings. A masters or doctoral degree (PhD etc.) drives statistically significant gains of 3% and 10%, respectively, relative to an equivalent developer with only a bachelor’s.

Going to college but not completing is associated with a 3% cut in earnings, while graduating with an associate degree decreases earning by about 8% vs. a bachelor’s. Again both are statistically significant.

The effect of professional degrees like JDs and MDs is not statistically significant due to wide confidence intervals.

-   Very few people with these degrees are working as software developers, so establishing a precise estimate is difficult.

Not much can be said for developers that never reached the college level—due to lack of data, the confidence intervals are too wide to draw meaningful conclusions. However, these folks are almost certainly worse off than someone with an advanced degree.

![](/content/images/2019/01/undergrad_major.png)

Interestingly, computer science is not the best-paid college major.

-   In fact, other engineering disciplines (civil, mechanical, electrical, etc.) (**5% increase**), business degrees (accounting, finance, etc.) (**4% increase**), and math / stats majors (**3.4% increase**) all earn more, all else equal.

Outside of these areas, college major is largely irrelevant to developer pay. The meme of the [underpaid, over-educated](https://www.nytimes.com/2015/08/02/opinion/sunday/were-making-life-too-hard-for-millennials.html) sociology major doesn’t apply.

-   This could be encouraging in that it shows you can be paid for good dev work regardless of your field of study
-   With dev schools and bootcamps like [Lambda](https://lambdaschool.com/) popping up all over, this is great news if you are considering a career change and worried you might be at a disadvantage.
-   On the other hand, it does raise the question—why aren’t developers who studied the most apparently relevant field paid more than all others?
-   Does this say anything about undergraduate computer science teaching, which is often criticized for being [irrelevant and out of touch](https://www.cio.com/article/3293010/hiring-and-staffing/10-reasons-to-ignore-computer-science-degrees.html) with professional software development work out in the wild?

The exceptions are information systems / technology and web development / design degrees, which bear a pay discount of 2.4% and 10% respectively.

-   Why? My take is that these degrees are often associated with private, for-profit schools [not often known for their quality](https://www.nytimes.com/2016/09/18/business/itt-educational-services-files-for-bankruptcy-after-aid-crackdown.html)

![](/content/images/2019/01/education_parents.png)

Your parent’s level of education matters as a software developer.

Developers with at least one parent holding a professional degree like a JD or MD will earn 5.3% more than a developer whose parents never earned more than a bachelor’s degree. Masters and doctoral degrees among parents are associated with a 2.2% increase in pay for the children, though this relationship narrowly misses statistical significance for doctoral degrees.

Associate degrees, some college, and high school all have significantly negative impacts of 2.8-4%.

Primary school and no formal education were too rare in the dataset to generate precise effect estimates.

It’s interesting to consider how these effects manifest themselves throughout the professional life of a developer

-   We can all imagine ways in which parental education could have meaningful follow-on effects in the lives of children
-   It is also possible that, with more controls, these effects would dissipate or lose significance
-   For example, the effect may channel itself through income or other variables that correlate with (or are influenced by) education, rather than schooling itself

![](/content/images/2019/01/education_types.png)

Participation in [hackathons](https://hackathon.guide/) is clearly associated with higher pay—more than 4.3% higher, as are full-time developer training programs (bootcamps), which provide a 3% bump.

We’ve witnessed an [explosion in the number of coding bootcamps](https://www.coursereport.com/reports/2018-coding-bootcamp-market-size-research) over the past decade. The pay bump is equivalent in magnitude to a master’s degree, which is incredible given bootcamps take months to complete, not years, and are generally cheaper in tuition than an advanced degree.

Contributing to open source software, or OSS (certainly an educational experience, in a sense), has a positive but not statistically significant bump of 1.6%

-   Open-source penetration of the typical software stack has only increased over the years—it would make sense if active contribution to OSS is rewarded with higher pay.

Industry certifications get a big fat zero. No effect.

On the other hand, online programming courses, MOOCs, etc. are associated with a significant drop of slightly more than 2.4%. Given the [proliferation of online software development courses](https://www.creativebloq.com/web-design/online-coding-courses-11513890) over the years, this is an inconvenient finding.

-   That all said, I do worry about potentially confounding or omitted variables that have not been controlled for
-   I can’t imagine a strong causal link between taking an online course and lower pay, but I can certainly imagine that the "type of person” who takes an online course might also be the type of person that generally earns less

Likewise, the type of person to participate in a hackathon likely has other, unobserved traits that drive higher income

-   Certain companies, which may be higher paying on average, also host hackathons in their offices

In other words, correlation may not be causation here, even after controlling for many variables.

![](/content/images/2019/01/self_taught_types.png)

As an avid self-learner myself, I am disappointed to see that no form of self-learning seems to have a statistically discernible impact on earnings.

-   Books and e-books were close to achieving statistical significance, but even so, the point estimate is only 1%.

Self-learning is a great way to, in targeted fashion, learn exactly what you need to know about a certain topic or area of knowledge. For many, it is more effective than traditional teaching methods.

However, it doesn’t seem to impact pay in and of itself.

#### Takeaways

-   Education matters, largely in the ways one would guess
-   If you’ve already graduated from college, advanced degrees, hackathons, bootcamps, and open source projects can be smart ways to increase your value as a developer in ways that show up in your paycheck
-   If you haven’t graduated yet, know that the exact major you pick matters less than showing interest or commitment to the craft, at least when it comes to pay

![](/content/images/2019/01/nesa-by-makers-701360-unsplash.jpg)

### Professional Characteristics

![](/content/images/2019/01/years_coding.png)

Professional development experience matters much more than casual coding. The gains from more years of general coding experience tend to plateau after 15 years, while professional coding experience continues to pay dividends well into the 30 year range.

The benefit from experience with casual coding (relative to 0-2 years) maxes out at 10-15% and actually begins to decline after 26 years.

-   Again, this is holding other factors equal, including professional experience, so this may make sense
-   Someone who learned to code 30 years ago and has little more to show for it than someone with only 10 years of coding experience might raise flags.
-   We must be careful however—taking the year one learned to code as given, years since learning to code correlates perfectly with age, suggesting that ageism may be leaking in here. I explore age specifically elsewhere in this post.

The fastest gains from professional experience come in the first ten years.

-   A developer with 9-11 years of professional experience can expect to earn 30% more than a newbie, all else equal, which translates to about 2.5 percent year-over-year gains.
-   Of course, “all else” is likely not equal for any given developer across a ten-year span, suggesting even greater potential annual raises for a given developer in the real world.

After the first decade, the line is nearly straight, aside from a mid-career slump at the 25-year mark.

-   Do not be fooled, however—constant absolute gains in fact represent declining growth
-   In other words, each year of professional development experience provides a nearly-constant dollar raise but a declining percentage raise. I explore this phenomenon further in a upcoming post

With that caveat, it is still encouraging to see that developer pay does not fully plateau or reverse course with additional professional experience over time.

![](/content/images/2019/01/employment.png)

No surprise here—working part-time does not help your earnings.

More interestingly, developers working as freelancers or independent contractors make more than those employed full-time

-   This may be compensated for by what is almost surely more variable income
-   In general, stable, full-time employment does come at a cost, not only in software development but in many other areas of the economy

Reverse causation could also be at play here—someone who knows they could make more as a freelancer is exactly the kind of person who would become one.

![](/content/images/2019/01/dev_type.png)
*(Read: “DevOps specialists make 2% more than non-DevOps specialists, all else equal”)*

It’s no surprise that managers and executives earn more. This includes [product managers](https://qz.com/766658/the-highest-paid-workers-in-silicon-valley-are-not-software-engineers/), who might not write significant amounts of code themselves.

-   Engineering managers and C-suite execs make 10% more, while product managers make 5.8% more, all else equal.

The only non-manager role that makes meaningfully more than its counterparts is DevOps:

-   DevOps specialists are about 2.2 percentage points higher on the pay scale than the non-DevOps average for similar developers.

Conversely, there are a number of roles that make noticeably less despite similar levels of experience, education etc.

-   Database admins (DBAs), sysadmins, designers, QA / testing developers make 2.5-7.5% less than developers who don’t fall under these categories
-   With the exception of designers, these roles are commonly seen as the “back office” of IT, though one could easily find counter examples depending on the specific team or organization
-   Perhaps needless to say—academics and students earn meaningfully less than those outside of academia

#### Takeaways

-   Professional experience matters and never stops mattering
-   Casual development experience helps too—but only so much, measured both by the amount of experience and the earnings impact
-   Among those working in industry, role matters positively for managers and DevOps professionals and negatively for system and database administrators, as well as designers

![](/content/images/2019/01/ilya-pavlov-87438-unsplash.jpg)

### Languages, Frameworks, Databases, Platforms, and Tools

![](/content/images/2019/01/languages-1.png)
*(Read: “Developers who have done extensive development work involving Golang earn more than those who haven’t”)*

The main implication of these results—languages largely don’t matter to pay. The vast majority of languages do not provide a statistically significant pay bump or discount relative to jobs that don’t require that language. The pay scale for programming languages is quite flat across the universe of languages.

Hack is a clear outlier—working with Hack yields a 25% bump. However, don’t drop everything to go learn Hack right now

-   Hack is not a widely used language
-   It was developed by Facebook, and unlike other languages and frameworks that come out of the big tech giants, Hack has not had the same marketing push placed behind it.
-   The effect we see here likely reflects developers who work at Facebook itself—there’s no sign of a robust hiring market for Hack talent.

Outside of Hack, on the positive side we find Objective-C, C++, Go, VBA, and TypeScript with statistically significant income effects, ranging from 2.1 to 3.9 percent.

On the negative side we have JavaScript, VB.NET, CSS, and PHP

-   JS, CSS, and PHP likely reflect the lower pay of certain web development jobs
-   These languages represent fundamental technologies of web development
-   For that same reason however, they are considered table stakes
-   Hence, using these technologies on the job doesn’t provide much of a pay bump, though you may be quite employable

Please note that the results above are **not** saying “knowing language X increases / decreases pay.” Knowing a language is not likely to ever _harm_ one’s pay, but working a dev job that uses it might (relative to other development work).

![](/content/images/2019/01/frameworks.png)

React—what needs to be said?

-   The [cutting-edge JavaScript framework](https://reactjs.org/) originated and maintained by Facebook is widely and wildly popular, especially among front-end developers looking for an easy-to-use but powerful way to craft compelling user interfaces.
-   React is associated with a 4.1% pay increase and is the only framework with a statistically significant positive impact.

Working with .NET Core and PyTorch negatively impacts income

-   I do not have a hypothesis about .NET Core, but I do know that [PyTorch was historically used much more among researchers](https://code.fb.com/ai-research/announcing-pytorch-1-0-for-both-research-and-production/) than industry practitioners, likely driving lower pay
-   Usage has since branched out as the framework has added production-grade capabilities and tooling.

![](/content/images/2019/01/databases.png)

One word—cloud. Almost all of the databases with positive income effects are hosted in the cloud, often exclusively as a managed service by one of the major public cloud service providers (Google, Amazon, Facebook).

Google’s BigQuery data warehouse tops the list, followed closely by Amazon’s competing Redshift. Apache Hbase and Hive come next. Afterwards it’s Amazon’s DynamoDB and Microsoft’s Azure Tables, Cosmos DB, and SQL. The list continues with Memcached, Redis, Elasticsearch, Google Cloud Storage, and Amazon RDS and Aurora—all positive and statistically significant.

Older database technologies from legacy vendors dominate the negative end. This includes Oracle’s once ubiquitous databases, MySQL, SQL Server, MariaDB, and IBM’s Db2.

It’s fascinating to see the impact of the cloud revolution laid out in stark relief. There are meaningful income gains associated with working with next-gen cloud-enabled databases. This fact should be top of mind for developers looking to upgrade their skills and pay.

![](/content/images/2019/01/platforms.png)

I’ll say it again—cloud. The various public clouds are the only platforms aside from Windows Phone with positive, statistically significant effects on developer earnings. Azure is associated with 4.1% higher pay, GCP 2.7%, AWS 1.7%.

And Windows Phone? Not going to try to explain that one.

Android, Arduino and Drupal had meaningful negative association with income—2.4%, 4.6%, and 8.5% respectively.

Android’s ubiquitous operating system needs no introduction, though it’s negative pay relationship might need an explanation. Unfortunately, I am stumped here.

Arduino is an [open-source prototyping platform](https://www.arduino.cc/) with associated hardware microcontrollers that eases development of electronic devices. Though used for serious development work, [Arduino is also heavily used by engineering students](https://www.arduino.cc/en/main/education) as well, which likely drives the negative value.

Drupal is a PHP-based content management system whose [popularity peaked in 2011](https://trends.google.com/trends/explore?date=all&geo=US&q=drupal) but has since been on a slow decline.

![](/content/images/2019/01/ide.png)

You might not think that a developer’s choice of IDE or text editors wouldn’t matter—and you’d be (largely) correct! The associated effects here are generally quite small.

However, IntelliJ stands out from the pack

-   Billed by JetBrains, its creator, as the “Java IDE for professional developers”, IntelliJ users make 3.1% more than non-users
-   Of course, this is not an entirely useful comparison for someone who doesn’t use Java in their development work

Notepad++ and Atom are associated with slightly lower earnings, on the order of 2%, which is statistically significant. Notably, both are popular but are generally viewed more as text editors than fully-fledged IDEs.

PhpStorm, Xcode, and Coda also had negative effects that were statistically significant.

![](/content/images/2019/01/methodology.png)

The incredibly popular Agile leads the pack among project management methodologies. Devs who use Agile in their development work earn 3.9% more than those who don’t.

The less common extreme programming methodology is associated with 2.7% higher pay.

Lean comes in at 2.5%, while Scrum developers earn 1.5% more, all else equal.

Prince2 appears to be at outlier, but this is mostly due to limited data—only 0.1% of developers in the sample use the methodology.

![](/content/images/2019/01/version_control.png)

For the most part, the exact form of version control used does not matter.

The key takeaway here—any kind of version control is better than no version control, but please, please avoid copying and pasting files to a network share.

![](/content/images/2019/01/check_in_code.png)

The best practice appears to be the best paid

-   Checking in code multiple times per day
-   It’s clearly important to check in code on some kind of cadence—at least monthly, ideally
-   If you are checking in code less than once per month or not at all, your pay is likely being impacted as a result.

Not to sound like a broken record, but the reverse causation caveat applies here as well. It is completely plausible that checking in code more often increases a developers pay. It’s also possible that companies that pay more are more likely to follow and mandate development best practices, such as frequent code check-ins.

![](/content/images/2019/01/communication_tools.png)

It seems too convenient that in a survey run by Stack Overflow that its own enterprise product would be associated with significantly higher developer pay, but I can only go where the data takes me. Developers using Stack Overflow Enterprise earn 9.4% more than those who don’t.

This likely picks up some effect of simply working at a company that uses Stack Overflow Enterprise, which may be higher paying on average.

Other communication tools had smaller positive impacts like the incredibly popular Slack, Atlassian’s Confluence, HipChat, and internal intranet sites (wikis, Google sites etc), which all have earnings effects in the 2-3% range.

Trello and Facebook are negative enough to be statistically significant.

![](/content/images/2019/01/number_monitors.png)

Surprisingly, developers who use two monitors earn 2.8% less than those who use only a single monitor, all else equal.

At this time, it’s not clear why more monitors would negatively impact earnings. The potential for enhanced productivity may be swamped by the temptation to use the additional screen real estate for [non-work activities](https://www.reddit.com/).

No other monitor count had a statistically significant effect on income relative to one.

#### Takeaways

-   The tools associated with higher developer pay are quite interesting and not necessarily what one might expect
-   In some cases, the most popular tools also pay the most
-   In other cases, more obscure tools appear to have an advantage
-   Check in your code (at least every once in a while)!
-   One clear trend is the impact the move to the cloud is having on developers—the effects of the public cloud on developer pay are large and consistently statistically significant across the big 3 U.S. clouds
-   Knowing how to use and leverage these next-generation computing environments and finding a job that employs those skills can drive meaningful pay improvements for the average developer

![](/content/images/2019/01/marion-michele-191320-unsplash.jpg)

### Work Life, Health, and Wellness

![](/content/images/2019/01/wake_time.png)

These results are admittedly difficult to interpret. There is no clear linear time trend for the impact of wake up time on earnings.

After 7am, later does appear to be somewhat better—up to a point. Strangely, developers between 11am and 12pm see 15.4% higher pay than those who are up before 5am.

The most important takeaway is this—**have a set schedule**. This will do more good than optimizing for a specific wake-up time. Not having a regular wake up time was associated with 7.7% lower pay for software developers.

![](/content/images/2019/01/exercise.png)

Exercise is strongly associated with earnings. While only exercising once or twice a week does not appear impactful, exercising 3 times or more per week is associated with 2-2.9% higher pay than a similar developer who does not exercise at all.

It is possible however that reverse causation could cause developers who earn more to work out more—perhaps because they have more time or can more easily afford a gym membership. Alternatively, higher paying companies often have gyms on-premises, making it easier to work out more.

![](/content/images/2019/01/skip_meals.png)

Don’t skip meals. Any amount of meal skipping was associated with lower pay, though never statistically significant. Developers are not rewarded for this unhealthy work habit.

![](/content/images/2019/01/ergonomic_devices.png)

Among ergonomic devices, [standing desks](https://thewirecutter.com/reviews/best-standing-desk/) were associated with 3.2% higher pay, which is statistically significant.

Again, higher paying companies are potentially more like to provide employees with standing desks, so the direction of causality here is questionable.

![](/content/images/2019/01/hours_computer.png)

Time spent at the computer each day does not have a meaningful relationship with pay. Handcuffing yourself to your laptop is not going to earn you higher pay as a developer.

![](/content/images/2019/01/hours_outside.png)

Unlike time spent at the computer, time spent outside does have an impact on pay, with the ideal amount being 1-2 hours. Spending fewer than 30 minutes outside is associated with 2.4% lower pay than 30 minutes to an hour.

Spending more than 4 hours outside was associated with lower pay, but there were not enough developers who do this regularly to generate a precise estimate.

#### Takeaways

-   The data suggest that common best practices are often the best way to go
-   Spend at least a small amount of time outside each day
-   Skipping meals will only grow your bank balance to the extent you save money on lunch
-   Exercising a few times per week is better than never hitting the gym

![](/content/images/2019/01/hans-peter-gauster-252751-unsplash.jpg)

## Conclusion

There are many important takeaways from the study and the charts above.

This analysis is a first attempt at exploring the various factors that affect developer pay. To that end, I hope that it is illuminating and informative.

However, as with many attempts to answer difficult questions, the analysis raises as many questions as it answers.

I am publishing the full code to reproduce this analysis because I believe open source, replicable research is the key to the robust advancement of knowledge

-   I would love for this analysis to serve as starting point for others who wish to elevate the state of knowledge on this important topic
-   If you find an error or disagree with some aspect of the analysis, feel free to submit edits (pull requests) to my [GitLab](https://gitlab.com/whoisnnamdi/highest-paid-software-developer) or [GitHub](https://github.com/whoisnnamdi/highest-paid-software-developer) repositories

I care deeply about the technology industry. But solving its issues and compounding its strengths demands a rigorous understanding of its component elements. [Developers are a critical piece of the tech puzzle](https://www.cnbc.com/2018/09/06/companies-worry-more-about-access-to-software-developers-than-capital.html), and they deserve our attention.

## Appendix

## Data

As in my last post, I leverage data from Stack Overflow’s annual software developer survey, which asks about income, in addition to many other questions of interest.

Examples include

-   Which of the following best describes the highest level of formal education that you’ve completed?
-   Approximately how many people are employed by the company or organization you work for?
-   Which of the following programming, scripting, and markup languages have you done extensive development work in over the past year?

I’m interested in how answers to these questions affect income. While it’s impossible to completely avoid issues of reverse causality or correlation rather than causation, by using regression augmented with machine learning techniques, described here, we can have greater confidence that our results accurately represent the true relationship income.

Essentially, we’ll analyze how each possible answer affects income, holding all other answers constant.

I limit the dataset to only non-retired US respondents above the age of 18 with income between $10,000-250,000. Responses above $250K have a higher tendency to be troll (i.e. made up) responses, which I’d like to exclude, and few answers come in above this threshold regardless.

This leaves us with a dataset of approximately 11,000 developers.

### Methodology

I estimate the following equation on the data:

$$\log(income) = \beta\_0 + \beta\_1T + \beta\_2X + \epsilon$$

Where \\(T\\) is our trait of interest, \\(\\beta\_1\\) is the effect of that trait on income relative to the base category, \\(X\\) is a set of controls (in our case, the respondent's answers to other questions in the survey), \\(\\beta\_2\\) is the set of effects for each respective control, and \\(\\epsilon\\) is the irreducible error in our estimate.

Assuming we’ve included a “complete” and “correct" set of controls, this should provide a reasonably accurate estimate of the relationship of the trait of interest, \\(T\\), with income. The log transformation of income means our results will be in roughly percentage terms, implying that trait \\(T\\) is associated with an increase in income of \\(\\beta\_1\*100\\%\\) relative to the “base" category, all else equal. The base category will vary by trait.

Selecting the right set of controls is non-trivial. One has any number of degrees of freedom to select a subset of controls among all those available, opening the doors to “p-hacking” and other infamous behavior, which can lead to incorrect (biased) estimates of the parameters of interest.

To avoid this, I leverage a powerful machine learning technique, Double Selection (specifically Double Lasso Selection), to do principled covariate selection for each trait \\(T\\), rerunning the regression for every trait. As described in [Belloni, Chernozhukov, Hansen (2011)](https://arxiv.org/abs/1201.0224), this should provide a more accurate estimate of the income effect of each trait than simply using all the covariates or attempting to manually select a subset. I won’t cover the method in detail here, but refer to the original paper for more information. [This post](https://medium.com/teconomics-blog/using-ml-to-resolve-experiments-faster-bd8053ff602e) provides a relatively intuitive explanation, and is also where I originally learned of the technique.

Long story short, Double Selection makes us much more confident that the results represent the accurate effects.

_This post has been published on [www.productschool.com](http://www.productschool.com) communities_ ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[The Growth-Share Matrix of Software Development]]>
                        </title>
                        <description>
                            <![CDATA[]]>
                        </description>
                        <link>https://whoisnnamdi.com/the-growth-share-matrix-of-software-development/</link>
                        <guid isPermaLink="false">the-growth-share-matrix-of-software-development</guid>
                        <category>
                                        <![CDATA[ Developers ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Thu, 22 Nov 2018, 23:45:00 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512163462-growth_share_matrix_no_title.png" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
**Human capital is our greatest asset.**

Like financial capital, [the all-powerful force of compound growth](https://nnamdi.net/you-dont-understand-compound-growth/) means that a small difference in the rate of skill acquisition over time can lead to massive differences in career outcomes.

Choosing which skills to hone is therefore one of the most important keys to professional growth and success in any arena.

Among various forms of human capital, **technical aptitude** is quickly becoming **the** mission-critical skill for 21st century knowledge work.

However, there are any number of technologies that one could dive deep into and attempt to master, with varying usefulness and practical applicability.

**So how does one decide where to "invest" among a sea of options?**

A mental model I've found surprisingly helpful for this task is the [growth-share matrix](https://www.economist.com/news/2009/09/11/growth-share-matrix), a framework concocted 50 years ago by the Boston Consulting Group.

The framework was originally conceived as a tool to help executives prioritize different business units based on their respective relative market shares and growth. The two dimensions separate the market landscape into quadrants, each with certain characteristics:

-   Stars (High Growth / High Share)
-   Cash cows (Low Growth / High Share)
-   Question marks (High Growth / Low Share)
-   Dogs (Low Growth / Low Share)

BCG advised clients to invest in the stars, exploit the cows for their cash flow, evaluate the potential of the question marks, and exit or sell the dogs ASAP.

The growth-share matrix was originally intended to apply to product lines or business units - an asset a corporation could _own_. In that respect, you might imagine this framework has limited applicability to programming languages, given no single person "owns" any given language.

Not so fast!

I argue that we each _do_ own a little piece of a programming language, not in the form of equity or stock, but in the form of **human capital**.

Through careful curation of a "portfolio" of useful skills, we earn a return on our learning efforts - rewards for our time and effort.

Learning a programming language is a _perfect_ example of this.

But what makes a programming language _useful_?

**The value of any given programming language is a direct derivative of the number of other individuals who know that language and the number of companies using that language to develop and ship products.**

That may sound obvious to some, but it is in fact quite counter-intuitive.

In most of life's skills we seek to master the rare things - the things no one else can do. We think that by differentiating ourselves through a unique set of talents we will shine brighter in an increasingly competitive world. Learning that which is rare will pay meaningful dividends, so the thinking goes.

While yes, knowing an obscure language that few others have familiarity with might carve out a nice niche in the market for you to charge highly for your rare talents, I would argue that, for most, it is actually _more_ valuable to know a language that **lots of other people also know**, rather than one only a few have ever worked with.

Most people think that programming is how you speak to _computers_. Really, it's how you speak to **other developers**.

Due to network effects and the increasing size, scale, and scope of software development projects and teams, knowing the "[lingua franca](https://en.wikipedia.org/wiki/Lingua_franca)" is **much more** valuable than being an expert in some endangered language, soon to be discarded to the trash bin on the desktop of history.

Paradoxically, having low _personal_ market share in a high market share _language_ is actually not such a bad thing.

## Building the Matrix

![Matrix movie still](https://images.unsplash.com/photo-1526374965328-7f61d4dc18c5?ixlib=rb-0.3.5&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=1080&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ&s=59773981c4a4762fe474590959ddf064)

To construct the growth-share matrix for programming languages, we will leverage StackOverflow's [annual developer survey](https://insights.stackoverflow.com/survey/). For the [2018 edition](https://insights.stackoverflow.com/survey/2018/), they surveyed over 100,000 developers from around the world, covering a wide range of topics from job satisfaction to salary. Here we'll focus on US-based developers.

The key question for our purposes is:

> "Which of the following programming, scripting, and markup languages have you done extensive development work in over the past year?”

Answers to this question should give us a rough proxy of the popularity of any given language, as defined by the proportion of developers who have worked with a particular language.

For growth, we can compare the answers to this question across 2017 and 2018 to come up with an estimate of the growth of each language. We'll define growth as the % growth rate of the proportion of respondents who've worked with the language in the past year. So, a language that went from 10% coverage to 13% would be considered to have grown 30% (rather than 3 percentage points).

Quadrant boundaries will be set at the median growth rate and relative market share.

One final piece - as is convention with the growth-share matrix, we will show market share relative to the language with the most market share. Therefore, the axis will end at 100% (representing the most popular language). We will also show this on a log scale to better showcase the distribution, which tends to be quite crowded below 10% relative market share.

We now have all we need to build our growth-share matrix!

## Inside the Matrix

Take a look at the results:

![](/content/images/2018/11/growth_share_matrix-3.png)

One striking feature that immediately jumps out - very few languages saw a net decline in popularity. **Almost every language grew**, which by definition implies that the average developer is using an increasingly wide array of languages in their work.

Let’s spin through each quadrant and discuss some of the highlights.

 #mc\_embed\_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; width:100%;}

Receive more long-form posts on the business of tech 

## Stars

![pink star ornament decor](https://images.unsplash.com/photo-1481015172496-8cfcb0d85e59?ixlib=rb-0.3.5&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=1080&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ&s=c67577076a4f8934b61e78fa16a91909)

**_Python_** _-_ [Python has existed for decades](https://en.wikipedia.org/wiki/Python_\(programming_language\)) but only recently hit its stride as a go-to language for data analytics and machine learning use cases. Python is widely-regarded as one of the best languages for data-driven analysis given its relative ease of use and massive set of open source libraries that simplify and accelerate analytics. Python's syntax is quite simple compared to other languages. Ease of use and speed are especially important in data science, as data scientists often run and re-run numerous iterations of a model before settling on a preferred specification. The growing popularity of interactive and replicable computing environments like Jupyter notebooks dovetails nicely with Python's surging share among developers. I'm personally quite pleased to see Python's high popularity given I've spent the past 2 years self-teaching myself the language!

**_Ruby_** _-_ Ruby has historically been known for its extreme ease of use and strength within web development. Many a web developer wrote their first web app in Ruby. [The Ruby on Rails framework](https://rubyonrails.org/) only extended this user-friendliness further, making Ruby incredibly popular among developers who want a no-frills way to quickly develop and deploy functional web applications. For several reasons however, Ruby's growth is slowing and has been for a few years now. [No, Ruby is not “dead”](https://www.techrepublic.com/article/the-death-of-ruby-developers-should-learn-these-languages-instead/), but it will likely migrate to the cash cow zone soon as the initial fanfare wears off. Ruby continues to be a great language that serves developers well.

**_Go_** _-_ A new language seeing [rapid adoption](https://medium.com/@kevalpatel2106/why-should-you-learn-go-f607681fad65) among developers, Go simplifies the process of writing code, thereby making developers more efficient. Go was initially birthed at Google, where technical teams were trying to solve engineering problems that only seemed to be multiplying in an era of increasingly large codebases, multicore processors, and network-aware applications. Go is built with concurrency in mind, making it relatively easy to build multi-threaded applications. Outside of Google, major companies making use of Go include Uber, Netflix, Adobe, IBM, Intel, Dropbox, CloudFlare, and more.

**_TypeScript_** _-_ I debated including TypeScript as its own language here given its strong similarities to and overlap with JavaScript, but developers with experience in the language seem to be a distinct group worth highlighting. The language has also seen a [surge of growth](https://thenewstack.io/typescript-getting-popular/) in the past few years. The fundamental goal of TypeScript is to ease development of large-scale applications that would otherwise be written in vanilla JavaScript. Accordingly, TypeScript is a superset of JavaScript that also compiles to simple JavaScript. Why the distinction then? Typescript adds a number of features to core JavaScript common to other languages, such as classes and modules, in addition to strong typing, generics and interfaces. TypeScript is developed and maintained by Microsoft.

**Takeaway** - These languages are highly popular and would constitute a solid foundation any budding developer or product manager. If you don't already have basic proficiency in at least some of the stars - I implore you: **learn these growing tools of the trade**.

## Cash Cows

![black and white dairy cow on green grasses during daytime](https://images.unsplash.com/photo-1446126102442-f6b2b73257fd?ixlib=rb-0.3.5&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=1080&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ&s=7147b447746e0bd07fed39b36de70c9a)

**_JavaScript_** _-_ [JavaScript has become the go-to language](https://www.simplytechnologies.net/blog/2018/4/11/why-is-javascript-so-popular) for modern web development, with a number of [spin-off frameworks](https://raygun.com/blog/popular-javascript-frameworks/) that leverage its core elements. JavaScript is more popular than Java today, due to the ubiquity of web applications today and the move SaaS and other web-based models for application consumption. Here, JavaScript is leading the charge, and the numbers reflect that. However, it should be noted that, despite similar nomenclature, [Java and JavaScript are not closely related](https://www.geeksforgeeks.org/difference-between-java-and-javascript/) (it's a long story). Both are object-oriented, but the similarities end there.

**_Java_** - Java has long been a popular language for cross-platform development, and this flexibility has continued as new platforms have emerged, such as mobile. One of Java's many conventions is the idea of "[write once, run anywhere](https://en.wikipedia.org/wiki/Write_once,_run_anywhere)", meaning that code written in Java can be run on any other platform that supports Java with no recompiling. When complete, Java applications are compiled into [bytecode](https://en.wikipedia.org/wiki/Bytecode) which runs on a Java Virtual Machine. Originally built by Sun Microsystems, through acquisitions it's ended up in the hands of Oracle today.

**_SQL_** _-_ SQL (Structured Query Language) is an old workhorse that needs no introduction. It has existed for quite some time and is the main means by which analysts query and pull data from relational databases and data warehouses. Despite the popularity of “[NoSQL](https://www.mongodb.com/nosql-explained)” and other non-relational frameworks, [SQL remains king](https://raygun.com/blog/popular-javascript-frameworks/), and in recent years many of these other frameworks have bolted on SQL-like interfaces in order to ease data extraction and transformation. As companies collect data from a greater range of diverse sources and continue to store this information in central databases, SQL will only increase in importance.

**_The C Family_** _\-_ No big surprise here - the extended family of C languages has held a strong position within the software development community for some time and continues to serve as the backbone for many critical applications we know and love today. Further, C has found its way into other languages as well. For example, the reference implementation of Python, CPython, is written in C and Python, and significant chunks of the core Python codebase are actually written in C due to it being a compiled (rather than interpreted) language and thus having faster performance at runtime. [C is a hugely influential language](https://stackify.com/popular-programming-languages-2018/) that will not be going away any time soon.

**_PHP_** _-_ PHP lands squarely in the cash cow category. PHP is a server-side scripting language primarily suited for web development, as evidenced by its original meaning of “personal home page". [Numerous popular websites and web applications are built on PHP](https://en.wikipedia.org/wiki/Programming_languages_used_in_most_popular_websites), including, perhaps mostly famously, WordPress. However, the language has stagnated in terms of popularity, in part to due to its [clunkiness](https://eev.ee/blog/2012/04/09/php-a-fractal-of-bad-design/) and security vulnerabilities, where PHP has historically suffered from a number of severe exploits (ex: [SQL injection](https://en.wikipedia.org/wiki/SQL_injection)). That said, this is another language with incredible market share that will continue to see broad use for quite some time.

_**Swift**_ - The popularity of Swift derives directly from the underlying popularity of macOS and iOS devices which, though a minority of overall smartphone shipments, represents a massive install base, especially among more affluent western populations. Launched in 2014, Swift initially saw massive growth, [becoming one of the fastest growing languages in history](https://9to5mac.com/2018/03/09/swift-ranking-programming-languages/). Swift is heavily influenced by Objective-C, another cash cow, which it recently surpassed in popularity. As the brainchild of Apple, Swift will live or die by Apple's own success, so plan accordingly.

**Takeaway** - These languages really pay the bills. If you are already proficient in any of the above languages, great, leverage that saved time to pick up some skills in the rising stars. If you do not know these languages well today, evaluate how practical / necessary they are for the specific set of projects you want to work on now or in the near future.

 #mc\_embed\_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; width:100%;}

Receive more long-form posts on the business of tech 

## Question marks

![question mark neon signage](https://images.unsplash.com/photo-1484069560501-87d72b0c3669?ixlib=rb-0.3.5&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=1080&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ&s=8248324d302ce16e516054824000b147)

**_Rust_** _-_ Rust is a relatively new programming language that only appeared on the scene in the last decade. While Rust is technically a general-purpose language, due to its low-level nature, it is best used for embedded systems running close to bare metal. Comparisons are often made between Rust and C++, in part driven by their syntactic similarities. [Rust is often known to create enthusiastic fans among its users](https://medium.com/mozilla-tech/why-rust-is-the-most-loved-language-by-developers-666add782563). Though far from being one of the more popular languages, it is truly loved by the people who use it most. Development on Rust is quite active today, ensuring the language will stay on the bleeding edge for the foreseeable future.

_**Scala** -_ Like Go, Scala is language oriented towards improving developer productivity. The name Scala is a portmanteau of "scalable" and "language", which hints at original intent of the language to enable high performance of large-scale applications and userbases. Scala is built on JVM and JavaScript runtimes and combines elements of object-oriented and functional programming. Due to these strong connections, Scala is often seen as "next-gen" Java. Scala is uniquely suited for parallel and distributed computing, providing a level of future-proofing that many legacy languages lack. Though popular among a certain subset of developers, its growth appears to have [prematurely slowed](https://dzone.com/articles/the-rise-and-fall-of-scala) relative to languages like Go or Rust. Its boosters hope that Scala may one day overtake Java, but this won't happen for some time, if ever.

_**R**_ - R slightly missed the cutoff for star status, but given its incredible ~40% growth rate the language will easily cross the boundary next year. R is exploding in popularity for the same reasons as Python, though [most consider Python to be relative winner](https://www.datacamp.com/community/tutorials/r-or-python-for-data-analysis) in terms of speed, ease of use and general applicability. R's historical strength in data science and statistical analysis is now powering a major renaissance for R. Enthusiasts celebrated [R's 25th anniversary](https://blog.revolutionanalytics.com/2018/08/r-generation.html) earlier this year, and with the helpful tailwind of data science, the language shows no signs of slowing down.

_**Haskell**_ - Function over form, or in the case of Haskell, have both. Haskell is a [purely functional](https://en.wikipedia.org/wiki/Purely_functional_programming) programming language, meaning that the language focuses on functions that take immutable values as input and produce the exact same output every single time. It's also [lazy](https://en.wikipedia.org/wiki/Lazy_evaluation), which simply means results are not evaluated until absolutely necessary. These and other features make Haskell a very powerful and efficient language in the right hands but also potentially limit its applicability. Haskell's cult following is growing rapidly from its small base, but it's hard to say how long this will continue.

**Takeaway** - They're called _question marks_ for a reason. No one really knows how the future will play out for these emerging technologies. They are probably not worth betting the farm on today, but they are also prime candidates for becoming the next "must-know" tools among forward-looking dev teams. Keep an eye on them.

## Dogs

![](/content/images/2018/11/doge.jpg)

**_Visual Basic (All Flavors)_** \- VB.NET, VBA, VB6 - whichever your flavor, the Visual Basic ecosystem has clearly fallen from grace. VB.NET is one of only two languages in the growth-share matrix to actually lose share in 2018. [Significant chunks of VB's functionality exist in C# now](https://arstechnica.com/information-technology/2017/02/microsofts-developer-strategy-c-for-fancy-features-visual-basic-for-beginners/), and Microsoft's stance towards the languages has not been 100% clear, having gone from originally planning to end support for the language in 2008 to recently declaring that [Windows 10 will support the VB runtime](https://docs.microsoft.com/en-us/previous-versions/visualstudio/visual-basic-6/visual-basic-6-support-policy) for the lifetime of the OS. This is great for legacy applications built using Visual Basic, but these will inevitably need to be rewritten in a modern language or be end-of-lifed.

**Takeaway** - Unlike real dogs, dogs within the growth-share matrix are bound to be controversial. Developers and development teams need to seriously grapple with the current state of affairs these languages face and whether or not it's advisable to spend significant time and resources building applications powered by these less popular languages.

## (Human) Capital Allocation

![person holding pen](https://images.unsplash.com/photo-1523006520266-d3a4a8152803?ixlib=rb-0.3.5&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=1080&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ&s=fc3154037ad3cd02837b7591d6dc1424)

The moral of the story - **think critically about where to invest your time**.

To be clear - it’s not the end of the world if you pick the “wrong” language. In fact, there really aren’t any wrong choices here, even the "dogs". **Use the best tool for the job.** However, it certainly helps to avoid the transition costs inherent in trying to reposition oneself or play catch up later on.

If anything, don’t try to reposition yourself _per se_, but rather, seek to enhance your overall value and breadth of capabilities by acquiring at least intermediate mastery in several different languages. Again, similar to spoken languages, people who can converse in multiple valuable languages often gain disproportionate value from their learning efforts, which tend to [compound on one another](https://nnamdi.net/you-dont-understand-compound-growth/), especially when learning the basic features which form the building blocks of many dialects.

Remember, this mental model, though imperfect, is arguably flexible enough to accommodate many skills, _not just programming_.

I hope this framework is useful to you as you decide where to grow your human capital as a technically savvy individual.

_You can find the full backup to this analysis in both Jupyter notebook and .py script format at my [GitLab](https://gitlab.com/whoisnnamdi/growth-share-matrix) or [GitHub](https://github.com/whoisnnamdi/growth-share-matrix)._

_This post has been published on [www.productschool.com](http://www.productschool.com) communities_ ]]>
                        </content:encoded>
                    </item>
                    <item>
                        <title>
                            <![CDATA[You Don't Understand Compound Growth]]>
                        </title>
                        <description>
                            <![CDATA[]]>
                        </description>
                        <link>https://whoisnnamdi.com/you-dont-understand-compound-growth/</link>
                        <guid isPermaLink="false">you-dont-understand-compound-growth</guid>
                        <category>
                                        <![CDATA[ Founders ]]>
                                    </category>
                                <category>
                                        <![CDATA[ Investors ]]>
                                    </category>
                                
                        <dc:creator>
                            <![CDATA[ Nnamdi Iregbulem ]]>
                        </dc:creator>
                        <pubDate>Tue, 2 Oct 2018, 08:13:00 GMT</pubDate>
                        <media:content url="https://fbnlxna1ujeyw7ap.public.blob.vercel-storage.com/1766512161572-einstein.jpg" medium="image" />
                        <content:encoded>
                            <![CDATA[ 
Einstein once ([supposedly](https://www.snopes.com/fact-check/compound-interest/)) said:

> Compound interest is the most powerful force in the universe

Of compound interest, Warren Buffet [proclaims](https://www.marketwatch.com/story/this-warren-buffett-rule-can-work-wonders-on-your-portfolio-2016-04-26):

> Over time it accomplishes extraordinary things

Compound interest, or growth, is one of the, if not the most, powerful and impactful forces in nature.

And yet, it is also one of the **most consistently misunderstood** in the world of business.

How so?

Simply, we misapply the term "compound growth" to things that do not actually grow in compound fashion.

Let's first establish what "compound growth" even means.

I propose the following operative definition:

> Compound growth ~ constant growth

The fact is, very few objects, organisms or organizations can sustain truly compounding growth over any extended period.

From an observer's or investor's perspective, it's quite easy to fool yourself into thinking compound, exponential growth is much more common than it really is. And it's understandable given how often the term is thrown around. Firms in fleeting phases of fast growth can visually demonstrate their breakneck pace with the ubiquitous, infamous "[hockey stick](https://andrewchen.co/the-most-common-mistake-when-forecasting-growth-for-new-products-and-how-to-fix-it/)" chart.

Who could argue with that?

As an entrepreneur or operator, you too can fall prey to your own fictions - convincing yourself you've "cracked the code" when you've only really cracked the piggy bank. Irrational exuberance eventually turns concave, finally ending in a plateau of linearity.

Through some examples, I hope to demonstrate that compound growth 1) implies constant growth 2) is exceedingly rare and 3) is incredibly important to building a large, valuable business.

But before we get to business, let's talk about - bacteria.

## Bacteria and Bricklayers

![Bacteria](https://upload.wikimedia.org/wikipedia/commons/3/32/EscherichiaColi_NIAID.jpg)

In bacteria populations, growth is **fixed**. Subject to the resource constraints of the environment they inhabit, bacteria grow at a constant rate indefinitely.

A simple example to illustrate the point:

Let's say we have some bacteria that reproduce on a fixed time schedule, one doubling per minute to keep the numbers simple.

We start with a single bacteria cell. After one minute, we'll have two bacteria. With time, the population grows as such:

-   1
-   2
-   4
-   8
-   16
-   ...

Now we ask the question, how fast does our bacteria population grow (in percentage terms)?

The number of bacteria cells one minute from now is:

$$n\_{t+1} = 2n\_t$$

Which implies the minute-over-minute growth rate is:

$$\frac{n\_{t+1}}{n\_t} - 1 = \frac{2n\_t}{n\_t} - 1 = 2 - 1 = 1$$

or 100%.

This is an example of perfectly compounding growth, also referred to as **exponential** or **geometric growth**.

Put simply, how fast the bacteria grow is entirely independent of population size. In other words, growth and scale are perfectly uncorrelated.

Importantly, **most things do not work this way**.

### Layering on

Let's look at another example - constructing a brick wall.

Assume a bricklayer can lay 10 bricks per hour. The brick count will proceed as follows

-   0
-   10
-   20
-   30
-   40
-   ...

The brick count grows by 10 bricks per hour.

Going through the same growth rate calculations from above:

The number of bricks 1 hour from now will be:

$$n\_{t+1} = n\_t + 10$$

Which implies hour-over-hour growth is:

$$\frac{n\_{t+1}}{n\_t} - 1 = \frac{n\_t + 10 - n\_t}{n\_t}= \frac{10}{n\_t}$$

Notice that the growth rate depends on how many bricks we've already laid. This is **linear** or **arithmetic growth**. Because the number of bricks laid each hour is static through time, growth (in percentage terms) necessarily slows down. Scale is in the denominator. Therefore, growth and scale are negatively correlated: more scale -> less growth.

Sure, initially we are growing the brick count quite fast - 100% in fact. But by the time we reach 30 bricks, our forward-looking growth rate has fallen to 33%. At 100 bricks, we'll only be growing 10% - which is a far cry from our halcyon days of tech reporters and venture capitalists gawking at our growing (tech enabled) bricklaying operation.

## Two flavors of growth

The key difference between the bricks and the bacteria is that one has **scale invariant growth (SIG)** and the other... doesn't.

OK OK, friends who reviewed this before publishing said that was a big word/phrase to suddenly drop. So let's take a step back and examine this phenomenon visually before moving forward.

A great way to do this is plot the growth rate of the bacteria and bricks over time:

![bacteria-bricklayer-1](/content/images/2018/09/bacteria-bricklayer-1.png)

The bacteria grow at a constant rate over time. For the bricklayer, growth simply... collapses.

I've plotted this chart hundreds of times over the years, and for most startups the growth plot looks _eerily_ similar to the bricks here.

Growth is not the natural order; growth cannot be taken for granted. As we get **larger**, we get _slower_.

I mentioned correlation earlier. The correlation between growth and scale in the case of the bacteria is **0** - perfectly uncorrelated.

For the brick count, the correlation is **\-0.7**, a very strong negative correlation.

We've now established two ends of a spectrum we can use to characterize various forms of growth.

On one side, we have linear/additive/arithmetic/correlated growth, and on the other we have exponential/multiplicative/geometric/uncorrelated growth.

![](/content/images/2018/09/growthspectrum.png)

The question now is, where do various things fall along this spectrum? Said another way, how accurate is it to say that "XYZ" grows in compounding fashion?

Let's walk through some more examples.

### Debt

Compound growth is often used in reference to compound interest earned on a financial instrument of some sort.

Anyone who has ever suffered through mounting credit card debt knows this quite well. **Debt grows like bacteria** - it multiplies without end at a rate that depends entirely on the interest rate and not at all on the current balance.

1%, 5%, 10% - whatever the interest rate, unless paid off, debt continues to grow without end. If only paid off partially, the remaining balance will continue to grow.

Not a bad business model if you ask me.

### World GDP Per Capita

Growth is not the natural state of affairs. For most of human history there was no meaningful economic growth or improvement in livings standards for the average person. Until recently, Life was nasty, brutish, short and... static:

![gdp-world](/content/images/2018/06/gdp-world.jpg)

Unless growth is literally contractual, as in the case of debt and interest, we can't take it for granted, as history plainly shows.

And it's not simply a question of the scale of the axis. If you zoomed into that long straight line, you wouldn't see a hockey stick growth pattern. Living standards actually **did not** improve meaningfully over time for the vast majority of human existence on this planet.

A few years of bad weather, major epidemics like the Black Death (the bacteria strike again), social upheaval - these events drastically impacted the day-to-day well-being and lives of our ancestors, often erasing decades of progress.

Even today, many parts of the world experience major swings in their rates of growth, especially within the developing world. Regions and countries can end up in severe economic doldrums, leading to entire lost generations.

Many stops and starts, fits and spurts.

However, before we get too depressed, let's look at a best case scenario.

### U.S. GDP

The good ol' US of A ('s real GDP):

![](/content/images/2018/06/real-gdp.png)

Looks pretty good huh? Let's look at the growth plot:

![realgdpgrowth](/content/images/2018/09/realgdpgrowth.png)

Ugh, this is pretty noisy. It's difficult to tell if growth is changing in significant ways year-to-year or if it is generally variation around a certain value.

This view hides some interesting detail. One neat math trick - taking the natural log rescales a metric such that, when graphed, _linearity implies constant growth_.

Do this, and the real GDP chart becomes:

![](/content/images/2018/06/logrealgdp.png)

Over this period, we can make a few interesting observations:

-   Log real GDP is impressively linear - one could fit a linear line to the above data fairly well, implying reasonably constant growth
-   That said, it is not perfectly linear, and therefore not perfectly compounding, per our earlier definition
-   We can see multiple distinct inflection points where growth changed, in connection with recessions (1970, 2008)

Taking advantage of these kinks in the curve, let's estimate the growth during each period through piecewise linear regression (i.e. the "line of best fit" for each period):

![logrealgdp-piecewisereg](/content/images/2018/06/logrealgdp-piecewisereg.png)

Annual real growth goes from **3.9%** in the 1947-1970 period, to **3.1%** in the 1970-2008 period, to **2.1%** in the 2008-2017 period.

The economists yelling and screaming that we are on [permanently lower trajectory](https://www.youtube.com/watch?v=1eCYq2vD5GY) after the most recent recession may have a point.

So not exactly constant growth, but still impressive given the real economy grew 8x+ over this period. Growth has roughly halved over a 70-year period.

In terms of the connection between growth and scale, the correlation here is **\-0.3**, which certainly indicates a relationship, but not a strong one.

We can therefore conclude that U.S. GDP grows in reasonably compound fashion.

### Revenue

Most businesses see their revenue growth rate tick down over time. This is even more true for companies that are growing quickly today.

On the other hand, some exceptional businesses have managed to drive truly compound growth over long periods.

Take Amazon for example, which has exhibited incredible revenue growth over time ($B):

![amzn-revenue](/content/images/2018/06/amzn-revenue.png)

This is an impressive chart in its own right. But I am actually more impressed by the log-transformed chart, which is nearly a straight line:

![amzn-logrev](/content/images/2018/06/amzn-logrev.png)

**Amazon has grown at a nearly constant rate over almost two decades**, despite increasing scale by 64x over the period.

At best, one could identify a slight kink in growth in 2011. Replicating the piecewise analysis, we can see that Amazon grew ~30% year-over-year from 2000 to 2011 and ~23% year-over-year from 2011 to 2017.

![amzn-logrev-piecewisereg](/content/images/2018/06/amzn-logrev-piecewisereg.png)

Amazon's growth-scale correlation? **\-0.1**!

It's hard to put into words how impressive that single number is. Worth reiterating: **most things do not work this way**.

Amazon is an exceptional business that has evidently identified a way to grow at a nearly constant rate over many years. A combination of tapping into the long-run secular growth of e-commerce and deft expansion into seemingly orthogonal spaces (for example, via Amazon Web Services) that in fact leverage the core infrastructure the company's built up over time has enabled it to grow in bacteria-like fashion.

## Growth functions: Are you adding or multiplying?

Every growing business needs an honest answer to this question: Is your business growing through multiplication, like the Amazonian bacteria, or addition, like the brick wall?

**Businesses that simply "add" must necessarily slow down**, by the simple math we outlined earlier. Scale begins to work against you, making it harder and harder to maintain a rapid growth pace. **Eventually, you will, figuratively, hit a wall**.

An example of an additive growth function is paid customer acquisition through a channel like Google Adwords.

Spending $100 on Adwords is going to generate some number of users. Spending another $100 is probably going to generate a similar number of users, and so on.

**There's no "magic" here**. This is "buying growth" in the most direct manner.

If anything, customer acquisition through paid channels that you do not control (and Adwords is the epitome of this) tends to get _less_ efficient over time as you saturate keywords etc.

Like a bricklayer at the end of a long day, businesses reliant on this form of growth tend to run out of gas sooner or later.

Sure, you can attempt to stack bricks at a faster and faster rate, raising venture capital when you can no longer self-fund the endeavour, building the wall ever higher...

_But this too will pass_. Eventually, some proportion of those users _must_ stick around and continue to buy from you without meaningful additional spend on your part, otherwise you'll find yourself on the proverbial **"acquisition treadmill"**, unable to jump off without significant disruption to the business.

A number of companies in the subscription e-commerce "send me a box with a psuedo-random assortment of goods" space fall squarely into this category. Users churn at high rates, requiring more and more fuel to be poured on the paid acquisition fire to keep the train going.

On the other hand, **businesses that "multiply" can grow indefinitely**. Their "growth functions" are inherently multiplicative. Users beget more users. Revenue begets more revenue.

The classic exponential, multiplicative growth function is the **viral word-of-mouth (WOM)** or **referral program**.

PayPal built a viral engine in its early days, giving users money for each additional friend they referred to the service:

![](/content/images/2018/06/paypal-viral.png)

Dropbox replicated this, giving out additional space for signing up friends:

![](/content/images/2018/06/dropbox-viral.png)

The act of sharing a Dropbox file or folder with someone who wasn't yet a user generated even more sign-ups:

![](/content/images/2018/09/dropboxshare.png)

Whatever the approach, it is _vitally_ important that every business vigorously search for and identify exponential growth opportunities. It is mathematically inevitable that an additive, linear growth engine that does not compound on itself will eventually peter out, or even collapse like a wall built too high.

Likewise, investors must diligently sift through the noise to find the few bacteria-in-a-hay-stack that will drive true, long-term value creation. Ignore the steep trajectory in the short-run. Instead, focus on the **curvature of the horizon**.

**Scale invariant growth** is the key to building a large, meaningful business.

Go find it.

MathJax.Hub.Queue(\["Typeset",MathJax.Hub\]); ]]>
                        </content:encoded>
                    </item>
                    
        </channel>
    </rss>
