{"@adamekLassoInferenceHighDimensional2022":{"title":"Lasso Inference for High-Dimensional Time Series","links":["LASSO","@belloniHighDimensionalMethodsInference2014","@vandegeerAsymptoticallyOptimalConfidence2014","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Lasso Inference for High-Dimensional Time Series\nRobert Adamek, Stephan Smeekes, Ines Wilms\nLink\nAbstract\n\nIn this paper we develop valid inference for high-dimensional time series. We extend the desparsified lasso to a time series setting under Near-Epoch Dependence (NED) assumptions allowing for non-Gaussian, serially correlated and heteroskedastic processes, where the number of regressors can possibly grow faster than the time dimension. We first derive an error bound under weak sparsity, which, coupled with the NED assumption, means this inequality can also be applied to the (inherently misspecified) nodewise regressions performed in the desparsified lasso. This allows us to establish the uniform asymptotic normality of the desparsified lasso under general conditions, including for inference on parameters of increasing dimensions. Additionally, we show consistency of a long-run variance estimator, thus providing a complete set of tools for performing inference in high-dimensional linear time series models. Finally, we perform a simulation exercise to demonstrate the small sample properties of the desparsified lasso in common time series settings.\n\nThe key aim of the authors is to do inference in a lasso context, which is complicated by the fact that the variable selection feature of lasso renders standard inference invalid. Basically, when you do variable selection on the outcome variable, you potentially drop relevant covariates for properly measuring the coefficient on a “treatment” variable of interest. This makes inference on the parameters invalid. Therefore one needs post-selection inference methods. @belloniHighDimensionalMethodsInference2014 is an example of this, leveraging orthogonalization via Frisch-Waugh partialling out (post-double-selection) which ensures that covariates relevant to the treatment and/or outcome variable are kept in the regression.  The debiased/desparsified lasso of @vandegeerAsymptoticallyOptimalConfidence2014 is another approach.\nIn the desparsified lasso approach, you effectively add back a small amount of “noise” to the zeros from the original lasso estimates. This undoes model selection, but in return, you get valid inference, normally distributed uncertainty, etc. By definition the estimator is no longer sparse, but now you can make scientific statements about the coefficients of interest. One can think of this as “weak sparsity”.\nWe also need to address the IID issue. Another key tension that these authors address is the fact that lasso cannot be applied off the shelf to time series data, as the data are not IID. There is other research that also explores this, so this is not the first entry in the literature. They extend the desparsified lasso to the time series context. Their approach also allows the errors to be non-Gaussian and heteroskedastic. \nUnfortunately my sense is that the methods in this paper only apply to stationary data, or at least that is an assumption of the method, per these quotes (non-stationary data does not have constant or even finite moments). One may be able to get around this by first regressing non-stationary variables on a lag and using the residuals as regressors:\n\nAssumption 1(i) ensures that the error terms are contemporaneously uncorrelated with each of the regressors, and that the process has finite and constant unconditional moments.\n…\nWe provide a complete set of tools for uniformly valid inference in high-dimensional stationary time series settings, where the number of regressors N can possibly grow at a faster rate than the time dimension T .\n\nFrom some example DGPs they provide:\n\nAlso assume that the vector of exogenous variables wt is stationary and geometrically β-mixing as well with finite 2  ̄ m moments.\n…\nthe K × K matrices Φi satisfy appropriate stationarity and 2  ̄ m-th order summability conditions.\n\nHere’s my attempt at explaining desparsified lasso in simple terms:\n\nThe idea is to unsparsify the original lasso estimates, with the goal of removing the bias generated by standard lasso, which enables valid inference\nTake the initial lasso estimates and add to them some “noise” equal to the covariance of the residualized covariate in question with the residualized outcome variable (i.e. the regression scores) scaled by a factor, where the residualization is done via standard lasso\n\n\nReferences\npaperreadonline"},"@agarwalManyShotInContextLearning2024":{"title":"Many-Shot In-Context Learning","links":["tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Many-Shot In-Context Learning\nRishabh Agarwal, Avi Singh, Lei M. Zhang, Bernd Bohnet, Luis Rosias, Stephanie Chan, Biao Zhang, Ankesh Anand, Zaheer Abbas, Azade Nova, John D. Co-Reyes, Eric Chu, Feryal Behbahani, Aleksandra Faust, Hugo Larochelle – 2024\nBasically, use all the examples you can within the prompt. Performance doesn’t max out until you get to pretty large scale, on the order of hundreds or thousands of example. In other words, if accuracy is your main concern, you should use as much of the context window as you can.\n\nReferences\npaperreadonline"},"@atheyGeneralizedRandomForests2018":{"title":"Generalized Random Forests","links":["local-projections","impulse-responses","random-forests","@chernozhukovDoubleDebiasedMachine2017","Local-projections","Impulse-responses","instrumental-variables","@stockIdentificationEstimationDynamic2018","@rameyMacroeconomicShocksTheir2016","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Generalized Random Forests\nSusan Athey, Julie Tibshirani, Stefan Wager\nLink\nAbstract\n\nWe propose generalized random forests, a method for non-parametric statistical estimation based on random forests (Breiman, 2001) that can be used to fit any quantity of interest identified as the solution to a set of local moment equations. Following the literature on local maximum likelihood estimation, our method considers a weighted set of nearby training examples; however, instead of using classical kernel weighting functions that are prone to a strong curse of dimensionality, we use an adaptive weighting function derived from a forest designed to express heterogeneity in the specified quantity of interest. We propose a flexible, computationally efficient algorithm for growing generalized random forests, develop a large sample theory for our method showing that our estimates are consistent and asymptotically Gaussian, and provide an estimator for their asymptotic variance that enables valid confidence intervals. We use our approach to develop new methods for three statistical tasks: non-parametric quantile regression, conditional average partial effect estimation, and heterogeneous treatment effect estimation via instrumental variables. A software implementation, grf for R and C++, is available from CRAN.\n\nWhen you have an estimator with low bias but high variance, averaging multiple noisy instances of that predictor helps reduce variance. It’s worth thinking about how to apply this in other arenas, such as local projections, which are known for their low bias but high variance. This logic would suggest estimating multiple impulse responses via local projections and then averaging them to get a lower variance estimate. How to generate these estimates is the key question. I’d imagine two potential options: (1) estimate the same model on different subsamples of the data, (2) estimate different models with different controls included on the same data:\n\nbecause individual trees \\hat\\mu_{b}(x) have low bias but high variance, such averaging meaningfully stabilizes predictions\n\nReminds me that it’s worth using random forests as the estimator for @chernozhukovDoubleDebiasedMachine2017 style estimates of local projections. It still surprises me that more people haven’t considered this, especially given all the work around deriving more “causal” impulse responses via instrumental variables variants of local projections (@stockIdentificationEstimationDynamic2018, @rameyMacroeconomicShocksTheir2016).\n\nReferences\npaperreadonline"},"@axlerLinearAlgebraDone":{"title":"Linear Algebra Done Right","links":["tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Linear Algebra Done Right\n**Sheldon Axler – **\nChapter 1: Vector Spaces\nComplex Numbers\nComplex numbers were invented so we could take square roots of negative numbers.\nComplex numbers are defined as an ordered pair of real numbers a and b, and are typically written as a +bi. One can even think of the set of real numbers as being a subset of the set of complex numbers, where b=0 for all real numbers. Note that a complex number is considered a single number, even though it involves an ordered pair of real numbers.\nAll the typical arithmetic operations and properties apply to complex numbers (commutativity, associativity, etc), so you can work with them fairly similarly.\nMultiply complex number as so:\n(a + bi)(c+di) = (ac-bd) + (ad+bc)i\nThe calculation for multiplying complex numbers doesn’t need to be memorized, as it can always be re-derived as needed. That said, if you need some intuition, notice that i^2=-1, so when you multiply bi by di, that becomes -bd.\nFields\nA field is a set of at least two distinct elements along with various operations of addition and multiplication. Examples of fields include the set of all real numbers and the set of all complex numbers.\nVector Spaces\nA vector space is a set of numbers with addition and multiplication that have the typical properties we associate with those operations. Elements of a vector space are called vectors.\nWe can define addition or scalar multiplication as the set as functions that respectively maps pairs of numbers within the set to a new numbers also within the set.\n\nReferences\npaperreadonline"},"@baiPrincipalComponentsEstimation2013":{"title":"Principal components estimation and identification of static factors","links":["@stockDynamicFactorModels2016","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Principal components estimation and identification of static factors\nJushan Bai, Serena Ng – 2013\n\n\n                  \n                  Abstract \n                  \n                \n\nIt is known that the principal component estimates of the factors and the loadings are rotations of the underlying latent factors and loadings. We study conditions under which the latent factors can be estimated asymptotically without rotation. We derive the limiting distributions for the estimated factors and factor loadings when N and T are large and make precise how identification of the factors affects inference based on factor augmented regressions. We also consider factor models with additive individual and time effects. The asymptotic analysis can be modified to analyze identification schemes not considered in this analysis. © 2013 Elsevier B.V. All rights reserved.\n\n\nAnother description of the named factor normalization covered in @stockDynamicFactorModels2016. Think I get it at this point, though I somewhat question its utility\n\nReferences\npaperreadonline"},"@bardesVICRegVarianceInvarianceCovarianceRegularization2021":{"title":"VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning","links":["Yann-LeCun","self-supervised-learning","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning\nAdrien Bardes, Jean Ponce, Yann LeCun\nAbstract\n\nRecent self-supervised methods for image representation learning maximize the agreement between embedding vectors produced by encoders fed with different views of the same image. The main challenge is to prevent a collapse in which the encoders produce constant or non-informative vectors. We introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with two regularizations terms applied to both embeddings separately: (1) a term that maintains the variance of each embedding dimension above a threshold, (2) a term that decorrelates each pair of variables. Unlike most other approaches to the same problem, VICReg does not require techniques such as: weight sharing between the branches, batch normalization, feature-wise normalization, output quantization, stop gradient, memory banks, etc., and achieves results on par with the state of the art on several downstream tasks. In addition, we show that our variance regularization term stabilizes the training of other methods and leads to performance improvements.\n\nself-supervised learning method for training embeddings from images (though could probably be applied to other modalities) where the model generates outputs from two transformed version of the input and tries to ensure that the output embeddings are similar (invariance), the individual dimensions of the embeddings vary enough within a batch (variance), and the covariances among the dimensions are minimized (covariance).\nLoss function is weighted average of the three terms, with the covariance term being constrained to the smallest weight (which makes sense given it’s units are likely large) and the other weights forced to be equal (variance and invariance “matter” equally).\nBenefits of the approach are that you don’t need contrastive examples or normalization.\n\nThe most important features of the approach are variance and invariance, without which the embeddings simply collapse. The covariance feature enhances performance to near-SOTA levels but isn’t required to get the approach at least working to some extent.\n\n\nReferences\npaperreadonline"},"@bardsenForecastingLevelsLog2011":{"title":"Forecasting levels of log variables in vector autoregressions","links":["Nassim-Taleb","tags/literature/paper","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Forecasting levels of log variables in vector autoregressions\nGunnar Bårdsen, Helmut Lütkepohl\nLink\nAbstract\n\nSometimes forecasts of the original variable are of interest, even though a variable appears in logarithms (logs) in a system of time series. In that case, converting the forecast for the log of the variable to a na¨ıve forecast of the original variable by simply applying the exponential transformation is not theoretically optimal. A simple expression for the optimal forecast under normality assumptions is derived. However, despite its theoretical advantages, the optimal forecast is shown to be inferior to the na¨ıve forecast if specification and estimation uncertainty are taken into account. Hence, in practice, using the exponential of the log forecast is preferable to using the optimal forecast.\n\nMain point here is that the common recommendation that you modify the predictions of a log-linear model when moving to log levels units may not actually be correct. Rather, you can just naively exponentiate and do just fine:\n\nthe common practice of forecasting the logs of a variable and then obtaining a forecast of the original variable by applying the exponential function is a useful strategy in practice.\n…\nfor typical economic variables, gains in forecast precision from using the optimal rather than the na ̈ıve forecast are not likely to be substantial. In fact, in practice the optimal forecast may well be inferior to the na ̈ıve forecast.\n\nThey arrive at this conclusion by first deriving a simpler form of the predicted value, then showing in simulation that you get smaller RMSE if you use the naive estimator instead of a post hoc adjustment:\n\nfor variables which have typical features of some economic variables, using the optimal forecast is likely to result in efficiency losses if the forecast precision is measured by the root mean square error (RMSE).\n\nThe standard recommended adjustment is the following, which is straightforward to derive if you assume normal forecast errors:\nE(\\exp(x)) = \\exp(\\mu + \\frac{1}{2}\\sigma^2)\nThere are three main reasons for this counterintuitive result:\n\nThe standard adjustment assume normal forecast errors, which may not be true in fact\nThe adjustment factor itself (the variance of the forecast errors) must itself be estimated, introducing error\nStationary variables that are log transformed have bounded error distributions even out to infinity, and those errors are small relative to the level of the variable, so the adjustment doesn’t do much\n\nWhile one should be careful with non-stationary variables, errors driven by misspecification / estimation error will tend to drown out everything else, making the naive forecast perform relatively well vs. the “optimal” adjusted forecast:\n\nfor integrated variables, the naıve forecasts generally perform better than the optimal forecasts, with the relative gains increasing with the forecast horizon.\n\nThis seems like a classic case where being too confident about the DGP / model leads to serious issues. Sometimes, simpler is better, especially when operating under substantial uncertainty, which we tend to be most of the time even when we don’t acknowledge it, per Nassim Taleb.\n\nReferences\npaperonline"},"@barigozziInferenceHeavyTailedNonstationary2022":{"title":"Inference in Heavy-Tailed Nonstationary Multivariate Time Series","links":["Stochastic-trend","stationarity","Fat-Tails","@onatskiSpuriousFactorAnalysis2021","Principal-component-analysis","@penaNonstationaryDynamicFactor2006","@zhangIdentifyingCointegrationEigenanalysis2019","tags/literature/paper","tags/online"],"tags":["literature/paper","online"],"content":"Inference in Heavy-Tailed Nonstationary Multivariate Time Series\nMatteo Barigozzi, Giuseppe Cavaliere, Lorenzo Trapani – 2022\n\n\n                  \n                  Abstract \n                  \n                \n\nWe study inference on the common stochastic trends in a non-stationary, N -variate time series yt, in the possible presence of heavy tails. We propose a novel methodology which does not require any knowledge or estimation of the tail index, or even knowledge as to whether certain moments (such as the variance) exist or not, and develop an estimator of the number of stochastic trends m based on the eigenvalues of the sample second moment matrix of yt. We study the rates of such eigenvalues, showing that the first m ones diverge, as the sample size T passes to infinity, at a rate faster by O (T ) than the remaining N −m ones, irrespective of the tail index. We thus exploit this eigen-gap by constructing, for each eigenvalue, a test statistic which diverges to positive infinity or drifts to zero according to whether the relevant eigenvalue belongs to the set of the first m eigenvalues or not. We then construct a randomised statistic based on this, using it as part of a sequential testing procedure, ensuring consistency of the resulting estimator of m. We also discuss an estimator of the common trends based on principal components and show that, up to a an invertible linear transformation, such estimator is consistent in the sense that the estimation error is of smaller order than the trend itself. Importantly, we present the case in which we relax the standard assumption of i.i.d. innovations, by allowing for heterogeneity of a very general form in the scale of the innovations. Finally, we develop an extension to the large dimensional case. A Monte Carlo study shows that the proposed estimator for m performs particularly well, even in samples of small size. We complete the paper by presenting two illustrative applications covering commodity prices and interest rates data.\n\n\n\n\n                  \n                  Summary \n                  \n                \n\nMethod for estimating the number of common stochastic trends in nonstationary, fat-tailed data that doesn’t require knowing the tail index.\n\n\nwww.youtube.com/watch\nThe key idea of this paper analytically is to scale the covariance matrix of the data in levels by the covariance matrix of the data in differences. Doing so ensures that eigen-analysis is scale-free and independent of the fat-tailedness of the data. The scaling is important, otherwise you’re simply in the same realm analyzed by @onatskiSpuriousFactorAnalysis2021 where the first few eigenvalues are destined to be large even if there are no true factors.\nPCA is always superconsistent in the presence of integrated processes, regardless of how many individual time series you have.\nInteresting implicit test of stationarity – if the test cannot find at least one common trend, the data is by definition stationary.\nThe one uncertainty I have about this paper is whether or not you need to standardize the variables before taking the eigenvalues. The paper doesn’t say that you have to, perhaps because multiplying by the inverse of the covariance of the differences takes care of this? Also have some questions about detrending the variables.\n\nReferences\n@penaNonstationaryDynamicFactor2006\n@zhangIdentifyingCointegrationEigenanalysis2019\n@onatskiSpuriousFactorAnalysis2021\npaperonline"},"@barigozziLargedimensionalDynamicFactor2021":{"title":"Large-dimensional Dynamic Factor Models: Estimation of Impulse–Response Functions with I ( 1 ) cointegrated factors","links":["tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Large-dimensional Dynamic Factor Models: Estimation of Impulse–Response Functions with I ( 1 ) cointegrated factors\nMatteo Barigozzi, Marco Lippi, Matteo Luciani – 2021\n\n\n                  \n                  Abstract \n                  \n                \n\nWe study a large-dimensional Dynamic Factor Model where: (i) the vector of factors Ft is I(1) and driven by a number of shocks that is smaller than the dimension of Ft ; and, (ii) the idiosyncratic components are either I(1) or I(0). Under (i), the factors Ft are cointegrated and can be modeled as a Vector Error Correction Model (VECM). Under (i) and (ii), we provide consistent estimators, as both the cross-sectional size n and the time dimension T go to infinity, for the factors, the loadings, the shocks, the coefficients of the VECM and therefore the Impulse–Response Functions (IRF) of the observed variables to the shocks. Furthermore, possible deterministic linear trends are fully accounted for, and the case of an unrestricted VAR in the levels Ft , instead of a VECM, is also studied. The finite-sample properties the proposed estimators are explored by means of a MonteCarlo exercise. Finally, we revisit two distinct and widely studied empirical applications. By correctly modeling the long-run dynamics of the factors, our results partly overturn those obtained by recent literature. Specifically, we find that: (i) oil price shocks have just a temporary effect on US real activity; and, (ii) in response to a positive news shock, the economy first experiences a significant boom, and then a milder recession.\n\n\n\n\n                  \n                  Summary \n                  \n                \n\nCommon factors among economic time series are often non-stationary and cointegrated, suggesting they should be jointly modeled with a VECM.\n\n\nImportant to keep in mind here that what’s new is the not estimator of the factor loadings (eigenvectors of covariance matrix of differed data, which is standard) but the estimator of the factors themselves, in which the loadings are applied to de-trended data rather than to differenced data. This gives you the factors in levels directly and doesn’t require cumulation at the end.\nNote that this method can potentially have issues if every series in the dataset has linear trends.\n\nReferences\npaperreadonline"},"@barnichonImpulseResponseEstimation2019":{"title":"Impulse Response Estimation by Smooth Local Projections","links":["Local-Projections","identification","@jordaEstimationInferenceImpulse2005","@plagborg-mollerEstimationSmoothImpulse","tags/online"],"tags":["literature/paper","online"],"content":"Impulse Response Estimation by Smooth Local Projections\nRegis Barnichon, Christian Brownlees\nAbstract\n\nLocal Projections (LP) is a popular methodology for the estimation of Impulse Responses (IR). Compared to the traditional VAR approach, LP allow for more flexible IR estimation by imposing weaker assumptions on the dynamics of the data. The nonparametric nature of LP comes at an efficiency cost and in practice the LP estimator may suffer from excessive variability. In this work we propose an IR estimation methodology based on B-spline smoothing called Smooth Local Projections (SLP). The SLP approach preserves the flexibility of standard LP, can substantially increase precision and is straightforward to implement. A simulation study shows that SLP can deliver substantial gains in IR estimation over LP. We illustrate our technique by studying the effects of monetary shocks where we highlight how SLP can easily incorporate commonly employed structural identification strategies.\n\nOne way to think about the large, single regression that is done in this paper is to analogize it to panel regression\n\nYou stack the variables for each of the units. Here, the “units” are the different horizons. So you end up with roughly N * H data points in a single regression rather than N data points in H different regressions. You can think about the outcome variable as y_{it}, where i is the unit and t is the time point, except here you effectively have y_{th}.\n\nSteps of smooth local projections\n\nTransform the impulse variable X from a Nx1 dimensional vector to a NxK dimensional matrix, via the transformation X @ B, where B is a vector of K values coming from K basis functions\nResidualize the outcome variable and the transformed impulse variable by the set of controls for each impulse horizon, appropriately lagged\nStack the residualized outcome variable and the NxK impulse variable by the horizon, yielding (NxH)x1 and (NxH)xK dimensional matrices for each\nRun a penalized regression, where the r-th difference between subsequent splined Xs is penalized by \\lambda. Note you can do this in one-shot via standard least squares by stacking the r-th difference matrix multiplied by \\lambda on to the X matrix. This is effectively finds the weights for the basis functions that work best across all horizons\nGet the final IRF by multiplying the matrix of basis functions (splines) by the weights vector\n\nReferences\n\nEstimation and Inference of Impulse Responses by Local Projections\nEstimation of Smooth Impulse Response Functions\n\nonline"},"@bartholdyUnbiasedEstimationExpected2003":{"title":"Unbiased estimation of expected return using CAPM","links":["tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Unbiased estimation of expected return using CAPM\nJan Bartholdy, Paula Peare – 2003\nInteresting paper that uses some fairly simple math to show why the kind of traditional time series regression approach to estimating expected returns leads to a downward bias in the estimates.\nThe idea is that when you think about the CAPM model the market risk premium that you multiply by the stock’s beta needs to be estimated, and we don’t have access to the actual world market portfolio so you need to use some proxy for that (let’s say the S&amp;P 500 or something similar) and these proxies are imperfectly correlated with the actual world market portfolio. That imputes a bias in the estimation of expected returns unless you’re careful to undo that bias. Now undoing that bias is non-trivial because it turns out that it requires estimating the correlation between the proxy returns and the true market returns, but the true market is by definition unobservable and so you can’t calculate the adjustment factor.\nThe fix is to use the Fama-MacBeth procedure:\n\nRun time-series regressions as you normally would to get estimated betas\nRun a cross-sectional regression in each time period that relates the excess returns of each stock to the estimated betas for stocks in that particular time period\nRun that regression over multiple time periods (e.g., 5 years)\nTake the average of the coefficients in each of those regressions over that period of time\n\nThat coefficient is an unbiased estimate for the true market risk premium, which is the degree to which investors are being paid to take market risk over and above the risk-free rate. That can then be multiplied by the estimated beta for any given stock in order to get its expected return. This will be an unbiased estimate of the stock’s expected return.\nThe issue here is quite analogous to that in regression of the problem of measurement error in the regressors. When you regress y on x, if there is measurement error in x that will cause attenuation bias in the estimated coefficients. Similarly, here the x’s are essentially the market risk premium which, because we are using a proxy for the market, have some sort of measurement error in them. And so the betas and then therefore the expected returns end up being downward biased.\n\nReferences\npaperreadonline"},"@bayerLiquidityChannelFiscal2020":{"title":"The Liquidity Channel of Fiscal Policy","links":["Local-projections","time-trend","@blanchardEmpiricalCharacterizationDynamic2002","residuals","tags/literature/paper","tags/online"],"tags":["literature/paper","online"],"content":"The Liquidity Channel of Fiscal Policy\nChristian Bayer, Benjamin Born, Ralph Luetticke\nAbstract\n\nWe provide evidence that expansionary fiscal policy lowers return differences between public debt and less liquid assets—the liquidity premium. We rationalize this finding in an estimated heterogeneous-agent New-Keynesian model with incomplete markets and portfolio choice, in which public debt affects private liquidity. This liquidity channel stabilizes fixed-capital investment. We then quantify the long-run effects of higher public debt and find little crowding out of capital, but a sizable decline of the liquidity premium, which increases the fiscal burden of debt. We show that the revenue-maximizing level of public debt is positive and has increased to 60 percent of GDP post-2010.\n\nLocal projection specification:\nx_{t+h}=\\beta_0 + \\beta_1t + \\beta_2t^2 + \\psi_h\\log{g_t} + \\Gamma(L)Z_{t-1} + u_{t+h}\nZ includes vector of controls, including four lags of everything. Note the use of a quadratic time trend.\nMakes the point that including lags means that the variable in question represents a shock and is therefore plausibly causal in its influence, a point also made by @blanchardEmpiricalCharacterizationDynamic2002:\n\n“Under the Blanchard and Perotti (2002)-predeterminedness assumption, the coefficient \\psi_h provides a direct estimate of the impulse response at horizon h to the government spending shock in t.” (Bayer et al., 2020, p. 6) (pdf)\n“This is equivalent to a two-step approach, where g_t is first regressed on lags of itself and additional covariates and the residual is then included in step 2 as the shock measure.” (Bayer et al., 2020, p. 6) (pdf)\n\nThis work uses levels of all the variables along with their lags and seems to arrive at sensible results, which I’ve generally found to be harder to achieve:\nTransclude of Local-projections#c91898\n\n\n\nReferences\npaperonline"},"@bertolissiLocalMixturesExperts":{"title":"Local Mixtures of Experts: Essentially Free Test-Time Training via Model Merging","links":["tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Local Mixtures of Experts: Essentially Free Test-Time Training via Model Merging\nRyo Bertolissi,  Jonas Hübotter, Ido Hakimi, Andreas Krause\n\n\n\nKey contributions\n\nTest-time model merging (training and merging many expert models for a single task / domain) beats traditional model merging (training and merging a few models for multiple tasks)\nTest-time model merging approaches performance of test-time training with almost no test-time compute overhead\n\nInductive vs transductive learning\n\nDuring traditional training, learn a model by inductively extracting general rules from data that can then be applied to downstream examples at test time.\nTransductive learning directly uses test examples (with no labels) to make predictions. You predict specific examples rather than attempting to learn a more general function, which is in some sense a simpler problem. No generalization beyond the specific test examples is required or expected.\n\nTest-time training (TTT)\n\nFine-tuning a model for every individual task (prompt)\nSignificantly improves model performance at high test-time computational cost\n\nTest-time model merging (TTMM)\n\nAt train-time, cluster the training data into local neighborhoods and train a small expert LoRA adapter for each cluster\nAt test-time, dynamically select a subset of LoRA adapters and merge their parameters to form a s single task specific model\nApproaches the performance of TTT without significant compute or memory cost\n\nModel merging\n\nIn a multitask setting, merge multiple expert models, typically small number of models / tasks. Happens once at the end of training, then model is fixed thereafter\nTTMM differs in that models are local to a specific task in question, merging many related local models for a single task at test-time\n\n\nReferences\npaperreadonline"},"@blanchardEmpiricalCharacterizationDynamic2002":{"title":"An Empirical Characterization of the Dynamic Effects of Changes in Government Spending and Taxes on Output","links":["Identification","unit-root","tags/literature/paper","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"An Empirical Characterization of the Dynamic Effects of Changes in Government Spending and Taxes on Output\nOlivier Blanchard, Roberto Perotti\nAbstract\n\nThis paper characterizes the dynamic effects of shocks in government spending and taxes on U. S. activity in the postwar period. It does so by using a mixed structural VAR/event study approach. Identification is achieved by using institutional information about the tax and transfer systems to identify the automatic response of taxes and spending to activity, and, by implication, to infer fiscal shocks. The results consistently show positive government spending shocks as having a positive effect on output, and positive tax shocks as having a negative effect. One result has a distinctly nonstandard flavor: both increases in taxes and increases in government spending have a strong negative effect on investment spending.\n\nTalks about the use of deterministic (linear, quadratic etc time trends) vs stochastic (“unit root with slowly changing drift”) trends. They talk about it a bit on page 1339.\nInteresting that they added controls interacted with the lags, allowing for quarter-dependence of the relevant effects/coefficients. I think that’s fine in principle but could have been a random thing added in to make the results look better.\n\nReferences\npaperonline"},"@budaShortVariableLags2023":{"title":"Short and Variable Lags","links":["Impulse-responses","local-projections","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Short and Variable Lags\nGergely Buda, Vasco M. Carvalho, Giancarlo Corsetti, João B. Duarte, Stephen Hansen, Álvaro Ortiz, Tomasa Rodrigo, José V. Rodríguez Mora – 2023\n\n\n                  \n                  Abstract \n                  \n                \n\nWe study the transmission of monetary policy shocks using daily consumption, corporate sales and employment series. We find that the economy responds at both short and long lags that are variable in economically significant ways. Consumption reacts in one week, reaches a local trough in one quarter, recovers, and declines again after three quarters. Sales follow a similar pattern, but the initial drop, while delayed (one month), is deeper. In contrast, employment falls monotonically for five quarters albeit with a smaller impact reaction. We show that these short lags are masked by time aggregation at lower —quarterly— frequencies.\n\n\n\n\n                  \n                  Summary \n                  \n                \n\nTK\n\n\nDemonstrates cool way to calculate level impulse responses from Y/Y growth IRFs.\nThere’s a lot of smoothing/filtering here – they use a 90-day moving average on the daily data, and then in addition use y/y growth rates in the local projections. So in effect you have extremely smoothed data, which might be necessary given the inherent noisiness of daily data. Something to potentially consider for my purposes as an alternative to LOESS.\n\nReferences\npaperreadonline"},"@canovaFAQHowExtract":{"title":"FAQ: How do I extract the output gap?","links":["Lindy-effect","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"FAQ: How do I extract the output gap?\n**Fabio Canova – **\n\n\n                  \n                  Abstract \n                  \n                \n\nI investigate the properties of gaps and potentials in a variety of DSGE models and their relationship with estimates obtained with standard approaches. Gaps display low frequency variations, have similar frequency domain representation as potentials, and are correlated with them. Transitory and permanent fluctuations display similar features, but are uncorrelated. I use a number of procedures to estimate the latent components. All approaches generate distortions. Gaps are best estimated with a polynomial filter; transitory fluctuations with a differencing approach. Explanations for the outcomes are given. I design a procedure that reduces the biases of existing methods.\n\n\n\n\n                  \n                  Summary \n                  \n                \n\nTK\n\n\nyoutu.be/nWeL1lKugik\nTypical statistical approaches for extracting or removing trends make several assumptions that don’t necessarily hold in the underlying DGP:\n\nTrend isn’t necessarily uncorrelated with cycle\nCycles are not necessarily transitory\nTrue cycles do not necessarily happen at business cycle frequencies\n\nPolynomial detrending / filtering is Lindy: it is the oldest procedure and arguably one of the best.\n”Gap” isn’t a meaningful concept without some sort of structural theory for “potential”. Given that I mostly avoid deep theoretical models in my work, I should be careful about any suggestion of “potential” in venture capital data.\n\nReferences\npaperreadonline"},"@castillo-martinezHowCentralBanks":{"title":"How do central banks control inflation? A guide for the perplexed","links":["tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"How do central banks control inflation? A guide for the perplexed\nLaura Castillo-Martinez, Ricardo Reis\nThis article argues that the central bank rests at the center of inflation, or rather, at the center of the factors and mechanisms determining inflation. This has various key components:\n\nPrices: The authors construct a model of prices conditional on real outcomes. Their model does not go so far as to solve for both prices and real outcomes as a function of exogenous shocks, so in that sense, it’s an incomplete model. In this model, supply and demand alone do not pin down inflation; that requires a central bank, “whose liabilities define what prices are in the first place.”\nControlling inflation: The authors argue that by (1) setting the interest rate on banks’ deposits at the central bank (bank reserves) in an “aggressive and transparent way”, (2) anchoring inflation expectations, and (3) having fiscal support to “prevent runs on its liabilities” (not sure what that means), the central bank can control inflation.\nDeterminacy: To achieve a given inflation target, monetary policy must ensure a determinate equilibrium. Multiple or indeterminate equilibria means the policy framework is incomplete in some way. Further, the central bank must form accurate estimates of the state of the economy and communicate those transparently.\nIntertemporal budget constraint: Like all economic actors, the central bank has an intertemporal budget constraint, enforced by the unwillingness of rational private agents to “hold the liabilities of an insolvent institution.” In meeting this constraint, the central bank’s tools include expenses, the composition of its assets, and the dividends it pays to the government.\n\n\nReferences\npaperreadonline"},"@chanWhyDidWe":{"title":"Why did we think wages are rigid for all those years?","links":["Do-the-simplest-thing-that-works","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Why did we think wages are rigid for all those years?\nSee-Yu Chan, Stephan Hobler, Thijs van Rens\nThis is a good example of Do the simplest thing that works — the paper essentially removes workers from the earnings distribution that report full dollar or half dollar earnings in their hourly or weekly wages, and then re-estimates the distribution of wage changes. While this is far from a bulletproof approach to accounting for measurement error, it’s extremely simple and just kind of makes sense.\n\nReferences\npaperreadonline"},"@chenLogsZerosProblems2024":{"title":"Logs with Zeros? Some Problems and Solutions","links":["tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Logs with Zeros? Some Problems and Solutions\nJiafeng Chen, Jonathan Roth – 2024\nWhen estimating average treatment effects (ATE) for positive-only outcomes, using log-like transformation that aren’t the log itself can cause issues. These include \\log(1+Y), \\text{arcsinh}(Y) and so on.\nUnfortunately, ATEs for these transformations are scale dependent, and thus should not be interpreted as percentage effects, as arbitrary changes to the scale will affect the estimated effect.\nThese are mostly used to avoid taking the log of zero. Unfortunately there’s no free lunch — there’s no treatment effect parameter that you can estimate after taking these transformations that is point identified.\nInstead, the authors recommend using Poisson regression to estimate the ATE in levels as a percentage:\n\\theta_{ATE\\%} = \\frac{E[Y(1)-Y(0)]}{E[Y(0)]}\ne^\\beta-1=\\theta_{ATE\\%}\nSo you read off the \\beta from the Poisson regression, run it through the quick equation above, and et voila you have your percentage ATE!\nNote this method completely abstracts from extensive (zero vs non-zero) vs intensive margin (non-zero vs non-zero) changes.\nAlso note that this is not literally estimating a percentage effect, it’s really estimating a level effect and then rescaling. This can be an important distinction depending on the application, so tread carefully.\n\nReferences\npaperreadonline"},"@cochraneHowBigRandom1988":{"title":"How Big Is the Random Walk in GNP?","links":["random-walk","stationarity","Impulse-responses","Don't-Discount-Interest-Rates","Random-walk","Beats-and-Misses-Are-Forever","RBC","@cochranePermanentTransitoryComponents1994","@hodrickExplorationTrendCycleDecomposition2020","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"How Big Is the Random Walk in GNP?\nJohn H. Cochrane\nLink\n\n\n                  \n                  Abstract\n                  \n                \n\nThis paper presents a measure of the persistence of fluctuations in GNP based on the variance of its long differences. That measure finds little long-term persistence in GNP. Previous research on this question found a great deal of persistence in GNP, suggesting models such as a random walk. A reconciliation of this paper’s results with previous research shows that conventional criteria for time series model building can produce misleading estimates of persistence.\n\n\nIt’s very difficult to distinguish between permanent shocks and merely transitory shocks with high persistence.\n\nThis paper reexamines the long-run properties of GNP and argues that GNP does, in fact, revert toward a “trend” following a shock. However, that reversion occurs over a time horizon characteristic of business cycles-several years at least. Therefore, the short-run properties of GNP are consistent with a model with very persistent shocks,\n\nFor stationary time series, long-term forecasts do not change in response to shocks. In other words, the impulse response function should fade to zero over a long enough horizon. This seems to be roughly the case for my analysis in Don’t Discount Interest Rates.\n\nIf the variance of the shocks to the random walk component is zero, the series is trend-stationary, and long-term forecasts do not change in response to shocks. If the variance of the shocks to the random walk component is equal to the variance of first differences, the series is a pure random walk.\n\nFor a time series with permanent fluctuations, like a random walk, lower values today imply lower forecasts out into the indefinite future. This seems to be what I found in Beats and Misses Are Forever – lower revenue today forecasts lower revenue one, two, and three years out.\n\nFluctuations in a random walk are permanent in the following sense: suppose that \\epsilon_t = - 1, so that y_t falls one unit below last period’s expected value. Then, since y_{t+j} = y_t + j\\mu + \\epsilon_{t+1} +  . . . + \\epsilon_{t+p} forecasts E_t(y_{t+j}) fall by one unit for the indefinite future. Also, a low or negative growth rate today implies nothing about growth rates in the future, and there is no tendency for future levels of GNP to revert to a trend line.\n\n\nHow much does a one-unit shock to GNP affect forecasts in the far future? If by one unit, it finds a random walk; if by zero, it finds a trend-stationary process like (1). It can also find numbers between zero and one, characterizing a series that returns toward a “trend” in the far future,\n\n\nThe size of the random walk component seems to have implications for the plausibility of various economic models. In particular, if the economy has a large random walk component, that is favorable of RBC style models of the economy, where fluctuations originate from real productivity shocks. If the random walk component is small or non-existant, that militates in favor of Keynesian or monetary theories of the business cycle, where the economy fluctuates around an exogenous trend due to monetary or fiscal policy.\n\nThe size of a random walk in GNP has been cast as a direct test between competing models of the economy. For example, Nelson and Plosser (1982) interpreted their result that GNP has a large random walk component as evidence for stochastic equilibrium models over traditional monetary or Keynesian business cycle models. They argued that traditional models produce only temporary deviations from trend, while models that find the ultimate source of GNP variability in technology shocks can produce permanent fluctuations.\n\n\nThis suggests that software companies are better modeled with an RBC-like approach, which is what I would have guessed (refer to Beats and Misses Are Forever)\nVenture capital seems to be better modeled by a Keynesian or monetarist approach, which is also quite intuitive given the whole premise of Don’t Discount Interest Rates is exactly that monetary policy has a large effect on venture activity\n\n\nReferences\nPermanent and Transitory Components of GNP and Stock Prices\nAn Exploration of Trend-Cycle Decomposition Methodologies in Simulated Data\npaperreadonline"},"@cochraneRiskReturnVenture2005":{"title":"The risk and return of venture capital","links":["Public-and-Private-Valuations-are-not-Comparable","Transaction-Comps-are-not-Valuation-Comps","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"The risk and return of venture capital\nJohn H Cochrane\n\nProblem: selection bias is rampant in venture capital, as we only observe valuations when a firm has an event, i.e. going public, raising a new round of private capital, being acquired, etc. These events are all more likely when a startup has experienced a high return. As a result, at any given time most companies haven’t had a recent valuation event\nIdea: construct a model of 1) how venture valuations evolve over time and 2) how this selection process occurs as a function of firm value and use maximum likelihood to fit the model to the measured return data\n\nRelated to Public and Private Valuations are not Comparable and Transaction Comps are not Valuation Comps\n\nReferences\npaperreadonline"},"@crouzetRDUncertaintyCycles":{"title":"R&D Uncertainty and Cycles","links":["tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"R&amp;D Uncertainty and Cycles\nNicolas Crouzet, Janice Eberly\nwww.aeaweb.org/webcasts/2026/intangible-capital\nSome interesting ideas here re investment under uncertainty. Standard thinking is that uncertainty reduces investment. But for R&amp;D style investment it can actually increase investment because the only way to learn about the potential value is to invest more and the uncertainty increases upside volatility. In traditional investment, the uncertainty comes from outside the system / project, and yes that does reduce investment, since that uncertainty / fluctuation in the value of the project is going to happen irrespective of what you do, so it’s better to just sit on your hands and delay investment until things stabilize. With R&amp;D, the uncertainty is internal to the project itself.\n\nReferences\npaperreadonline"},"@crumpSparseTrendEstimation2023":{"title":"Sparse Trend Estimation","links":["@phillipsBoostingWhyYou2021","@parkBayesianLasso2008","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Sparse Trend Estimation\nRichard K. Crump, Nikolay Gospodinov, Hunter Wieman – 2023\n\n\n                  \n                  Abstract \n                  \n                \n\nThe low-frequency movements of many economic variables play a prominent role in policy analysis and decision-making. We develop a robust estimation approach for these slow-moving trend processes which is guided by a judicious choice of priors and is characterized by sparsity. We present some novel stylized facts from longer-run survey expectations that inform the structure of the estimation procedure. The general version of the proposed Bayesian estimator with a slab-and-spike prior accounts explicitly for cyclical dynamics. The practical implementation of the method is discussed in detail and we show that it performs well in simulations against some relevant benchmarks. We report empirical estimates of trend growth for U.S. output (and its components), productivity and annual mean temperature. These estimates allow policy makers to assess shortfalls and overshoots in these variables from their economic and ecological targets.\n\n\nThe authors examine the evolution of long-horizon forecasts of the US economy and note that these forecast change quite slowly over time. Thus the first differences of these forecasts are usually zero, with rarer deviations in either direction when the forecast does change. The same is true of the second differences of these forecasts. These distributions don’t appear to be consistent with continuous distributions like the normal distribution. They are more consistent with mixture distributions.\nThis feels like a relatively “doable” paper to replicate\nThe March 2024 revision of the paper is definitely cleaner and easier to understand, though I’m biased from having read the first version of the paper.\nOne thing to notice is that though they omit any multivariate designation for the normals that define the distributions of the observed data and trend, you can tell they are multivariate by the fact that they are parameterized with vectors for the means and matrices for the variance.\nThe footnote 10 on page 8 of the August 2024 edit is very telling and aligned with some of how I’ve been thinking about this method. The paper has a whole bunch of intricate statistical setup for its Bayesian method, but at the end of the day this is just modeling something as I(2) where the innovations are Laplace distributed (technically a mixture of two Laplaces) along with a cycle where the autoregressive coefficients are also Laplace distributed. That’s it. The rest is really just detail. It makes me wonder if there could be a much simpler specification achieved via MLE or Kalman filter.\nGiven they do provide the formulation of the equivalent minimization problem in the paper, it may be possible to use standard optimization techniques, though I worry about computational difficulty given you’re effectively optimizing over each data point, leading to many variables.\nReplication\nSome interesting reflections after attempting to replicate the paper multiple times, with the most success this was recent attempt:\n\nWhen people say that a Laplace distribution is a scale mixture of normals with exponential mixing density, what they literally mean is that the variances are exponentially distributed, meaning they get less frequent the larger they are. How the infrequency scales with the variances depends on the \\lambda parameter, i.e. the variance of the original Laplace. The higher the variance of the original Laplace, the lower the \\lambda, the higher variance the exponential distribution, the less high variance normals are effectively “penalized”.\nYou are effectively sampling various normals from a single exponential distribution. It’s not that there are multiple different exponentials – there is only one, characterized by \\lambda, from which you sample all the variances, i.e. all the normals. It just so happens that when you sample a bunch of numbers from an exponential, call those things “variances” and use them to parameterize a bunch of normals centered around the same mean, you get a Laplace. That just happens by definition, that’s just the math. The rarity of higher variances comes for free via the exponential distribution. When combined with those relative frequencies, you get a Laplace.\nThe normals are integrated over to get the final Laplace distribution. The exact way this is done is fairly complicated in its full form, but it’s basically integrating over the normals conditional on their variances, then integrating over the variances themselves. So if the normals are f(x|s)  and the exponentials are p(s|a) where s is the variance of the normals and a is the scale parameter of the exponentials, then the integral for the density is \\int_0^\\infty f(x|s)p(s|a)ds which evaluates out to the Laplace density \\frac{a}{2}e^{-a|x|}\nYou need to use a mixture to represent the spike and slab density, since the whole point is that a single parameterized Laplace distribution can’t accommodate the stylized facts we see in the data – lots of zeros and a small number of larger magnitude changes. This was originally confusing because I thought there was only one mixture, the one generating the Laplace if you use the hierarchical setup. But no, you need to setup a mixture, because although there are “2 Laplaces” you are only pulling a single sample for each time period. It’s not a separate sample for each Laplace for each time period. The only way to set that up is via a mixture distribution, which gives you one effective distribution. Now, because you use a Bernoulli to pick between them, any given time period is only effectively using one Laplace or the other.\nI’m still not sure if its critical to use the mixture of normals approach. My sense is that they use that so that they can use a Gibbs sampler, but I don’t know enough about Gibbs sampling to know if I’m right on that. For my purposes it’s much simpler to just use Laplace distributions directly, though now with my new understanding maybe it wouldn’t be so bad to rewrite using normals and exponentials… I need to make sure that the “scale mixture of normals” can be represented in PyMC using the NormalMixture function or if there’s some subtle distinction there. I think the weights in that function are different than what comes out of an exponential distribution. I think the weights for the NormalMixture should actually be the relative frequency of the variances you are sampling from the exponential, not the variances themselves which is what I was previously doing.\nSomething seems to just completely blow up when you expand the sample size. The time to take to run the routine grows dramatically if you add just 5 more years of data, something like 50x. Strange.\n\n\nReferences\n@phillipsBoostingWhyYou2021\nThe king must die\n\nReally good discussion of why The Bayesian Lasso is not very good in practice\nSimilar to why this paper even exists, the Bayesian Lasso can’t properly balance between big and small values, so it ends up trying to compromise which shrinks the larger values too much and doesn’t shrink the small values enough\nThe core reason for this is that a single Laplace prior distribution doesn’t place enough probability mass near the spike at zero relative to the mass in the tails to ensure that you get a reasonable level of sparsity\n\npaperreadonline"},"@demirerEmergingMarketIntelligence":{"title":"The Emerging Market for Intelligence: Pricing, Supply, and Demand for LLMs","links":["@melitzDynamicOlleyPakesProductivity2015","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"The Emerging Market for Intelligence: Pricing, Supply, and Demand for LLMs\nFradkin Demirer, Peng Tadelis\nCost per token at the offered price is stable\nBut intelligence per token is up (→ token per intelligence is down)\nSo cost per unit of intelligence is down\nWeighted token price is actually up in 2025 (people shifted toward higher quality / more expensive models)\nThis is classic Simpson’s Paradox – across models price and quantity might actually be positively correlated, but within model they are negatively correlated\nAlso interesting that high throughput only matters without the model fixed effects, but once you have those it basically goes to zero. This implies that higher throughput models are used more (across models) but within model throughput is not important to usage patterns (or even slightly negative)\nLatency is like price – it matters more when you control for model, which means that in the unconditional data latency is positively correlated with quantity (high latency models are used more)\nContext length is like price – it’s positively correlated with quantity, but that is greatly reduced within model. It’s still a positive even within model though, but it’s a noisy relationship\nInteresting price trends, shows that most of the decline in price is from new cheaper model rather than old models getting cheaper. Also shows that a given closed source model doesn’t really get cheaper over time, only open source models get cheaper over time\n\nCan also see this visually:\n\n\nCould do something like @melitzDynamicOlleyPakesProductivity2015 to break down price changes between new models and existing models getting cheaper.\nAlso interesting that token weighted prices actually increased during 2025. Also points out that closed source models rarely change their pricing:\n\nPricing across categories much higher variance than intelligence by category, suggesting intelligence can’t explain. Rank correlation also appears to be moderate\n\n\nReferences\npaperreadonline"},"@dubeLocalProjectionsApproach":{"title":"A Local Projections Approach to Difference-in-Differences Event Studies","links":["Difference-in-Differences","difference-in-differences","Local-projections","Fixed-effects","@hoyosTariffsGrowthHeterogeneous","tags/literature/paper","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"A Local Projections Approach to Difference-in-Differences Event Studies\nArindrajit Dube, Daniele Girardi, Alan M Taylor\nLink\nVideo\nVideo\nAbstract\n\nRecent applied microeconometrics research proposes various difference-in-differences (DiD) estimators for the problem of dynamic heterogeneous treatment effects. We show that the problem can be resolved by the local projection (LP) estimators of the sort used in applied macroeconometrics. Our proposed LP-DiD estimator provides an overarching toolkit with several advantages. First, the method is clear, simple, easy to compute, and transparent and flexible in its handling of treated and control units. Second, it is quite general, including its ability to control for pre-treatment values of the outcome and of other covariates, as under conditional common trends. Third, the LP-DiD can nest other estimators, providing a framework that is not only rigorous but also encompassing. The LP-DiD estimator does not suffer from the negative weighting problem, and indeed can be implemented with any weighting scheme the investigator desires. Simulations demonstrate the good performance of the LP-DiD estimator in common settings. Two empirical applications illustrate how LP-DiD addresses the bias of conventional fixed effects estimators, leading to potentially different results.\n\nThe LP-DiD estimator is implemented via OLS via either sample restriction or including interactions against whether or not a particular unit is an unclean control at each point in time. Unclean controls are those units which are treated at some point in time other than the current period.\nThe LP-DiD estimator regresses future outcomes on a \\Deltatreatment indicator, \\Deltaoutcome lags,  \\Delta covariate lags, and time fixed effects. When the full sample is used, lagged outcomes, covariates, and time fixed effects are interacted with the unclean control indicator and the indicator is additional included as a regressor.\nMakes the note referred to in @hoyosTariffsGrowthHeterogeneous that you don’t need unit fixed effects for panel local projections with long differenced outcome variables:\n\nIn applications of the local projections estimator, it is common to employ the long difference \\Delta_ky_{it} ≡ y_{i,t+k} – y_{i,t–1} on the left side of the estimating equation, especially when y_{it} is expressed in logs. A reason is that 100 \\times \\beta_k can then be interpreted as an approximate percentage change in the outcome at time t + k due to treatment at time t, facilitating interpretation of effect sizes. ==This transformation also has the advantage of mechanically removing unit-specific fixed effects.==\n\nThe dropping of unclean controls doesn’t lead to as much data loss as you might think. It’s not that those future periods aren’t included. They are included as the dependent variable in the samples where the treatment originally happened.\n\n\nReferences\npaperonline"},"@fernaldDisappointingRecoveryOutput2017":{"title":"The Disappointing Recovery of Output after 2009","links":["@hodrickExplorationTrendCycleDecomposition2020","@hallWhyHasUS","stationarity","Dynamic-factor-model","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"The Disappointing Recovery of Output after 2009\nJohn Fernald, Robert Hall, James Stock, Mark Watson\nLink\nAbstract\n\nU.S. output has expanded only slowly since the recession trough in 2009, even though the unemployment rate has essentially returned to a precrisis, normal level. We use a growth-accounting decomposition to explore explanations for the output shortfall, giving full treatment to cyclical effects that, given the depth of the recession, should have implied unusually fast growth. We find that the growth shortfall has almost entirely reflected two factors: the slow growth of total factor productivity, and the decline in labor force participation. Both factors reflect powerful adverse forces that are largely unrelated to the financial crisis and recession—and that were in play before the recession.\n\nSome weird arbitrary choices in this paper but per @hodrickExplorationTrendCycleDecomposition2020 this is supposedly an example of “gold standard” econometric research, so I’ll try to follow the logic. I do think the use of unemployment to measure the state of the cycle is interesting and potentially applicable to other domains.\nNote that they are decomposing the growth rate itself, not the level of the series.\nWhen measuring growth rates they use the annualized Q/Q growth rate, which I’m not used to seeing but perhaps worth thinking about for my purposes. I’d imagine that this would tend to be highly seasonal (EDIT: Likely using seasonally adjusted data so this point is moot). Seems like the key to this is that changes in unemployment period to period have an expected value of zero. Thus a cycle that is a linear combination of various leads and lags of the change in unemployment will also have a zero expected value, though it might fluctuate up and down as we’d hope a cycle would.\nUnemployment recovers fairly consistently across recessions, as per @hallWhyHasUS, but output seemed to recover much more slowly in the financial crisis than in prior recessions. This paper posits that the reason lies in the trend level of GDP having slowed substantially during this time, such that the contemporaneous output gap was less pronounced than commonly believed.\nKey assumption is that the capital-output ratio should be stationary and reasonably stable over time. Thus increases and decreases can be interpreted as strong signals of either capital surplus and shortfall relative to the “ideal” level.\nGood example of a DFM, estimated on detrended growth rates of various economic variables. Here the DFM is used to forecast the cyclical component of each series, while the trend in each series is assumed to be constant at wherever it ended at the 2009 trough:\n\nThe 123 series are transformed into growth rates (for activity variables; see the online appendix for the details of other series); low-frequency trends are extracted, as discussed above; and six factors are then estimated using principal components.\n…\nIn the notation of equation 6, the factor model forecast of y_t is the sum of the trend projection μt and the projection of c_t computed using the detrended factors. Thus, the forecast error is an estimate of the irregular part z_t; subtracting this forecast error measures the growth shortfall of y_t.\n\n\n\nStandard deviation of GDP components  \n\n\nReferences\npaperreadonline"},"@fleuretFreeTransformer2025":{"title":"The Free Transformer","links":["tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"The Free Transformer\nFrançois Fleuret – 2025\nMaking latent variables explicit (explicitly conditioning on them) substantially simplifies probabilistic modeling. Attempting to model densities as autoregressive, which is to say, without explicitly conditioning on latent variables but instead conditioning only on past values, generates tons of complexity.\nWhy does this happen? (Hypothesis) When you don’t explicitly include the relevant latent variables, you implicitly make inferences about what they might have been based on the realized values of the prior values of the thing you’re modeling. You look at those past values and essentially come up with an educated guess for what value the latent variable took on. The problem is that this educated guess can be extremely complicated to generate precisely.\n\n\n                  \n                  Quote\n                  \n                \n\nIt requires an unnecessarily complicated computation, and greater capacity, to implicitly make post-hoc decisions or infer latent quantities from the generated tokens.\n\n\nAside from complexity, autoregressive modeling can set you astray if some of the tokens are way off the mark, making it hard to draw the proper inferences about the latent variable and thus the correct next token.\n\n\n                  \n                  Quote\n                  \n                \n\nIt may be sent off track during the process if, by mistake, a few tokens generated are erroneous, ambiguous or contradictory with those generated previously.\n\n\nThe authors propose using what is in effect a VAE to learn a “good” distribution for latent variable Z, Q(Z|S), the goal of which is to sample latents which are useful in structuring the generative process for S, the training sample. We can’t really know in advance what sorts of Zs will be helpful or that the model will choose to learn during training.\n\nReferences\nwww.youtube.com/watch\npaperreadonline"},"@gansORingAutomation":{"title":"O-Ring Automation","links":["tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"O-Ring Automation\nJoshua S Gans, Avi Goldfarb\nKey takeaways in an O-ring production process where task quality contributes multiplicatively to output:\n\nAutomating one task changes the quality of workers’ contributions to other tasks\nAutomation decisions are discrete – it’s optimal to automate multiple tasks at a time as a bundle rather than the “marginal” “continuous” task\nLabor income doesn’t necessarily fall under partial automation and can in fact rise because automation grows the value of the remaining manual tasks, which act as bottlenecks\n\n\nReferences\npaperreadonline"},"@gorodnichenkoForecastErrorVariance2020":{"title":"Forecast Error Variance Decompositions with Local Projections","links":["Local-projections","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Forecast Error Variance Decompositions with Local Projections\nYuriy Gorodnichenko, Byoungchan Lee\nLink\nAbstract\n\nWe propose and study properties of an estimator of the forecast error variance decomposition in the local projections framework. We find for empirically relevant sample sizes that, after being bias-corrected with bootstrap, our estimator performs well in simulations. We also illustrate the workings of our estimator empirically for monetary policy and productivity shocks.\n\nA simple method for estimating forecast error decompositions for local projections.\n:\n\nCalculate forecast errors for the endogenous variable of interest at each horizon h based on the information set at t-1 as the residuals of a regression of the change in the endogenous variable from t-1 \\rightarrow h  on lags of all variables\nFor each horizon h, regress the forecast errors at that horizon onto the shock innovations up through that horizon\nThe R^2 of those regressions is the forecast error variance due to the shock\n\n\nThis quantity can be understood as an R2 of the population projection of ft+h|t−1 on Zh t , or the probability limit of sample R2’s. This observation suggests a natural estimator of sh. First, the forecast errors for each horizon h are estimated using local projections. Second, the estimated forecast errors for the horizon h at time t are regressed on shocks that happen between t and t + h.The R2 in this regression is an estimate of sh.\n\nThe forecast errors can be interpreted as the change in the endogenous variable that couldn’t have be forecasted based on available information. The size (variance) of these errors will of course tend to grow with the horizon. Some portion of this variance is due to the shock variable, while the rest could be due to any number of factors (including direct shocks to the endogenous variable itself).\nIt might be easiest to simply include the future values of the shock of interest when running the forecast error regression above and see how much of a difference they make to the R-squared, in other words, identify the partial R^2 of those shocks.\n\nNote that one may implement this estimator by augmenting Equation (5) with shocks z_t, …, z_{t+h} and calculating the partial R^2.\n\n\nReferences\npaperreadonline"},"@gouletcoulombeNeuralPhillipsCurve2022":{"title":"A Neural Phillips Curve and a Deep Output Gap","links":["tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"A Neural Phillips Curve and a Deep Output Gap\nPhilippe Goulet Coulombe\nLink\nVideo\nAbstract\n\nMany problems plague the estimation of Phillips curves. Among them is the hurdle that the two key components, inflation expectations and the output gap, are both unobserved. Traditional remedies include creating reasonable proxies for the notable absentees or extracting them via some form of assumptions-heavy filtering procedure. I propose an alternative route: a Hemisphere Neural Network (HNN) whose peculiar architecture yields a final layer where components can be interpreted as latent states within a Neural Phillips Curve. There are benefits. First, HNN conducts the supervised estimation of nonlinearities that arise when translating a high-dimensional set of observed regressors into latent states. Second, computations are fast. Third, forecasts are economically interpretable. Fourth, inflation volatility can also be predicted by merely adding a hemisphere to the model. Among other findings, the contribution of real activity to inflation appears severely underestimated in traditional econometric specifications. Also, HNN captures out-of-sample the 2021 upswing in inflation and attributes it first to an abrupt and sizable disanchoring of the expectations component, followed by a wildly positive gap starting from late 2020. HNN’s gap unique path comes from dispensing with unemployment and GDP in favor of an amalgam of nonlinearly processed alternative tightness indicators – some of which are skyrocketing as of early 2022.\n\nHard to say how interesting this is without diving into the architecture. At a high level doesn’t sound particularly innovative. It’s basically a GAM where each component is a neural net. Could be wrong though, I haven’t read the paper, just watched the video above.\nThe separation of the network into different “hemispheres” is really what gives the model structure and allows you to interpret the various components. If you don’t do this then the output neurons would be effectively meaningless since they would each combine information from various inputs in complicated, black-box ways. Instead, by separating the pieces you end up with separate components which map to particular input information. Adding these components in simple linear fashion is also a form of structure, ensuring that their contributions remains linearly additive and separable.\n\nReferences\npaperreadonline"},"@greenwaldHowWealthWas":{"title":"How the Wealth Was Won: Factor Shares as Market Fundamentals","links":["Vector-autoregression","reduced-form","shock","Autoregressive-models","random-walk","unit-root","The-Universal-Law-of-SaaS-Growth","Backing-into-ARR","@greenwaldOriginsStockMarket2014","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"How the Wealth Was Won: Factor Shares as Market Fundamentals\nDaniel L Greenwald, MIT Sloan, Martin Lettau, Sydney C Ludvigson\nLink\nVideo\nVideo\n\n\n                  \n                  Abstract \n                  \n                \n\nWhy does the stock market rise and fall? From 1989 to 2017, the real per-capita value of corporate equity increased at a 7.5% annual rate. We estimate that 44% of this increase was attributable to a reallocation of rewards to shareholders in a decelerating economy, primarily at the expense of labor compensation. Economic growth accounted for just 25% of the increase, followed by a lower risk price (18%), and lower interest rates (14%). The period 1952 to 1988 experienced less than one third of the growth in market equity, but economic growth accounted for more than 100% of it.\n\n\nThe related work section of this paper is a good discussion of some of the different trade-offs of different methods within the context of economic modeling. On the one hand, VAR models are highly flexible but entirely statistical, thus lacking economic content outside of the rotation of the reduced form residuals to get the structural shocks. On the other hand, structural modeling is less flexible but add economic theory, aiding interpretation of various drivers and forces. While it’s not discussed directly in the paper, I would guess that an additional benefit of structural modeling is that it more easily let’s you work with non-stationary variables, which are difficult to deal with directly with purely statistical approaches for obvious reasons.\nIf I want to advance my ability to make economically meaningful claims within my essays, all roads seems to eventually lead to structural modeling.\n\n\n                  \n                  Quote\n                  \n                \n\nOur work also relates to the literature estimating log-affine SDFs in reduced form.12 These studies describe the evolution of the state variables and the SDF in purely statistical terms, for example using an estimated vector autoregression (VAR) for state dynamics. While less statistically flexible, our work features more economic structure, using separate and mutually uncorrelated fundamental components, as well as parametric restrictions on the SDF exposures obtained from theory, such as the leverage risk effect. This structure allows a much clearer interpretation of the drivers of asset prices. For example, unlike VAR-based models, which face the difficult task of transforming reduced-form residuals into identified structural shocks, our model allows us to directly read off the contribution of each latent state. We thus complement this literature by providing economic insight on the economic sources of market fluctuations, particularly the role of factor shares.\n\n\nHas a kind of cool, natural definition of operating leverage nested within their model – whenever one line item grows faster than another in response to the same core impulse, you can think of that as operating leverage. As with debt, which causes returns to grow faster than the thing that is driving the returns, operating leverage means your outputs grow faster than your inputs. Here, the input is a change in the earnings share and the output is a change in payouts to shareholders as a proportion of output (revenue).\n\n\n                  \n                  Quote\n                  \n                \n\nImportantly, (3) implies that the volatility of cash flow growth is amplified relative to earnings share growth — a form of operating leverage. For example, if ω = 6%, then an increase in the earnings share St from 12% to 18% increases the cash flow share from 6% to 12%. As a result, proportional growth in the cash flow share (100%) is twice as large as in the earnings share (50%), a phenomenon that we call the leverage effect. We note that this leverage effect should hold on average even if the reinvestment share is not exactly constant, so long as investment at long horizons is proportional to output rather than earnings.\n\n\nInteresting to think about variables having low frequency and high frequency components, where the main distinguishes characteristic is the persistence of the components. In a simple AR(1) autoregressive setup, this implies that the coefficient on the lag is higher for the low frequency component than the high frequency. In the most extreme case, the low frequency component could be model as a “permanent component” that evolves as a random walk (unit root) whereas the high frequency component could be modeled as a “transitory” white noise process. (Relevant to my work on The Universal Law of SaaS Growth, Backing into ARR)\n\n\n                  \n                  Quote\n                  \n                \n\nWe choose a two-component mixture for each process to allow the model to flexibly capture both high and low frequency variation in the latent states. Since equity gives its owners access to profits for the lifetime of the firm, it is a heavily forward-looking asset that is much more influenced by persistent rather than transitory fluctuations. Our mixture specification allows the model to accurately capture both low frequency movements that have greater impact on equity prices, as well as higher frequency movements that have a smaller impact on equity prices but may nonetheless drive much of the variation in the observable series. Correspondingly, we refer to the components of each latent state vector as the high or low frequency component\n\n\nOne thing I worry about with this paper is that in the structural model they take movements in the earnings share to be exogenous, which is kind of funny given that the hypothesis of the paper is that movements in the earnings share account for much of U.S. equity growth in the last few decades:\n\n\n                  \n                  Quote\n                  \n                \n\nIn our estimation, we assume an exogenous process for St that does not directly distinguish between shifts in these components, but note that our results in section II.B imply that most variation in St is driven by the labor share of domestic value added. Although we do not directly microfound variation in St, we note that it would be isomorphic to stochastic variation in a in a model where workers are paid their marginal product.\n\n\nBy taking all movements in the earnings share to be exogenous you sort of give it the maximum possible power to explain the data, since nothing else really causes the earning shares to move within the model. All of its movement is attributable to itself effectively.\nThis modeling assumption also makes me wonder whether you really need all this machinery in the first place. Again, if the fluctuation of the earnings shares is taken to be exogenous, you could just takes those earnings share shocks and plug them in as the impulse variable in a local projection and traces out the impulse response of the U.S. equity market to those movements.\n\nReferences\n@greenwaldOriginsStockMarket2014\npaperreadonline"},"@greenwaldOriginsStockMarket2014":{"title":"Origins of Stock Market Fluctuations","links":["Vector-autoregression","Moving-average-model","Don't-Discount-Interest-Rates","tags/literature/paper","tags/online"],"tags":["literature/paper","online"],"content":"Origins of Stock Market Fluctuations\nDaniel Greenwald, Martin Lettau, Sydney Ludvigson\nLink\n\n\n                  \n                  Abstract \n                  \n                \n\nThree mutually uncorrelated economic disturbances that we measure empirically explain 85% of the quarterly variation in real stock market wealth since 1952. A model is employed to interpret these disturbances in terms of three latent primitive shocks. In the short run, shocks that affect the willingness to bear risk independently of macroeconomic fundamentals explain most of the variation in the market. In the long run, the market is profoundly affected by shocks that reallocate the rewards of a given level of production between workers and shareholders. Productivity shocks play a small role in historical stock market fluctuations at all horizons.\n\n\n\n\n                  \n                  Summary \n                  \n                \n\nOverall a very interesting paper showing that most of the growth in the detrended value of the stock market over time is attributable to changes in the labor share. Important to my own efforts, demonstrates a decomposition of a non-stationary series into components attributable to various orthogonal shocks using a Cholesky decomposition within a VAR approach. Also includes some helpful discussion of the moving average representation of the VAR.\n\n\nThis uses a very similar methodology to the one I use in Don’t Discount Interest Rates to decompose the level of stock market wealth into various drivers. Notably, the authors argue for using separate regressions when regressing the outcome variable on the shocks, using a different regression for each shock rather than stuffing them all into the same regression. This works since all the shock are uncorrelated by assumption. This is obviously an important assumption, but clearly it can be rigorously justified.\n\nSince e_{c,t}, e_{y,t} and e_{a,t} are mutually uncorrelated and i.i.d., we estimate these equations separately by OLS with L = 16 quarters.\n\nThey do the same thing I do where I get the cumulative decomposition by summing up the effects on the first differences.\n\nThe effect on the log levels of stock wealth of each disturbance is obtained by summing up the effects on the log differences\n\nThe one area where they do differ is that they include a deterministic linear trend in stock market wealth (edit: not sure this is actually different from my approach, it’s implicit since the underlying regressions are on first differences), which of course soaks up some of the non-stationarity and leaves less room to be explained by the shocks. However, it seems in reporting their results they report the explained variance of the detrended series, so it’s still possible for the shocks to “fully explain” stock market wealth over time. This feels OK – it makes sense in the context of the stock market to have a deterministic term since there does seems to be a strong force driving the market up over time that’s probably out of scope of this paper to explain.\nOne interesting chart was one showing the detrended stock market value against the cumulated factor share shock. They move very closely together, suggesting a tight relationship. Perhaps interesting for Don’t Discount Interest Rates, though I have a feeling this kind of chart would be tricky to explain to folks (cumulated shock? detrended stock market?): \n\nReferences\npaperonline"},"@haddadHowCompetitiveStock2021":{"title":"How Competitive is the Stock Market? Theory, Evidence from Portfolios, and Implications for the Rise of Passive Investing","links":["@huStatisticalArbitrageUncertain","@koijenDemandSystemApproach","tags/online"],"tags":["literature","inbox/read","online"],"content":"How Competitive is the Stock Market? Theory, Evidence from Portfolios, and Implications for the Rise of Passive Investing\nValentin Haddad, Paul Huebner, Erik Loualiche\nVideo\nAbstract\n\nWe develop a framework to theoretically and empirically analyze investor competition on financial markets. The classic view assumes that markets are very competitive: if a group of investors changes its behavior, other investors react such that nothing happens in equilibrium. Our framework quantifies the strength of the competitive response. We estimate a demand system of institutional investors in the US stock market accounting for two layers of equilibrium: how investors compete with each other in setting their strategies and how prices adjust to clear asset markets. We find that investors react to the behavior of others in the market: when an investor is surrounded by less aggressive traders she trades more aggressively. This reaction reduces the equilibrium consequences of changes in individual behavior by 50%. However, it also implies that the stock market is far from the competitive ideal. A consequence of this result is that the large increase in passive investing over the last 20 years has led to substantially more inelastic aggregate demand curves for individual stocks, by 15%.\n\nHighlights\nEach investor demands some amounts of an asset, decreasing in the price of the asset by the elasticity:\nd_i = \\underline{d}_i - \\mathcal{E}_i \\times p\nIn equilibrium, demand sums up to supply:\n\\int_{i} D_{i}(p) = S\nElasticity is determined by individual effect and response to aggregate demand elasticity:\n\\mathcal{E}_i = \\underline{\\mathcal{E}_i} - \\chi \\times \\mathcal{E}_{agg}\nIn equilibrium, aggregate elasticity is equal to average investor elasticity (weighted by demand):\n\\int_i \\mathcal{E}_i D_i / S = \\mathcal{E}_{agg}\nCan think about \\chi as representing competition. If \\chi = 0, there is no competition — each investor follows their own strategy totally independent of the rest of the market. In perfect competition, \\chi = \\infty, and any change is completely counteracted by investor reaction. Related to Statistical Arbitrage with Uncertain Fat Tails\nLarge increase in passive investors over time. In no competitive world, this lead to proportional reduction in elasticity. In perfectly competitive market, increase in passive investors has no effect on elasticity.\nAuthors estimate that \\chi = 1.7, relatively stable over time.\nFollow similar methodology as A Demand System Approach to Asset Pricing, with logit specification for portfolio shares w_{ik}. Track portfolio shares relative to price as a measure of demand, modeled as investor-specific function of observables and investor-specific elasticity:\n\\log{\\frac{w_{ik}}{w_{i0}} - p_k} = \\underline{d}_{0i} + \\underline{d}&#039;_{1i}X_k - \\mathcal{E}_{ik}p_k + \\epsilon_{ik}\nInvestor-specific elasticity modeled as investor-specific function of observables and market elasticity:\n\\mathcal{E}_{ik} = \\underline{\\mathcal{E}}_{0i} + \\underline{\\mathcal{E}}&#039;_{1i}X_k - \\chi \\mathcal{E}_{agg,k}\nInvestors respond less to price movements for assets with more aggressive investors than for assets with less aggressive investors. If all other investors are more elastic by 1, lower my elasticity by 1.7.\nElasticities in general are low — this study finds 0.3. Price elasticity in the stock market is lower for large companies, approaching zero for the largest stocks:\n\n\nonline"},"@hallWhyHasUS":{"title":"Why Has the US Economy Recovered So Consistently from Every Recession in the Past 70 Years?","links":["Rober-Shimer","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Why Has the US Economy Recovered So Consistently from Every Recession in the Past 70 Years?\nRobert E Hall, Hoover Institution, Marianna Kudlyak\nLink\nVideo\nAbstract\n\nA remarkable fact about the historical US business cycle is that, after unemployment reached its peak in a recession, and a recovery begins, the annual reduction in the unemployment rate is stable at around one tenth of the current level of unemployment. For example, when the unemployment rate was 7 percent at the beginning of a year, the unemployment rate fell by 0.7 percentage points during the year. The economy seems to have an irresistible force toward restoring full employment. There was high variation in monetary and ﬁscal policy, and in productivity and labor-force growth during the recoveries, but little variation in the rate of decline of unemployment. We show that the evolution of the labor market involves more than the direct eﬀect of persistent unemployment of job-losers from the recession shock—unemployment during the recovery is elevated for people who did not lose jobs during the recession. We explore models of the labor market’s self-recovery that imply gradual working oﬀ of unemployment following a recession shock. We emphasize the feedback from high unemployment to the forces driving job creation. These models also explain why the recovery of market-wide unemployment is so much slower than the rate at which individual unemployed workers ﬁnd new jobs. The reasons include the fact that the path that individual job-losers follow back to stable employment often includes several brief interim jobs.\n\nResponse by Rober Shimer:\nVast majority of the reason that unemployment spikes during recessions is that unemployed people stop finding jobs, rather than employed people suddenly being laid off. It’s the hiring freezes, not the lay offs:  \n![[CleanShot 2022-08-13 at 10.20.03@2x.png]]\n\nReferences\npaperreadonline"},"@hamiltonPrincipalComponentAnalysis":{"title":"Principal Component Analysis for Nonstationary Series","links":["Principal-component-analysis","stationarity","Spurious-correlation","OLS","Hamilton-filter","@hamiltonWhyYouShould2017","data-generating-process","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Principal Component Analysis for Nonstationary Series\nJames D Hamilton, Jin Xi\nLink\nAbstract\n\nThis paper develops a procedure for uncovering the common cyclical factors that drive a mix of stationary and nonstationary variables. The method does not require knowing which variables are nonstationary or the nature of the nonstationarity. Applications to the term structure of interest rates and to the FRED-MD macroeconomic dataset demonstrate that the approach offers similar benefits to those of traditional principal component analysis with some added advantages.\n\nMethod for conducting Principal component analysis for non-stationary data.\nAvoid standard, static PCA when working with non-stationary data, as there’s a tendency for the factors that emerge to be spurious: they won’t represent the true factor structure if there is one, and they’ll tend to invent factors where none exist like how OLS generates correlations in sample where none exist in population. This happens for the same reasons as does spurious regression with time series data:\n\nTrending data isn’t stationary.\nData that isn’t stationary doesn’t have a defined mean or variance.\nThis violates the assumption of standard estimators like OLS and destroys any guarantee of correctness for the results.\nLike OLS, PCA is a least squares estimator and thus inherits similar strengths and weaknesses.\n\n\nFor a nonstationary variable, the population mean is undefined and the sample standard deviation diverges to infinity as the number of time-series observations gets large.\n\nFocusing on the cyclical, non-trending portion of a series mitigates these issues. This requires a filter that can extract this component. There’s no shortage of filters, but an easy one proposed by the same author of this method is the Hamilton filter (Why You Should Never Use the Hodrick-Prescott Filter):\n\nThe Hamilton filter extracts a stationary cyclical component from a diverse array of time series DGPs\nIt’s simple to use, only requiring running OLS on each series controlling for lagged values of the variable (with at least some delay) and extracting the residuals from this regression, which represent the cyclical component\nThis now stationary data form the input to PCA, which will yield factors which are themselves stationary and non-spurious\n\nSome interesting notes:\n\nUse log transformation (\\log X_t) for most variables, e.g. ones you would normally first difference. If a variable has negative values, use X_t / X_{t-1} instead\nUse lags that add up to a year’s worth of observations to account for seasonality\n\n\nReferences\npaperreadonline"},"@hodrickExplorationTrendCycleDecomposition2020":{"title":"An Exploration of Trend-Cycle Decomposition Methodologies in Simulated Data","links":["Robert-Hodrick","Hodrick-Prescott-Filter","Random-walk","stationarity","Stochastic-trend","Hamilton-filter","Autoregressive-models","@hamiltonWhyYouShould2017","@fernaldDisappointingRecoveryOutput2017","@cochraneHowBigRandom1988","tags/literature/paper","tags/inbox/process","tags/online"],"tags":["literature/paper","inbox/read","inbox/process","online"],"content":"An Exploration of Trend-Cycle Decomposition Methodologies in Simulated Data\nRobert Hodrick\nAbstract\n\nThis paper uses simulations to explore the properties of the HP filter of Hodrick and Prescott (1997), the BK filter of Baxter and King (1999), and the H filter of Hamilton (2018) that are designed to decompose a univariate time series into trend and cyclical components. Each simulated time series approximates the natural logarithms of U.S. Real GDP, and they are a random walk, an ARIMA model, two unobserved components models, and models with slowly changing nonstationary stochastic trends and definitive cyclical components. In basic time series, the H filter dominates the HP and BK filters in more closely characterizing the underlying framework, but in more complex models, the reverse is true.\n\nMakes the point that the HP filter tends to dominate the Hamilton filter when dealing with data that doesn’t follow relatively simple time series models like random walk or autoregressive processes.\n\nReferences\nWhy You Should Never Use the Hodrick-Prescott Filter\nThe Disappointing Recovery of Output after 2009\nHow Big Is the Random Walk in GNP?\npaperprocessonline"},"@hoyosTariffsGrowthHeterogeneous":{"title":"Tariffs and Growth: Heterogeneous Eﬀects by Economic Structure","links":["@dubeLocalProjectionsApproach","The-Universal-Law-of-SaaS-Growth","local-projections","Difference-in-Differences","selection-bias","fixed-effects","Local-projections","Impulse-responses","tags/literature/paper","tags/inbox/process","tags/online"],"tags":["literature/paper","inbox/read","inbox/process","online"],"content":"Tariffs and Growth: Heterogeneous Eﬀects by Economic Structure\nMateo Hoyos\nLink\nAbstract\n\nThis article presents evidence that the impact of tariffs on GDP per capita is mediated by economic structure. I use a panel of 161 countries from 1960 to 2019 to study the impact of changes in average tariff rates on GDP per capita. Using a local projections difference-in-differences (LP-DiD) approach allows me to flexibly control for the surge in GDP that precedes tariff reductions, which if ignored could bias the estimates, and to estimate medium-term dynamic effects. The results, consistent with a specific strand of the trade theory literature, establish that the tariff-growth nexus is contingent on economic structure: tariff reductions led to lower GDP per capita for nonmanufacturer countries, but higher GDP per capita for manufacturers. Additionally, the effects are persistent even twenty years after tariff reductions. The validity of the baseline estimates is confirmed by several robustness checks, especially the control for relevant confounders in the tariffs-growth nexus and a clean controls analysis aimed to address biases from heterogeneity as highlighted by recent differencein-differences literature. The results seem to be driven by heterogeneous effects in productivity and capital accumulation, in turn related to changes in the manufacturing share of GDP.\n\nResults\nMain finding of this paper is that the effects of globalization (proxied for by local tariffs) aren’t not uniformly positive for developing countries. The benefits depend importantly on the manufacturing export intensity of the exporter. Countries with high manufacturing export share see strong growth in GDP per capita post lowering tariffs. However, countries with low manufacturing intensity see declines in GDP per capita after dropping tariffs. \nGood example of leveraging insights and methods from @dubeLocalProjectionsApproach. Perhaps useful methodology for my The Universal Law of SaaS Growth efforts, since the setup is quite similar (panel data, per unit impulses)\nMakes the important point that local projections can suffer from pre-trends issues reminiscent of difference-in-differences if the outcome variable exhibits a trend relative to the timing of the impulse variable. It’s a form of selection bias. For example, countries that reduce tariffs in general were seeing rising GDP per capita ahead of tariff reduction, implying that the positive trend afterward may have already been baked in the cake. This kind of chart can easily be generated by simply running the same LP estimator on negative horizons (in levels, lagged outcome variable minus level at t-1). This can be avoided by including lags of the outcome variable: \n\ncountries reducing their tariffs are on different pretrends from those not changing them. In particular, the former countries display a relative surge in GDP before tariff reductions as compared to the latter. In other words, tariff changes are endogenous to the evolution of GDP, such that countries that decide to decrease tariffs do so after GDP has been on a relative increase. Failure to control for this surge constitutes a clear violation of the parallel trends assumption and may lead to biases in the treatment effect estimates.\n\nThey also test for non-linearity in the interaction with manufacturing by using six quantiles of manufacturing intensity. \nInteresting tidbits\nNotably, they only include time fixed effects in their panel local projections because the data is already differenced. This is a good point that I hadn’t previously considered, but it’s aligned with the standard time series econometrics pedagogy:\n\nI include only time fixed effects, as the equation is already in differences\n\nI do worry about applying this too broadly – in my areas of research growth rates of companies tend to be highly variable, much more so than the growth rates of countries. Seems like even after differencing if you know that there’s large differences in growth rates across units then unit fixed effects might still be a good idea.\nSimple way to eyeball how many lags you need in your local projections  to avoid pre-trend: graph the negative horizon impulse response with various lag lengths. Keep adding lags until pre-trends are no more: \nGood example of running local projections with interactions to explore heterogeneity: y_{c, t+h}-y_{c, t-1}=\\beta_h \\Delta T A_{c, t}+\\theta_h i n t_{c, t}+\\phi_h m_{c, t}+\\sum_{j=1}^8 \\sigma_h^j g_{c, t-j}+\\alpha_t+\\epsilon_{c, t}\n\nThe initial share of manufacturing exports, mc,t, is calculated as the average of this variable in the five years before tariff reductions, to avoid contemporaneous endogeneity that may run from GDP to manufacturing exports.\n\n\nWith this specification, the impact of tariff changes on growth varies with the initial level of the manufacturing share of exports. For example, if I want to calculate the cumulative change in GDP per capita at time t + h in relation to a one standard-deviation tariff reduction for a country with an initial manufacturing share of exports of 29 percent, I estimate it by calculating (-1) * S D(\\Delta T A) *\\left(\\beta_h+29 * \\theta_h\\right).\n\nInteresting definition of a clean control – country which hasn’t experienced a &gt;1 standard deviation tariff change in the last 10 years, with the term determined by observing that local projection effects seem to stabilize after 10 years.\n\nReferences\n@dubeLocalProjectionsApproach\npaperprocessonline"},"@hsuSubsetSelectionVector2008":{"title":"Subset selection for vector autoregressive processes using Lasso","links":["LASSO","Vector-autoregression","@wangRegressionCoefficientAutoregressive2007","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Subset selection for vector autoregressive processes using Lasso\nNan-Jung Hsu, Hung-Lin Hung, Ya-Mei Chang\nLink\nAbstract\n\nA subset selection method is proposed for vector autoregressive (VAR) processes using the Lasso [Tibshirani, R. (1996). Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society, Series B 58, 267–288] technique. Simply speaking, Lasso is a shrinkage method in a regression setup which selects the model and estimates the parameters simultaneously. Compared to the conventional information-based methods such as AIC and BIC, the Lasso approach avoids computationally intensive and exhaustive search. On the other hand, compared to the existing subset selection methods with parameter constraints such as the top-down and bottom-up strategies, the Lasso method is computationally efficient and its result is robust to the order of series included in the autoregressive model. We derive the asymptotic theorem for the Lasso estimator under VAR processes. Simulation results demonstrate that the Lasso method performs better than several conventional subset selection methods for small samples in terms of prediction mean squared errors and estimation errors under various settings. The methodology is applied to modeling U.S. macroeconomic data for illustration.\n\nRelatively straightforward paper the merely makes the point that you can apply lasso to vector autoregressions in a fairly braindead way and get better forecasting results than picking the lag length via Bayesian criteria.\n\nReferences\nRegression Coefficient and Autoregressive Order Shrinkage and Selection Via the Lasso\npaperreadonline"},"@huangWinningGoldIMO2025":{"title":"Winning Gold at IMO 2025 with a Model-Agnostic Verification-and-Refinement Pipeline","links":["tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Winning Gold at IMO 2025 with a Model-Agnostic Verification-and-Refinement Pipeline\nYichen Huang, Lin F. Yang\nI think this approach is quite interesting and should serve a good foundational approach for building very strong solvers across multiple domains, particularly those requiring high quality reasoning.\nGenerate → Verify → Fix → Repeat Until “Convergence” or Failure\n\nReferences\npaperreadonline"},"@inseadProductivitySlowdownAdvanced2023":{"title":"The Productivity Slowdown in Advanced Economies: Common Shocks or Common Trends?","links":["tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"The Productivity Slowdown in Advanced Economies: Common Shocks or Common Trends?\nINSEAD, John Fernald, Federal Reserve Bank of San Francisco, Robert Inklaar, University of Groningen, Dimitrije Ruzic, INSEAD – 2023\nThis papers contrasts two potential explanations for the common slowdown in productivity, as measured by TFP, across multiple advanced economics post-2007:\n\nA common shock, i.e. the Great Recession\nA common slowdown in trend\n\nIn this sense the paper is one big trend / cycle decomposition, where the question is whether the change in TFP growth across these advanced economics has been driven more by a major shock or a shift in trend.\nThe reason to doubt the shock explanation a priori is that prior research doesn’t show a clear level or growth effect of recessions on TFP. The research much more strongly suggests that such recessionary shocks have a persistent effect on labor markets. Another reason to doubt this version of the story is that the slowdown in TFP for the U.S. actually predates the Great Recession, which makes it less likely as a cause.\nThe common trend slowdown could be explained by the general technology-driven productivity boost of the mid-1990s finally loosing steam by the mid-2000s.\nGrowth decomposition\nMultiple forms of an aggregate growth decomposition are defined, with various strengths and weaknesses. The author eventually lands on the following, which helps deal with some of the endogeneity issues inherent in such an accounting exercise:\n\\Delta\\ln Y_t - \\Delta \\ln H_t = \\frac{\\alpha_t}{1-\\alpha_t}(\\Delta \\ln K_t - \\Delta \\ln Y_t) + \\Delta \\ln LC_t + \\frac{\\Delta \\ln TFP_t}{1-\\alpha_t}\n\nThe left hand side represent growth in output per hour worked.\nThe first term on the right hand side represents the change in capital-output ratio, or “capital deepening”\nThe second term is change in labor composition\nThe last term of is growth in TFP in “labor-augmenting” terms\n\nThe reason to use the capital-output ratio rather than capital alone is due to the endogeneity of capital with respect to output:\n\nIn most models, capital formation (investment) is endogenous, which to say it is itself determined by output in addition to driving output. This could drive interpretation issues in such a decomposition.\nHowever, in many models capital is cointegrated with output in the steady state and their ratio is relatively stable even in during a slowdown in technology or labor. Thus any deviations of the ratio over time can be more plausibly categorized as exogenous.\n\nNote that this decomposition is effectively a productivity decomposition rather than an aggregate growth decomposition.\nThis decomposition is used across various time periods and countries to break down labor productivity growth into the various drivers:\n\n\nReferences\npaperreadonline"},"@jonesGrowthIdeas2005":{"title":"Growth and Ideas","links":["Charles-I.-Jones","Scale-invariant-growth","First-Welfare-Theorem","Perfect-competition-is-not-optimal-under-increasing-returns-to-scale","Long-run-exponential-growth-requires-scale-invariant-growth-in-at-least-one-input","Ideas-generate-increasing-returns-because-they-are-nonrivalrous","tags/literature","tags/Economics","tags/Economics/Growth","tags/Economics/Macro","tags/Economics/Competition","tags/online","Imperfections-of-perfect-competition","Convexity","You-Don't-Understand-Compound-Growth","@jonesFutureEconomicGrowth"],"tags":["literature","Economics","Economics/Growth","Economics/Macro","Economics/Competition","online"],"content":"Growth and Ideas\nCharles I. Jones\nAbstract\n\nIdeas are different from nearly all other economic goods in that they are nonrivalrous. This nonrivalry implies that production possibilities are likely to be characterized by increasing returns to scale, an insight that has profound implications for economic growth. The purpose of this chapter is to explore these implications.\n\n\nLiterature Notes\nIncluding ideas in the production function leads to increasing returns to scale, as a doubling of physical inputs and ideas more than doubles output. This is a direct result of the fact that doubling physical inputs alone already doubles output, so layering in additional ideas must lead to an even larger increase. \n\n\n\nDon’t include “per worker” variables when considering the convexity/concavity of / returns to scale of a production function\n\n\n\nGrowth models with ideas in them means that factors cannot be paid their marginal products, as the marginal products add up to more than output itself. Standard competitive equilibrium runs into problems in such models.\n\n\n\nThe linearity critique makes the point that many endogenous growth models require the assumption of a linear differential equation in order to generate stable long-run exponential growth (or as I call it, Scale invariant growth). In a linear differential equation, absolute growth of a variable is proportional to scale.\n\n\n\nAny model with long-run exponential growth will involve a linearity, not just endogenous growth models\n\n\n\nLinearity / Scale invariant growth applies most naturally to population, since living beings reproduce in proportion to their number1\n\n\n\nPerfect competition will not deliver the optimal allocation of resources (First Welfare Theorem) in growth models with ideas that generate increasing returns to scale. \n\n\n\n\nPermanent Notes\n\nPerfect competition is not optimal under increasing returns to scale\nLong-run exponential growth requires scale invariant growth in at least one input\nIdeas generate increasing returns because they are nonrivalrous\n\nliteratureEconomicsGrowthMacroCompetitiononline\nImperfections of perfect competition // Scale invariant growth // Convexity\nFootnotes\n\n\nRelated to my bacteria analogy in You Don’t Understand Compound Growth, also mentioned in The Past and Future of Economic Growth: A Semi-Endogenous Perspective ↩\n\n\n"},"@jordaModelFreeImpulseResponses2003":{"title":"Model-Free Impulse Responses","links":["Vector-autoregression","local-projections","@dubeLocalProjectionsApproach","bias","Local-projections","Impulse-responses","@plagborg-mollerLocalProjectionsVARs2021","Local-projections-vs.-VARs","@liLocalProjectionsVs2023","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Model-Free Impulse Responses\nOscar Jorda – 2003\nVARs decompose the economy (or more literally, a vector of time series) into systematic/deterministic responses and random sources of variation.\nThe central benefit of local projections is that they do not require specifying and estimating an unknown multivariate dynamic system. This is especially relevant when the data may not in fact follow such a system, perhaps because certain variables are effectively exogenous to the subsystem of variables you are most interested in. Although they are most often used in macroeonomic studies, this feature makes local projections especially useful in micro settings, hence their adoption in the causal inference setting of LP-DiD (@dubeLocalProjectionsApproach).\nVARs are in fact ideal for one-period ahead forecasting. While they can be used for longer horizon forecasting as well, this put them firmly in the realm of extrapolation, leading to bias. They are global approximations to the true dynamic multivariate system, which again, doesn’t necessarily even exist in the first place. Thus in using them you are actually making a significant assumption right off the bat, whether you realize it or not.\nIf the data do follow a VAR, then a VAR will be the most efficient estimator. However, if the data do not follow such a structure, local projections can be more robust, especially as the forecast horizon increases. There is a good argument that, unless you have strong theory which suggests the data follows a VAR, you should remain agnostic to the DGP and use a local projection to estimate impulse responses. The only caveat to this is that the two methods estimate the same impulse response up to the horizon h corresponding to the chosen lag length p, i.e. while h\\leq p (@plagborg-mollerLocalProjectionsVARs2021). After that horizon, however, all bets are off – they no longer estimate the same impulse responses unless the DGP is a VAR, which you can’t know for certain in applied settings. See Local projections vs. VARs and @liLocalProjectionsVs2023.\nVARs are especially dangerous when the data is persistent or non-stationary, as the model misspecification biases imbued by the cross-horizon restrictions get amplified. These issues are less severe when the data is stationary, since no matter what the impulse responses need to trend toward zero with the horizon length. This will happen automatically in any reasonably accurate VAR estimated on stationary data.\n\nReferences\npaperreadonline"},"@karapanagiotiModelSelectionLocal":{"title":"Model Selection for Local Projections Instrumental Variable Methods - Empirical Application to Government Spending Multipliers.","links":["Local-projections","Vector-autoregression","LASSO","data-generating-process","regularization","local-projections","Impulse-responses","Principal-component-analysis","Dynamic-factor-model","@rameyMacroeconomicShocksTheir2016","@adamekLocalProjectionInference2022","OLS","@rameyGovernmentSpendingMultipliers","@chernozhukovHIGHDIMENSIONALMETRICS","@belloniInferenceHighDimensionalSparse2011a","@adamekLassoInferenceHighDimensional2022","@vandegeerAsymptoticallyOptimalConfidence2014","@zhangConfidenceIntervalsLow2014","@hsuSubsetSelectionVector2008","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Model Selection for Local Projections Instrumental Variable Methods - Empirical Application to Government Spending Multipliers.\nChrysoula Karapanagioti\nLink\nAbstract\n\nThis thesis tackles the problem of model selection for a single equation estimation method named Local Projection with Instrumental variable. Regularization techniques that choose the model and estimate the parameter concurrently are used to estimate the impulse response functions, which is especially beneficial in vector autoregressive contexts. The main focus is upon the Desparsified Lasso method. A simulation research as well as an empirical investigation to government expenditure multipliers illustrate the usefulness of the processes in terms of forecasting and model development. In addition, the main results indicate that the Desparsified Lasso produce much closer estimated impulse responses both in DGP and empirical analysis.\n\nThis paper explores how regularization methods can be used in concert with VARs and local projections to estimate impulse responses in high-dimensional contexts. Importantly, the author claims that these estimators incorporate the temporal dynamics and spatial dependence that VARs inherently provide. Thus, we’re not necessarily losing anything or estimating the wrong estimand by using these methods, which is a worry I’ve had.\n\nRecent advances in the literature have resulted in strategies that use regularized estimating techniques to choose variables in VARs. The high-dimensional VAR model is estimated using this method. Specifically, a selection of the regularization models developed by Nicholson et al. (2020) like Componentwise HLAg, Lag-Weighted Lasso, Disparsified Lasso by van de Geer et al. (2014) , Lasso by Tibshirani (1996), and Post Double Lasso by Chernozhukov, Hansen, and Spindler (2015) are used. These models incorporate relevant information (temporal dynamics and spatial dependence) that VARs inherently provide.\n\nAlternatives for reducing the dimensionality of VAR models include:\n\nBayesian VARs\nPrincipal component analysis\nDynamic factor models\n\nThe author goes right out and acknowledges that VARs with the same number of lags for every variable might not be ideal, which is something I’ve always wondered about. @rameyMacroeconomicShocksTheir2016 makes a similar point when she notes that control variables don’t need to be the same for each horizon of a local projection. Maybe that’s not exactly the same point, but there’s a similar line of thinking running through both:\nTransclude of @rameyMacroeconomicShocksTheir2016#d3cf71\nDesparsified Lasso\nThe point of desparsifying lasso estimates (as in Local Projection Inference in High Dimensions) is to obtain “uniformly reliable inference” which seems to be a fancy way of saying unbiased estimates of the coefficients. Desparsifying effectively undoes the coefficient shrinkage.\nI don’t totally understand the point of using Lasso in the first place if you then desparsify it. It must be the case that not all variables are unshrunk or rather they are not all fully unshrunk. I guess the idea is that the standard Lasso emphasizes predictive performance rather than unbiasedness of the coefficients, and so the coefficients can be far from their true values if that helps the regression. You might want to undo this effect. I would assume from the name that what happens in practice is that many formerly zeroed out coefficients come back with small absolute values.\nThe thinking seems to go – we use Lasso because we have too many covariates, but we pay the cost of bias in the coefficients. So we undo the sparsification (presumably on only the variables that weren’t shrunk all the way to zero).\nJudging from the desparsifying equation, it seems like you take the lasso betas and add back something the roughly tracks the degree to which a predictor is correlated to the residuals, which is possible because we aren’t using OLS. This correlation is then discounted for how variable the residualized predictor is.\nLag-Weighted Lasso\nStandard lasso is totally unstructured in the sense that all predictors are treated equally. One could imagine wanting to treat different coefficients differently based on prior beliefs. For example, if the regression includes lags, you might think that more recent lags are more important than further away lags, which should be more heavily penalized.\nGroup Lasso\nReally interesting note that if you apply ridge regression but group the variables in some way and then apply the penalization to the sum of the L2-norms, the estimator will actually pick or drop entire groups, even though ridge regression doesn’t normally reduce coefficients to zero. By penalizing the sum of L2-norms, you achieve parameter selection.\nResults\nDesparsified Lasso performs the best in the simulated examples, but it’s pretty hard to tell this from the tables.\n\nIt’s outperformance is more clear in the empirical example (from @rameyGovernmentSpendingMultipliers):\n\nThis guy’s write up of the results is terrible.\nThe big takeaway from this paper is that it’s probably worthwhile to improve my understanding of the desparsified / debiased lasso. It’s doing something interesting and different from both the standard lasso and the post-(double) lasso estimator (HIGH-DIMENSIONAL METRICS IN R, @belloniInferenceHighDimensionalSparse2011a). It’s clearly trying to achieve something similar to post-lasso, which is unshrinking the coefficients, but in the case of desparsified lasso it’s unshrinking potentially all the coefficients while post-lasso only unshrinks the coefficients that were selected in the first place. Standard lasso fails because it struggles to handle both bias and variable selection at the same time.\nThese would be good places to start:\n\nLasso Inference for High-Dimensional Time Series\nLocal Projection Inference in High Dimensions\nOn asymptotically optimal confidence regions and tests for high-dimensional models\nConfidence intervals for low dimensional parameters in high dimensional linear models\n\nInteresting tidbits\n\nAssuming that the variables are all already stationary, adding lags of the outcome to the RHS helps reduce standard errors.\nInvertibility in VARs implies that structural shocks can be recovered from the data, which is to say non-invertibility means that structural shocks cannot be recovered from the data. Whenever you hear “invertibility” you should simply think “the structural shocks can be identified.”\n\n\nReferences\nSubset selection for vector autoregressive processes using Lasso\npaperreadonline"},"@kimScienceScalingAgent2025":{"title":"Towards a Science of Scaling Agent Systems","links":["tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Towards a Science of Scaling Agent Systems\nYubin Kim, Ken Gu, Chanwoo Park, Chunjong Park, Samuel Schmidgall, A. Ali Heydari, Yao Yan, Zhihan Zhang, Yuchen Zhuang, Mark Malhotra, Paul Pu Liang, Hae Won Park, Yuzhe Yang, Xuhai Xu, Yilun Du, Shwetak Patel, Tim Althoff, Daniel McDuff, Xin Liu – 2025\nKey findings\n\nTool-heavy tasks are challenging in multi-agent context\nSingle agent systems with decent performance (&gt;45%) outperform multi-agent systems\nMulti-agent systems are best for parallelizable tasks and worst for sequential tasks\nMulti-agent token budgets are 1.6-6.2x those of single agent systems for same performance\nCoordination costs scale super-linearly with environmental complexity\nThere are increasing returns to scaling model intelligence\nAgent systems cap out at 3-4 agents in practice due to context window limitations\n\n\nReferences\npaperreadonline"},"@koijenDemandSystemApproach":{"title":"A Demand System Approach to Asset Pricing","links":["Robinhood","Variance-decomposition","The-Inelastic-Demand-for-Startup-Equity","Nassim-Taleb","Robert-Shiller","tags/online"],"tags":["literature","inbox/read","online"],"content":"A Demand System Approach to Asset Pricing\nRalph S J Koijen, Motohiro Yogo\nwww.youtube.com/watch\nPermanent Notes\nHighlights\nThink about asset pricing as a demand system. Movements in each assets can be traced back to specific investors.\nEstimate demand curve for each investor. Demand for each investor is function of asset prices, asset characteristics, and demand shocks.\nHeterogenous beliefs lead to different return expectation to the same asset characteristics:\n\nParsimonious representation of factor structure in returns consists of market equity (market cap), book equity, profitability, investment, and market beta:\n\n\nCan think about polynomials as approximation to exponential curves:\n\nCharacteristics-based demand function:\n\nLatent demand is demand for characteristics that are ubobserved:\n\nCan think of weighted latent demand as measure of investor sentiment toward a stock and dispersion across investors as measure of disagreement:\n\nHeterogeneous demand elasticities:\n\nMedian institution holds 67 stocks:\n\nMarket equity is endogenous, as in its almost certainly correlated with latent demand. Thus can’t simply run OLS on regression in market equity included as the error term / latent demand is correlated with one of the regressors (market equity). To get around this, authors instrument for market equity on a per investor basis using the AUM-weighted prevalence of a stock within the investable universe of OTHER investors, the idea being that this variation is exogenous relative to latent demand of a particular, held-out investors. It’s the market equity the stock would have if all the investors who could invest in it invested an equal weighted share in it (1/N rule, effectively). I’d imagine this would yield fairly similar market equity for a given stock across each investor, but I guess it’s important not to include the investors own ownership in the instrument in order not to pollute it:\n\nInstrument works well because different institutions have small and sufficiently varied investment universes, so you actually get some variation across stocks. Wouldn’t work well if everyone was basically investing in the same set of companies:\n\nStrong first stage, though I’m not sure what they mean when they say “minimum across institutions”:\n\n\n\nHouseholds are more elastic than mutual funds, banks, pension funds etc and have become more elastic over time (Robinhood?). Mutual fund are relatively less elastic.\n\nDisagreement among households has increased over time, spiked in 2008:\n\n\nAverage price impacts of a hypothetical drawdown have lowed over time across institution types:\n\n\n\n\nVariance decomposition of cross-sectional returns. Variance in returns across stocks largely driven by extensive margin of latent demand, as in portfolio weights among stocks that are held by investor. Thus returns are mostly explained by demand shocks that are unrelated to changes in observed characteristics in the stocks themselves, which is a big deal (The Inelastic Demand for Startup Equity):\n\n\n\nVariance decomposition results show very clearly that individual investor sentiment (both its magnitude and dispersion) meaningfully influences returns:\n\nMean reversion in latent demand implies return predictability, since those stock with high latent demand will see their prices fall or rise more slowly as latent demand falls toward zero/one. Expected returns based on mean reversion over subsequent month appear to explain some portion of realized excess returns (and an even higher portion for small stocks):\n\n\n\n\nPaper address point Nassim Taleb makes in Fooled by Randomness in reference to Robert Shiller that stock prices move too much relative to plausible changes in fundamental value, new news, etc. Cross-sectional stock returns are mostly explained by demand shocks that are unrelated to changes in stock characteristics themselves:\n\nonline"},"@kolesarDynamicCausalEffects2025":{"title":"Dynamic Causal Effects in a Nonlinear World: The Good, the Bad, and the Ugly","links":["tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Dynamic Causal Effects in a Nonlinear World: The Good, the Bad, and the Ugly\nMichal Kolesár, Mikkel Plagborg-Møller\nThe good news: local projections and VARs estimate the right thing even in the presence of non-linearities in the DGP\nThe bad news: purely data-based identification via heteroskedasticity or non-Gaussianity don’t work in the presence of non-linearities\nThe good news has a caveat – you generally need either the actual shock itself or at least an instrument / proxy. Identifying the shock via controls only works if that residualized shock is non-linearly unpredictable by the controls, which likely isn’t the case in most applications. This seems more damning for identification via controls than they are emphasizing, though I’m sure they understand that:\n\n“we also show that when control variables are needed to isolate a true shock (i.e., recursive or Cholesky identification), then positive weights can only be guaranteed if the linearly residualized shock is nonlinearly unpredictable by the controls, which may be a strong assumption in the absence of detailed institutional knowledge and high-quality data.” (Kolesár and Plagborg-Møller, 2025, p. 3)\n\nI love this passage:\n\n“linearity-based estimators are useful even when economic theory predicts a nonlinear relationship between the shock and the outcome of interest. For example, if the outcome variable has limited support, such as when it is binary or censored (say, due to a zero lower bound), nonlinearities are inherently present. If one is interested in characterizing the nonlinearities, then it makes sense to model them, and it is of course always a good idea to plot the raw data regardless. However, if one is interested in an overall summary of marginal effects, then linear local projections and VARs are theoretically coherent estimators” (Kolesár and Plagborg-Møller, 2025, p. 3)\n\nI read this as basically saying “even with weird outcome variables, local projections and VARs work, and you don’t need to bother characterizing the non-linearities explicitly.”\nThe data-based estimators are basically useless because they presume linearity of the structural model. If you attempt to fix that issue via a nonparametric approach, you end up with confidence intervals too wide to draw any conclusions at all:\n\n“When there is a dearth of direct shock measures or proxies, applied researchers frequently resort to identification via heteroskedasticity (Sentana and Fiorentini, 2001; Rigobon, 2003; Lewbel, 2012). Unfortunately, we show that these estimation approaches are sensitive to the assumption that the structural model is linear: the estimand can easily be nonzero when there is no causal effect, or negative when the true shock has a uniformly positive effect on the outcome of interest. Fixing these issues while still delivering informative inference appears difficult, since a natural nonparametric generalization of the identification strategy yields very wide identified sets.” (Kolesár and Plagborg-Møller, 2025, p. 3)\n\n\n“the nonparametric analogue of the identification assumptions yields an identified set so large that effectively any function of the data can be construed as a “shock”. Intuitively, the mere assumptions that the latent shocks are independent and non-Gaussian are vacuous in a nonparametric context: any collection of random variables can always be represented as some nonlinear function of independent uniformly distributed random variables.” (Kolesár and Plagborg-Møller, 2025, p. 4)\n\n\nReferences\npaperreadonline"},"@liCombiningInductionTransduction2024":{"title":"Combining Induction and Transduction for Abstract Reasoning","links":["tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Combining Induction and Transduction for Abstract Reasoning\nWen-Ding Li, Keya Hu, Carter Larsen, Yuqing Wu, Simon Alford, Caleb Woo, Spencer M. Dunn, Hao Tang, Michelangelo Naim, Dat Nguyen, Wei-Long Zheng, Zenna Tavares, Yewen Pu, Kevin Ellis – 2024\nThis paper is great at a minimum because it defines induction and transduction in a clean way and delineates between the two.\n\nInduction – learning an intermediate function for mapping from input to output, then applying that function at test time. So the direct output of induction is the function itself, not the final output of applying that function. In some sense inductive learning involves conjuring up various candidate functions and then evaluating them on the training data. Importantly, once the function is learned the original training data can hypothetically be discarded as they are not actively used in future predictions. Learn, then apply – it’s like studying for an exam that you won’t be allowed to bring any cheat sheets for.\nTransduction – outputs the answer directly without explicit construction of an intermediate mapping function. In this sense it’s more “direct” in nature, as the learner uses everything it’s seen so far to generate the answer at test time (all the training data), not just some function it’s learned. Transduction is naturally less explainable because there’s no intermediate step. It’s like showing up to an exam with all your prep material and figuring out in real-time how to answer the questions based on those materials.\n\n\n\nReferences\npaperreadonline"},"@liLocalProjectionsVs2023":{"title":"Local Projections vs. VARs: Lessons From Thousands of DGPs","links":["local-projections","@herbstBiasLocalProjections","Vector-autoregression","@barigozziLargedimensionalDynamicFactor2021","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Local Projections vs. VARs: Lessons From Thousands of DGPs\nDake Li, Mikkel Plagborg-Møller, Christian K. Wolf – 2023\nNever use local projections in small samples unless you have overwhelming concern for bias, in which case you should really use bias-corrected LP a la @herbstBiasLocalProjections. In such scenarios, use a Bayesian VAR or a least a standard least squares VAR.\n\nReferences\n@barigozziLargedimensionalDynamicFactor2021\npaperreadonline"},"@lopez-liraCommonFactorsReally2020":{"title":"Do Common Factors Really Explain the Cross-Section of Stock Returns?","links":["Where-Is-the-Alpha-in-AI","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Do Common Factors Really Explain the Cross-Section of Stock Returns?\nAlejandro Lopez-Lira, Nikolai L. Roussanov – 2020\nwww.youtube.com/watch\n\n\n                  \n                  Quote\n                  \n                \n\nThe central insight of asset pricing theory is that only systematic risk should be rewarded with an average return in excess of the risk-free rate. In particular,  the arbitrage pricing theory (APT) of Ross (1976) posits that certain securities earn  higher expected returns than others only because they are more exposed to common  (i.e., undiversifiable) risk factors. Conversely, the expected excess returns of portfolios hedged against all systematic risk should be zero…\n\n\nExcess returns can only come from systematic risk, that is to say, risk factors that are common across stocks and therefore can’t be diversified away. These systematic risks drive time series variation in returns. Intuitively, you get paid for taking risks that were forced upon you by the market, any any risk that you can somehow get around via diversification shouldn’t pay you anything. If you somehow manage to hedge against all the systematic risks, your return should be zero. This is in some sense a “no free lunch” theorem – there is no (excess) return without systematic risk. “Alpha” is in some sense the violation of this principle, represented as a numeric value.\nRelevant to Where Is the Alpha in AI?\n\nReferences\npaperreadonline"},"@lubanaPriorsTimeMissing2025":{"title":"Priors in Time: Missing Inductive Biases for Language Model Interpretability","links":["@adamekLassoInferenceHighDimensional2022","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Priors in Time: Missing Inductive Biases for Language Model Interpretability\nEkdeep Singh Lubana, Can Rager, Sai Sumedh R. Hindupur, Valerie Costa, Greta Tuckute, Oam Patel, Sonia Krishna Murthy, Thomas Fel, Daniel Wurgaft, Eric J. Bigelow, Johnny Lin, Demba Ba, Martin Wattenberg, Fernanda Viegas, Melanie Weber, Aaron Mueller – 2025\nSeems like a super important acknowledgement that SAEs until this point have assumed iid data, with no temporal dependency or non-stationarity. A handful of plots across representative language datasets disproves this notion entirely. So a core prior embedded within the SAE approach is essentially false. Now, all priors are false to some extent, so that alone is not necessarily a critical error. SAEs themselves embed a prior of sparsity, which is probably correct, but we can’t know the exact right level of that sparsity, so that prior will be “wrong” in some sense. However, that kind of error seems much more benign than falsely assuming the data is iid when it is not.\nI think some concepts need to be disentangled here (no pun intended):\n\nWhether the data is iid\nWhether the data is non-stationary\n\nThe data could be non-iid, but conditionality iid once you control for past information. Or, the data could have temporal dependency and dynamics but still be stationary (standard AR(1) with persistence &lt; 1, for example).\nI think this paper is saying both that the data is not iid and that the data is non-stationary. But again, those are two separate and independently important points, so one should be careful not to conflate them.\nIn the end, the authors settle on a methodology they call Temporal Feature Analysis.\n\nIn computational neuroscience, when analyzing data from dynamical  domains (e.g., audio, language, or video), a commonly made assumption is that there is contextual  information present in the recent history that informs the next state—this part of the signal is deemed  predictable, slow-changing,  invariant, or dense. Meanwhile, the remaining  signal corresponds to new bits of information added by the observed state at the next timestep—this  part can be deemed novel, fast-changing, variant, or sparse with respect to the context. We argue LM  activations are amenable to a similar generative model.\n\nI love the additional words they use to describe something that I’m mostly familiar with from my reading of the time series econometrics literature. There’s some risk of mixing up concepts that mean different things in different contexts, but in general I think this added richness in vocabulary is helpful.\nVarious ways of thinking about this in econometrics:\n\nDeterministic component vs random component\n(Slow-moving) State variables\nPermanent vs transitory component\n\nAgain, I’m sensitive to the fact that these aren’t all actually the same thing. But they scratch at similar ideas and help provide more of a 360 view on a certain cluster of ideas — within a single step of a time series, there exists a component that is more “durable”, “slow to change”, “predictable”, “temporally dependent”, “endogenous”, “persistent”, potentially “non-stationary”, etc. And there’s another component that is “fast moving”, “exogenous”, “transitory”, “stationary”, “iid”, “white noise”-like, “random”, “unpredictable”, “noisy”, etc.\nSidebar: the points made in this paper help bring home the idea that I’ve encountered elsewhere that applying sparsity methods to time series data is tricky and requires care. See @adamekLassoInferenceHighDimensional2022 (“lasso cannot be applied off the shelf to time series data, as the data are not IID.”)\n\nReferences\npaperreadonline"},"@lustigGovernmentDebtMature":{"title":"Government Debt in Mature Economies. Safe or Risky?","links":["tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Government Debt in Mature Economies. Safe or Risky?\n**Hanno Lustig, Howard Kung, Roberto Gomez Cram – **\nGovernment have a choice to either protect / insure (1) taxpayers or (2) bondholders against government spending shocks (which naturally reduces the value of Treasury securities):\n\nTaxpayer protection: Yields increase in response to unfunded fiscal expansion, value of debt declines, hurting bondholders. The value of debt declines because investors rationally mark them down in response to reduced or risky government cash flows (taxes minus spending, i.e. surpluses). Inflation increases. Large scale asset purchases by the central bank come from desire support the value of the bonds. Government debt has some associated beta, as yield co-move with discount rate innovations, generating a positive correlation between stock and bonds (“risky debt” regime, fiscal dominance)\nBondholder protection: Government promises to pay for increasing spending with additional taxation. Yields do not move. Inflation expectations do not move. To the degree the central bank acts, it’s to provide liquidity to ensure well-functioning markets. Zero beta (“safe debt” regime, monetary dominance)\n\n\nThe value of government debt is marked down via three channels:\n\nIncrease in long-term expected inflation, particularly in response to an increase in long-term debt\nDecline in convenience yields due to the increased supply of Treasuries compressing the “narrow” convenience yield\nIncreased risk-free real rates due to increased supply of safe assets\n\nEmpirical results: the risky debt / fiscal dominance paradigm seems to be a better fit for recent U.S. data and other advanced economies. Spending shocks induce a devaluation of the government bond portfolio.\n\nYields increase substantially in the COVID era, suggesting markets did not believe the fiscal expansion to be supported by future taxation. Bondholders experienced substantially negative real returns due to devaluation of the U.S. government debt portfolio.\nThe stock-bond correlation turned positive during the COVID era after being largely positive for the last two decades.\n\n\nReferences\nwww.gsb.stanford.edu/insights/united-states-borrowing-binge-about-burst\npaperreadonline"},"@nowakMathematicalFoundationsMachine":{"title":"Mathematical Foundations of Machine Learning","links":["tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Mathematical Foundations of Machine Learning\nRobert Nowak\nConditional probability\nDefined as the solution to the equation p(x, y) = p(y|x)p(x) or, rearranged, p(y|x) = p(x,y)/p(x)\nIn this way, we can think of conditional probabilities as a mathematical object rather than a more intuitive “words” explanation.\nMarginal probability\nTo marginalize is to sum over all possible values of a variable. Thus, if the joint density in question is p(x,y), the marginal probability of a random variable X is p(x) = \\sum_yp(x,y) = \\int_yp(x,y)dx\nMarginalization is often useful when you need to remove a random variable from consideration. By marginalizing above, y drops out of the equation.\nExpected value\nThe expected value of a function of a random variable is E[f(X)] = \\sum_xf(x)p(x) = \\int f(x)p(x)dx\nNote that this works for any function – you just evaluate the function at each potential realization and multiply that by the probability of that realization, then sum it all up. You can also use this for things like conditional expectations, where the “function” is one of the variables and the probability is the conditional probability: E[Y|X] = \\sum_yyp(y|x) = \\int yp(y|x)dx\n\nReferences\npaperreadonline"},"@parkBayesianLasso2008":{"title":"The Bayesian Lasso","links":["tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"The Bayesian Lasso\nTrevor Park, George Casella – 2008\n\n\n                  \n                  Abstract \n                  \n                \n\nThe Lasso estimate for linear regression parameters can be interpreted as a Bayesian posterior mode estimate when the regression parameters have independent Laplace (i.e., double-exponential) priors. Gibbs sampling from this posterior is possible using an expanded hierarchy with conjugate normal priors for the regression parameters and independent exponential priors on their variances. A connection with the inverse Gaussian distribution provides tractable full conditional distributions. The Bayesian Lasso provides interval estimates (Bayesian credible intervals) that can guide variable selection. Moreover, the structure of the hierarchical model provides both Bayesian and likelihood methods for selecting the Lasso parameter. Slight modifications lead to Bayesian versions of other Lasso-related estimation methods, including bridge regression and a robust variant.\n\n\n\n\n                  \n                  Summary \n                  \n                \n\nTK\n\n\nThe nice part about a fully Bayesian approach to Lasso is that you don’t have to worry about doing cross-validation. You just set your priors and you’re ready to go. Even the \\lambda parameter in the Laplace coefficient prior can be set via maximum likelihood.\nHierarchical priors\n\nPrior on coefficients,  \\beta\nPrior on variance of coefficients, \\sigma^2\nPrior on the Laplace parameter, \\lambda\n\nPriors\nPrior on coefficients, \\beta (conditional Laplace distribution):\np(\\beta|\\sigma^2)= \\prod_{j=1}^p \\frac{\\lambda}{2\\sqrt{\\sigma^2}}e^{-\\lambda|\\beta_j|/\\sqrt{\\sigma^2}}\n\\lambda here is equivalent to \\sqrt{\\sigma^2}/b, where b is the scale parameter of the Laplace distribution. So in a way \\lambda is the inverse variance / precision.\nPrior on variance of coefficients, \\sigma^2 (non-informative scale-invariant):\np(\\sigma^2) = 1/{\\sigma^2}\nNote that it’s presumed the data has been standardized, so the above is technically scale-invariant. Also note that this kind of prior is very similar to an exponential, one-sided Laplace, or Gamma distribution with shape parameter = 1.\nPrior on the Laplace parameter, \\lambda (note \\lambda^2, not \\lambda):\np(\\lambda^2) = \\frac{\\delta^2}{\\Gamma(r)}(\\lambda^2)^{r-1}e^{-\\delta\\lambda^2}\nwith r = 1 for most use cases and \\delta &gt; 1. This is a Gamma distribution. If r=1 it actually becomes an exponential distribution.\nEstimation\nThe authors propose a Gibbs sampling approach to estimating the Bayesian Lasso, which exploits the fact that the Laplace distribution can be represented as a mixture of normal distributions.\n\nReferences\npaperreadonline"},"@pengPreliminaryReportDistro2024":{"title":"A Preliminary Report on Distro","links":["tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"A Preliminary Report on Distro\nBowen Peng, Jeffrey Quesnelle, Dillon Rolnick, Ari Lotter, Umer H. Adil, Esteban La Rocca – 2024\nStatus quo\n\nDistributed data parallelism (DDP) and fully shared data parallelism (FSDP) are used to split model training among GPUs\nTypically only a single model is trained, and all of that model’s weights get shared across GPUs, synchronized after each training step\nSynchronizing weights means sending across the entire set of weights in a relatively I/O intensive operation between GPUs, requiring high bandwidth interconnect and the GPU to be placed physically close to one another\n\nContribution\n\nDemonstrate 4-5x OOM reduction in GPU communication, enabling training of LLMs in low bandwidth contexts\nNew optimizer that matches AdamW in convergence performance\n\nImplications\n\nCheaper GPU clusters\nChanges the relative trade-off between higher VRAM GPUs and interconnect, which maps to compute-heavy workloads vs I/O-heavy operations (interconnect much more expensive than VRAM)\nDecouples interconnect bandwidth from model size, enabling scaling without increasing interconnect bandwidth\nEnables the creation of distributed and decentralized training networks across heterogeneous networking\n\n\nReferences\npaperreadonline"},"@phillipsBoostingWhyYou2021":{"title":"Boosting: Why You Can Use the Hp Filter","links":["boosting","Hodrick-Prescott-Filter","bias","stationarity","@hamiltonWhyYouShould2017","Why-You-Should-Never-Use-the-Hodrick-Prescott-Filter","tags/literature/paper","tags/inbox/process","tags/online"],"tags":["literature/paper","inbox/read","inbox/process","online"],"content":"Boosting: Why You Can Use the Hp Filter\nPeter C. B. Phillips, Zhentao Shi\nAbstract\n\nWe propose a procedure of iterating the HP filter to produce a smarter smoothing device, called the boosted HP (bHP) filter, based on L2-boosting in machine learning. Limit theory shows that the bHP filter asymptotically recovers trend mechanisms that involve integrated processes, deterministic drifts, and structural breaks, covering the most common trends that appear in current modeling methodology. A stopping criterion automates the algorithm, giving a data-determined method for data-rich environments. The methodology is illustrated in simulations and with three real data examples that highlight the differences between simple HP filtering, the bHP filter, and an alternative autoregressive approach.\n\nIt’s important to use a lower penalty parameter for smaller timeseries, otherwise the HP filter will tend to over-penalize (over-smooth) the data.\nIn some ways the choice of lambda for the HP filter has less to do with the frequency of the data and more to do with the amount of data you have. The real reason lambda tends to align with data frequency is that similar frequency data tends to be of similar length. If you have similar frequency but significantly different length, it can make sense to use a different lambda penalization.\nProbably makes sense to bias towards setting \\lambda too high to begin with, rather than too low. Setting \\lambda too low causes the HP filter to effectively over-fit the data, meaning that the residuals/cycle is too small/low variance, which can’t really be undone by further filtering/boosting. On the other hand, if \\lambda is too high initially, the filter under-fits, meaning the trend will be biased and the errors/cycle will be too “large,” which can always be fixed via reapplication of the filter, progressively shaving off more non-stationary variation.\nRepo for bHP filter\n\nReferences\nWhy You Should Never Use the Hodrick-Prescott Filter\nWhy You Should Never Use the Hodrick-Prescott Filter\npaperprocessonline"},"@plagborg-mollerEssaysMacroeconometrics":{"title":"Essays in Macroeconometrics","links":["Impulse-responses","Local-projections","@greenwaldOriginsStockMarket2014","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Essays in Macroeconometrics\nMikkel Plagborg-Møller\nLink\n\n\n                  \n                  Abstract \n                  \n                \n\nThis dissertation consists of three independent chapters on econometric methods for macroeconomic analysis. In the first chapter, I propose to estimate structural impulse response functions from macroeconomic time series by doing Bayesian inference on the Structural Vector Moving Average representation of the data. This approach has two advantages over Structural Vector Autoregression analysis: It imposes prior information directly on the impulse responses in a flexible and transparent manner, and it can handle noninvertible impulse response functions. The second chapter, which is coauthored with B. J. Bates, J. H. Stock, and M. W. Watson, considers the estimation of dynamic factor models when there is temporal instability in the factor loadings. We show that the principal components estimator is robust to empirically large amounts of instability. The robustness carries over to regressions based on estimated factors, but not to estimation of the number of factors.In the third chapter, I develop shrinkage methods for smoothing an estimated impulseresponse function. I propose a data-dependent criterion for selecting the degree of smoothing to optimally trade off bias and variance, and I devise novel shrinkage confidence sets with valid frequentist coverage.\n\n\nSmoothness priors sharpen inference (i.e. reduce standard errors) for impulse response functions, since smoother IRFs have fewer effective free parameters. In other words, if you shrink toward a polynomial, that polynomial has fewer parameters than some other complex function you could come up with. It has to look a certain way (like a polynomial), irrespective of the data, which reduces uncertainty. There’s only so many ways to skin a cat, and likewise there’s only so many ways to draw a polynomial of a particular finite order that loosely fits the data. The coefficients of the polynomials effectively define the IRF, so reduced parameter uncertainty is equivalent to reduced IRF uncertainty.\nExplicit confirmation that if you have the correct shocks, you can just regress the outcome variable on the shocks directly a la local projections. This is what they do in Origins of Stock Market Fluctuations.\nSeems that an inherent drawback of the SVMA approach is that the parameters (which are the impulse response function) are under-identified, so it’s not just that the method makes priors easy and transparent to impose, they must be imposed. You could say that this isn’t much of a disadvantage, since in the SVAR setting you often impose the equivalent of priors as well, but in a more obtuse and less direct fashion.\n\nReferences\npaperreadonline"},"@snellScalingLLMTestTime2024":{"title":"Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters","links":["tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters\nCharlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar – 2024\n\nScaling test-time compute can be more effective than scaling model parameters (which increases both train and test time compute)\nMechanisms explored\n\nSearch + reward model\nAdaptive logits\n\n\nEffectiveness of test-time compute depends on task difficulty\n\nImplies test-time compute should be task dependent\n\n\nWith a good enough base model, a small model augmented with test-time compute can outperform a larger one\n\n\nReferences\npaperreadonline"},"@stockDynamicFactorModels2016":{"title":"Dynamic Factor Models, Factor-Augmented Vector Autoregressions, and Structural Vector Autoregressions in Macroeconomics","links":["Dynamic-factor-model","Vector-autoregression","identification","OLS","omitted-variable-bias","Impulse-responses","Forecast-error-variance-decomposition","Moving-average-model","@baiPrincipalComponentsEstimation2013","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Dynamic Factor Models, Factor-Augmented Vector Autoregressions, and Structural Vector Autoregressions in Macroeconomics\nJ.H. Stock, M.W. Watson\nLink\nAbstract\n\nThis chapter provides an overview of and user’s guide to dynamic factor models (DFMs), their estimation, and their uses in empirical macroeconomics. It also surveys recent developments in methods for identifying and estimating SVARs, an area that has seen important developments over the past 15 years. The chapter begins by introducing DFMs and the associated statistical tools, both parametric (state-space forms) and nonparametric (principal components and related methods). After reviewing two mature applications of DFMs, forecasting and macroeconomic monitoring, the chapter lays out the use of DFMs for analysis of structural shocks, a special case of which is factor-augmented vector autoregressions (FAVARs). A main focus of the chapter is how to extend methods for identifying shocks in structural vector autoregression (SVAR) to structural DFMs. The chapter provides a unification of SVARs, FAVARs, and structural DFMs and shows both in theory and through an empirical application to oil shocks how the same identification strategies can be applied to each type of model.\n\nDynamic Factor Models\nThis article is basically the whole textbook on DFMs. It’s super helpful, though I don’t think for practical purposes one should code up the algorithms on your own.\nAt some point I might take some notes here but for now I think I can simply leverage this stuff out of the box (e.g. using the R package dfms) without worrying too much about the core details.\nStructural Vector Autoregressions\nIf shocks were observed, you could simply run OLS of the outcome variable on the current and past shock values, and you’d be good to go. If the shock is a true shock, it’s uncorrelated with other variables, so there’s no omitted variable bias. The coefficients will be unbiased and represent the impulse response of the outcome to the shock:\n\nIf a time series of shocks were observed, it would be straightforward to estimate the effect of that shock, say \\varepsilon_{1t}, on a macro variable y_t by regressing y_t on current and past values of \\varepsilon_{1t}. Because the shock \\varepsilon_{1t} is uncorrelated with the other shocks to the economy, that regression would have no omitted variable bias. The population coefficients of that regression would be the dynamic causal effect of that shock on the dependent variable, also called the structural impulse response function (SIRF). The cumulative sum of those population coefficients would be the cumulative causal effect of that shock over time, called the cumulative SIRF.\n\nSince the shocks are typically not in fact observed, the whole point of SVARs is to leverage the assumption that the forecast errors / innovations of the VAR fully represent the space of structural shocks to uncover the impact of those structural shocks on relevant variables of interest. Economists use the term “span the shocks,” but all this means in practice is that the VAR innovations are linear combinations of all the structural shocks in the economy and vice versa. Said this way, this seems like a fairly strong assumption, but accepting it at least in principle lets us do some very interesting things.\n\nThe premise of SVARs is that the space of the innovations to a vector of time series variables Y_t—that is, the one step ahead forecast errors of Y_t based on a population projection of Y_t onto its past values—spans the space of the structural shocks. Said differently, in population the econometrician is assumed to be as good at one step ahead forecasting of the economy as an agent who directly observes the structural shocks in real time. The task of identifying the structural shock of interest thus reduces to the task of finding the linear combination of the innovations that is the structural shock.\n\nHowever, if one is only interested in the effect of a subset of shock, only those shocks need to be spanned by the VAR innovations. For example, if I’m only interested in the effect of a single shock, then only that shock needs to be a linear combination of the VAR innovations in the system. This is a helpful distinction in theory, though I wonder in practice to what extent results would ever depend on this being true vs the broader assumption of full spanning needing to be true.\nIn practical terms, this tends to mean that you need at least as many variables as shocks you want to identify. Adding more variables than that helps to ensure the structural shocks of interest are in fact fully spanned, but the math works as soon as you have enough variables / reduced form equations in the VAR.\nForecast error variance decomposition (FEVD): the breakdown of how important each shock is in explaining the variation in some outcome variable. FEVDs are calculated as the relative size of the variance of a shock to the overall variance in the forecast errors in the outcome variable over future periods (the h-step ahead forecast errors). This it is calculated separately for each horizon (h), shock (j), and outcome variable triplet (i). In a way, you can think about the FEVD as simply using the idea of a system of equations to say that the innovations of one variable can be explained by the structural shocks within the system, explicitly calculating the relative contributions.\nHistorical decomposition (HD): These are literally just the moving average representation of the time series. Thus once you have the impulse response function you effectively also have the historical decomposition. Another way to think about this is that you can project the demeaned observed variable on current and past shocks and that result is the contribution of the shocks to that variable.\n\nReferences\n@baiPrincipalComponentsEstimation2013\npaperreadonline"},"@stockIdentificationEstimationDynamic2018":{"title":"Identification and Estimation of Dynamic Causal Effects in Macroeconomics Using External Instruments","links":["local-projections","vector-autoregression","Local-projections","instrumental-variables","Vector-autoregression","omitted-variable-bias","@nakamuraIdentificationMacroeconomics2018","shock","Dynamic-factor-model","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Identification and Estimation of Dynamic Causal Effects in Macroeconomics Using External Instruments\nJames H. Stock, Mark W. Watson\nLink\nVideo\nAbstract\n\nExternal sources of as-if randomness — that is, external instruments — can be used to identify the dynamic causal effects of macroeconomic shocks. One method is a one-step instrumental variables regression (local projections – IV); a more efficient two-step method involves a vector autoregression. We show that, under a restrictive instrument validity condition, the one-step method is valid even if the vector autoregression is not invertible, so comparing the two estimates provides a test of invertibility. If, however, lagged endogenous variables are needed as control variables in the one-step method, then the conditions for validity of the two methods are the same.\n\nMakes the point that there can be more shocks than economics variables in your system of equations:\n\nIf we collect all such structural shocks and measurement error together in the m \\times 1 vector \\varepsilon_t, the n \\times 1 vector of macroeconomic variables Y_t can be written in terms of current and past \\varepsilon_t\n…\nIn general, the number of shocks plus measurement errors, m, can exceed the number of observed variables, n.\n\nThe unit effect normalization – fixing the shock variable such that a one unit movement corresponds to a unit movement in the impulse variable of interest. This solves the scale ambiguity issues the arises from the fact that the true shock is unobserved. It also underpins the local projections approach,  as it enables regressions in terms of observables.\nProves that local projections with instrumental variables can be used in situations when the equivalent VAR would be invertible:\n\nwe provide conditions for instrument validity for LP-IV, and show that under those conditions LP-IV can estimate dynamic causal effects without assuming invertibility, that is, without assuming that the structural shocks can be recovered from current and lagged values of the observed data.\n…\nThe structural moving average \\Theta(\\text{L}) in (5) is said to be invertible if et can be linearly determined from current and lagged values of Y_t:\n…\nIn the linear models of this article, condition (24) is equivalent to saying that \\Theta(\\text{L})^{-1}\u0001exists.\n\n\nInvertibility implies that the VAR is fully specced out. In other words, the system contains all the data you need to uncover the true shocks and you don’t need to augment it with anything, nor would you benefit from doing so:\n\nunder invertibility, a forecaster using a VAR would find no value in augmenting her system with data on the true macroeconomic shocks, were they magically to become available.\n…\na forecaster using a VAR who magically stumbled upon the history of true shocks would have no interest in adding those shocks to her forecasting equations.\n…\nIf invertibility holds, then knowledge of the past true shocks would not improve the VAR forecast. If instead those forecasts were improved by adding the shocks to the regression – infeasible, of course, but a thought experiment – then the VAR has omitted some variables, and that omission is an indication of the failure of the invertibility assumption.\n\nCovers the omitted variable bias interpretation of invertibility as discussed in @nakamuraIdentificationMacroeconomics2018:\n\nOne interpretation provided in the literature on invertibility is that invertibility implies that there are no omitted variables in the VAR (e.g. Fernandez-Villaverde et al., 2007): because invertibility implies that the spans of \\varepsilon_T and \\nu_t are the same, there is no forecasting gain from adding past shocks to the VAR.\n\nInvertibility as meaning structural shocks can be expressed as linear combination of current and past values of the observed data:\n\nThe structural MA representation Y_t = D(\\mathrm{L}) \\varepsilon_t represents Y_t in terms of current and past values of the structural shocks \\varepsilon_t. The moving average is said to be invertible if \\varepsilon_t can be expressed as a distributed lag of current and past values of the observed data Y_t. SVARs typically assume \\varepsilon_t = H^{-1} \\eta_t = H^{-1}A(\\mathrm{L})Y_t, so an SVAR typically imposes invertibility.\n\nFinally hit me that the “moving average” representation simply means representing current variables as a linear combination of current and past innovations / shocks, while the “VAR” representation means representing residuals / shocks as a linear combination of current and past observed data.\nYou can get the moving average representation of a VAR by regressing the outcome variables on its lags, then regress the outcome variables on the residuals from the first regression. This yield the MA coefficients. It is also therefore an easy way to invert a VAR without literally inverting the coefficient matrix.\nSolutions to omitted variable bias in the context of a VAR:\n\nInclude large number of variables via DFM, FAVAR, BVAR etc\ninstrumental variables estimation\n\n\n\nReferences\npaperreadonline"},"@taylorSystematicShiftsScaling2021":{"title":"Systematic shifts in scaling behavior based on organizational strategy in universities","links":["tags/online"],"tags":["literature","inbox/read","online"],"content":"Systematic shifts in scaling behavior based on organizational strategy in universities\nRyan C. Taylor, Xiaofan Liang, Manfred D. Laubichler, Geoffrey B. West, Christopher P. Kempes, Marion Dumas\nAbstract\n\nTo build better theories of cities, companies, and other social institutions such as universities, requires that we understand the tradeoffs and complementarities that exist between their core functions, and that we understand bounds to their growth. Scaling theory has been a powerful tool for addressing such questions in diverse physical, biological and urban systems, revealing systematic quantitative regularities between size and function. Here we apply scaling theory to the social sciences, taking a synoptic view of an entire class of institutions. The United States higher education system serves as an ideal case study, since it includes over 5,800 institutions with shared broad objectives, but ranges in strategy from vocational training to the production of novel research, contains public, nonprofit and for-profit models, and spans sizes from 10 to roughly 100,000 enrolled students. We show that, like organisms, ecosystems and cities, universities and colleges scale in a surprisingly systematic fashion following simple power-law behavior. Comparing seven commonly accepted sectors of higher education organizations, we find distinct regimes of scaling between a school’s total enrollment and its expenditures, revenues, graduation rates and economic added value. Our results quantify how each sector leverages specific economies of scale to address distinct priorities. Taken together, the scaling of features within a sector along with the shifts in scaling across sectors implies that there are generic mechanisms and constraints shared by all sectors, which lead to tradeoffs between their different societal functions and roles. We highlight the strong complementarity between public and private research universities, and community and state colleges, that all display superlinear returns to scale. In contrast to the scaling of biological systems, our results highlight that much of the observed scaling behavior is modulated by the particular strategies of organizations rather than an immutable set of constraints.\n\nHighlights\n\nRevenue and expenses at universities and colleges follow power law behavior with respect to enrollment\n\n“We show that, like organisms, ecosystems and cities, universities and colleges scale in a surprisingly systematic fashion following simple power-law behavior.”\n\nMost socioeconomic metrics scale with population as a power law with exponent 1.15\n\n“from total wages and GDP to the number of social interactions and number of patents produced, scale with population size as a power law with an exponent of 1.15.”\n…\n“Consequently, on a per capita basis, socio-economic metrics increase proportionally to X0.15, implying that on a per capita basis, larger cities promote more social interactions and greater production of patents, and therefore more innovation”\n\nOn average, universities and colleges scale revenue and expenses linearly with student enrollment\n\n“financial throughput scales linearly with size, suggesting that, on average, there is no advantage to being larger at least as far as these economic indicators are concerned”\n\n\nResearch universities scale revenue and expenses superlinearly with respect to enrollment\n\n“research universities (both public and non-profit private) scale superlinearly: as they enroll more students, their revenues and expenditures increase faster than linearly”\n…\n“Research universities (public and private) scale superlinearly in all activities and sources of revenue, but sacrifice affordability. As they grow larger, they seek to attract increasingly prestigious faculty (as indicated by the superlinear scaling in faculty pay, especially in private universities) and charge higher tuition, also attracting better-resourced students, who later on enjoy higher earnings. The fact that both research and educational outcomes scale superlinearly suggest that these activities are synergistic.”\n\nCommunity colleges demonstrated sublinear scaling with respect to enrollment, leading to declining per capita spending as they scale:\n\n“community colleges and state colleges display remarkably sublinear scaling, that is, financial throughput per student decreases with size, representing strong overall economies of scale”\n\n\n“non-profit private colleges and professional schools scale roughly linearly with size, indicating little advantage in being larger”\n\nFor profit colleges demonstrate linear revenue scaling and sublinear expenditure scaling, implying rising profit margins as they scale:\n\n“for-profit colleges display linear scaling in revenue but sublinear scaling in expenditure, which implies that they are able to make aprofit by exploiting economies of scale in their costs.”\n\n\n“Maintenance and administrative costs scale slightly superlinearly, but do not systematically outpace production processes (teaching, research and student completions). This indicates that there are no apparent diseconomies of scale in maintenance function. On the contrary, efficiency in maintenance seems to support the diversification of activities”\n\n\n“average mid-career earnings increase with a school’s mean test scores and its out-of-state tuition costs”\n\nonline"},"@vivianoDynamicCovariateBalancing2023":{"title":"Dynamic covariate balancing: Estimating treatment effects over time with potential local projections","links":["local-projections","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Dynamic covariate balancing: Estimating treatment effects over time with potential local projections\nDavide Viviano, Jelena Bradic – 2023\nMain difference from standard local projections\n\nYou estimate the LPs on the conditional expectation of potential outcomes rather than the conditional expectation of actual outcomes\nYou estimate the LPs recursively, which requires first estimating the furthest out horizon and then using those calculations as part of your estimates for the shorter horizons\nUses a method called dynamic covariate balancing to reweight observations to ensure covariate balance between treated and control units.\n\nWhy have this distinction\n\nThe estimand of standard local projections doesn’t explicitly take a stance of the autocorrelation of the treatment variable. This makes interpretation of the LPs ambiguous, since interpreting what effect you are measuring requires a view on to what degree future treatments are influenced by past treatments and covariates. Note that this doesn’t mean that LPs are “wrong” in any way, merely that their interpretation is tricky and they may not be estimating the thing you want. The only way to interpret an LP as representation only the effect of a singular treatment with no future treatments is if you assume treatments are independent over time (no autocorrelation). This is almost certainly not true unless you are using a properly constructed white noise shock variable with no autocorrelation.\nThis is an important distinction particularly when you have an explicit series of treatments that you want to estimate the effect of. In such situations, you need to be clear about about assigning effects to various treatments. Not so in the standard LP case, where you only care about a single, initiating treatment and whatever happens from there is fair game / not explicitly modeled\nIn effect, this method lets you explicitly estimate the effect of a single treatment followed by no future treatments and the effect of two consecutive treatments. On the other hand, LPs estimate something in the middle – the effect of a single treatment, with no control over or insight into subsequent treatments.\n\nImportant assumptions\n\nThis method takes no stance on modeling selection into treatment, i.e. propensity scores. The issue with propensity scores is that you can, of course, be wrong in your estimation an in addition they can be unstable in certain situations. The nice part about this method is that, via covariate balancing, you can achieve an as-good-as solution relative to propensity scores without literally knowing true propensity model and with lower variance.\nBecause you model potential outcomes, the intertemporal dependence of treatments doesn’t matter, since by assumption potential outcomes are not affected by treatment. This lets you retain full control over the particular treatment effects you analyze\nCan be used on imbalanced panels\n\n\nReferences\npaperreadonline"},"@wang1000LayerNetworks2025":{"title":"1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities","links":["Representation-Learning","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities\nKevin Wang, Ishaan Javali, Michał Bortkiewicz, Tomasz Trzciński, Benjamin Eysenbach\n\nProblem: how to scale RL productively and efficiently given sparse rewards, which make it challenging to train large RL models\nIdea: reinforcement learning + self-supervised learning = self-supervised RL\nHow: deep contrastive RL with modern architectural improvements\n\nContrastive RL (CRL)\nIncreasing available data in scalable way via GPU-accelerated RL frameworks\n100x deeper networks (1024 layers) vs. traditional RL (2-5 layers)\nResidual connections\nLayer normalization\nSwish activation (don’t know what this is)\nExperimental: batch size and network width\n\n\nContext: unsupervised goal-conditioned locomotion and manipulation tasks, with no demonstrations or rewards, driving agent to explore the environment from scratch\nResults: depth-wise scaling of RL models pays off\n\nSignificant performance increase vs. baselines, emergent capabilities at certain network scales\nNetworks that can be productively scaled depth-wise\n\n\n\nDetails\nObjective: maximize expected reward r_g(s_t, a_t) (probability of reaching goal in next time step) over possible policies \\pi(a|s,g) where the goal g is simply a function of certain state g = f(s)\nContrastive reinforcement learning: actor-critic method that pushes the policy to maximize the critic, which is the L2-norm between the embeddings of the (state, action) tuple \\phi(s,a) and the goal \\psi(g), written as f_{\\phi,\\psi}(s,a,g). The embeddings are generated by two neural networks which are trained to maximize the difference between the critic evaluated on samples from the agent’s own trajectory and the critic evaluated on samples where the goal has been swapped to a different random goal from another trajectory. This is a form of Representation Learning but in the context of reinforcement learning.\nExperimental Results\nEmergent policies through depth: significant jumps in performance observed at particular critical network depths, varying by task / environment. The authors highlight the humanoid environments in particular, where “to the best of our knowledge, this is the first goal-conditioned approach to document such behaviors on the humanoid environment.” Residual connections appear to be critical to enabling this, as networks without this component do not see improved performance with depth.\n\n\nDepth outperforms width: scaling depth appears to work much better than scaling network width, with a doubling of depth from 4 to 8 outperforming the widest networks in all test environments. This is again most notable in the humanoid environments. Width does help as well, but not to the same degree as depth.\n\nScaling the critic helps more than scaling the actor: scaling the critic networks drive substantial performance gains while scaling the actor is only marginally helpful. Worth noting however that this does contrast prior work that found scaling the actor to be harmful rather than merely unhelpful, which isn’t the case here.\n\nScaling batch size is beneficial with sufficient deep networks: batch size has historically not been a useful lever for RL performance. However, experiments here show that once the network reaches a certain depth, larger batches sizes do seem to outperform smaller ones. This suggests that larger networks have the ability to learn from larger batches in a way that smaller networks do not, which perhaps explains why batch size generally been found to be unhelpful in prior research.\n\nDeep networks learn better representations: deep networks appear to learn better representations in their training process. The authors evaluate this in two ways – investigating the Q values computed at various points in an environment and by looking where the the agent tends to spend time in embedding space throughout a trajectory. The Q values for the larger networks are more sophisticated in ways that likely improve task performance, while the trajectories of the larger networks cluster around the goal in embedding space in a broad way relative to smaller networks which tend to form much tighter clusters.\n\n\n\nReferences\npaperreadonline"},"@zhangRecursiveLanguageModels":{"title":"Recursive Language Models","links":["tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Recursive Language Models\nAlex L. Zhang, Tim Kraska, Omar Khattab\nTheir implementation: github.com/alexzhang13/rlm\n\nReferences\npaperreadonline"},"@zouAdaptiveLassoIts2006":{"title":"The Adaptive Lasso and Its Oracle Properties","links":["lasso","LASSO","OLS","bias","tags/literature/paper","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"The Adaptive Lasso and Its Oracle Properties\nHui Zou\nAbstract\n\nThe lasso is a popular technique for simultaneous estimation and variable selection. Lasso variable selection has been shown to be consistent under certain conditions. In this work we derive a necessary condition for the lasso variable selection to be consistent. Consequently, there exist certain scenarios where the lasso is inconsistent for variable selection. We then propose a new version of the lasso, called the adaptive lasso, where adaptive weights are used for penalizing different coefficients in the \\ell_1 penalty. We show that the adaptive lasso enjoys the oracle properties; namely, it performs as well as if the true underlying model were given in advance. Similar to the lasso, the adaptive lasso is shown to be near-minimax optimal. Furthermore, the adaptive lasso can be solved by the same efficient algorithm for solving the lasso. We also discuss the extension of the adaptive lasso in generalized linear models and show that the oracle properties still hold under mild regularity conditions. As a byproduct of our theory, the nonnegative garotte is shown to be consistent for variable selection.\n\nAdaptive LASSO is just normal lasso with some additional weights on the regressor penalization based on the inverse coefficients coming from some other initial procedure, such as OLS. Gives LASSO the oracle property. Equivalently, you can divide the regressors by their respective coefficients in the first stage.\nStandard LASSO tends to use shrinkage parameter that is tuned for prediction, making it too small for variable selection purposes.\nThe coefficients from standard LASSO tend to be too small (i.e. biased) for variable that should in fact have large coefficients (over shrinkage), and small true parameters have a tendency to be zeroed out.\nHigh multicollinearity between predictors tends to lead to poor variable selection by LASSO.\nasgl is a good Python library for adaptive LASSO (GitHub)\nSome example R code from Stack Overflow:\n# get data\ny &lt;- train[, 11]\nx &lt;- train[, -11]\nx &lt;- as.matrix(x)\nn &lt;- nrow(x)\n \n# standardize data\nymean &lt;- mean(y)\ny &lt;- y-mean(y)  \nxmean &lt;- colMeans(x)\nxnorm &lt;- sqrt(n-1)*apply(x,2,sd)\nx &lt;- scale(x, center = xmean, scale = xnorm)\n \n# fit ols \nlm.fit &lt;- lm(y ~ x)\nbeta.init &lt;- coef(lm.fit)[-1] # exclude 0 intercept\n \n# calculate weights\nw  &lt;- abs(beta.init)  \nx2 &lt;- scale(x, center=FALSE, scale=1/w)  \n \n# fit adaptive lasso\nrequire(glmnet)\nlasso.fit &lt;- cv.glmnet(x2, y, family = &quot;gaussian&quot;, alpha = 1, standardize = FALSE, nfolds = 10)\nbeta &lt;- predict(lasso.fit, x2, type=&quot;coefficients&quot;, s=&quot;lambda.min&quot;)[-1]\n \n# calculate estimates\nbeta &lt;- beta * w / xnorm # back to original scale\nbeta &lt;- matrix(beta, nrow=1)\nxmean &lt;- matrix(xmean, nrow=10)\nb0 &lt;- apply(beta, 1, function(a) ymean - a %*% xmean) # intercept\ncoef &lt;- cbind(b0, beta)\n“We have shown that the lasso cannot be an oracle procedure.” (Zou, 2006, p. 3) (pdf)\n“We now define the adaptive lasso. Suppose that \\hat{\\beta} is a root-n–consistent estimator to \\beta^*; for example, we can use \\hat{\\beta} (ols). Pick a \\gamma &gt; 0, and define the weight vector \\hat{\\text{w}} = 1/|\\hat{\\beta}|^{\\gamma} .” (Zou, 2006, p. 3) (pdf)\n“We have shown that although the lasso variable selection can be inconsistent in some scenarios, the adaptive lasso enjoys the oracle properties by utilizing the adaptively weighted \\ell_1 penalty.” (Zou, 2006, p. 8) (pdf)\n\nReferences\npaperonline"},"Autoregressive-models":{"title":"Autoregressive models","links":["tags/e","tags/online"],"tags":["e","online"],"content":"Autoregressive models\nAutoregressive models seem like a workhorse for modeling various timeseries data without having to make strong theoretical claims about the structure of the underlying variables.\nThe AR(1) model in particular is ubiquitous across economic modeling. Economists model many diverse processes using AR(1) models, especially anything where one believes disturbances eventual dissipate but have some persistence over time.\n\nReferences\neonline"},"Ben-Franklin's-Autoencoder":{"title":"Ben Franklin's Autoencoder","links":["tags/e","tags/online"],"tags":["e","online"],"content":"Ben Franklin’s Autoencoder\nBen Franklin’s method of reading a piece of writing, taking notes on it, and then trying to reproduce the original piece from his notes is identical to an autoencoder architecture in deep learning.\n\nReferences\neonline"},"Beveridge-Nelson-Decomposition":{"title":"Beveridge-Nelson Decomposition","links":["Stochastic-trend","stationarity","@cochraneHowBigRandom1988","tags/e","tags/online"],"tags":["e","online"],"content":"Beveridge-Nelson Decomposition\nThe Beveridge-Nelson (BN) Decomposition breaks down a time series, y_t, into a stochastic trend and stationary cycle. The trend, or permanent component, represents the long-run forecast of y_t at time t, after all transitory shocks have died out. In fact, the BN decomposition is defined so that this is true.\nConsider the time series\ny_t=z_t+c_t\nwhere\nz_t=\\mu+z_{t-1} + (\\sum_{j=0}^\\infty a_j) \\epsilon_t\n-c_t=(\\sum_{j=1}^\\infty a_j) \\epsilon_t + (\\sum_{j=2}^\\infty a_j) \\epsilon_{t-1} + (\\sum_{j=3}^\\infty a_j) \\epsilon_{t-2} + ...\nHere z_t is the trend and c_t is the cycle.\nz_t essentially counts the full impact of of the current shock, even future effects that have not yet occurred. Since it incorporates its own lag, it also by definition includes the full impact of all past shocks as well. In a sense, the permanent component “books all the revenue upfront”.\nc_t cumulates all the temporary impacts from past shocks that have not yet transpired in the current level of the series. In other words, it only books the “future revenue”, dropping anything that’s been booked already.\nThe net of “all impacts past, present, and future” and “impacts in the future” is “impacts past and present”, i.e. the time series itself.\n\nReferences\n@cochraneHowBigRandom1988\neonline"},"Career-success-and-career-capital-are-cointegrated":{"title":"Career success and career capital are cointegrated","links":["shock","Autoregressive-models","Random-walk","@lettauConsumptionAggregateWealth","tags/e","tags/inbox/write","tags/online"],"tags":["e","inbox/write","online"],"content":"Career success and career capital are cointegrated\nCareer success and career capital can diverge periodically but will over the medium to long-run converge. In that sense they are cointegrated – they exist in long-run equilibrium with one another. My personal belief is that career success is the mean reverting part of the system. That is – career success way above or below earned career capital will be transitory. If you get a lucky stream of “success shocks”, you’ll eventually fall back down to your actual level of career capital. Same in the reverse.\nThat said, my sense is that these deviations are somewhat persistent. I don’t have a strong sense of the exact autoregressive coefficient nor the error correction coefficient, but I don’t think this stuff dissipates within a year. It probably takes at least one job turn, which is 2+ years for most white collar professionals.\nNote that in isolation both career success and career capital will appear to be highly persistent random walks, but as in @lettauConsumptionAggregateWealth the true driver the permanent changes can be found via cointegration analysis.\n\nReferences\newriteonline"},"Careers-are-a-random-walk":{"title":"Careers are a random walk","links":["Random-walk","Stochastic-trend","deterministic-trend","data-generating-process","Career-success-and-career-capital-are-cointegrated","tags/e","tags/inbox/write","tags/online"],"tags":["e","inbox/write","online"],"content":"Careers are a random walk\nIt’s a common statement that careers only make sense in retrospect. Another version of this is the Steve Jobs quote that “you can only connect the dots looking backward”:\n\nCareers only make sense in hindsight. – What would a map of your career look like?\n\nIt strikes me that this is simply another way of saying that careers are random walks. Random walks can only be rationalized looking backward, and by definition cannot be rationalized going forward because the trend itself is stochastic (Stochastic trend). There is no deterministic trend that can be used to extrapolate forward. You can go ahead and fit one but it won’t mean anything.\nIf we take this idea seriously, attempts to rationalize careers, even after the fact, are foolish in the same way that “technical analysis” of the stock market is a waste of time. You fool yourself that there are stories in the data that might be useful for predicting the future or guiding behavior. If careers are a random walk, there are no stories. Or, said differently, there is only one story, which is the one that happened, as we can never know the DGP with any certainty or precision.\nSee also: Career success and career capital are cointegrated\n\nReferences\newriteonline"},"Change-leads-to-insight-far-more-often-than-insight-leads-to-change":{"title":"Change leads to insight far more often than insight leads to change","links":["tags/e","tags/online"],"tags":["e","online"],"content":"Change leads to insight far more often than insight leads to change\n\nReferences\neonline"},"Correlation-between-prices-and-quantities-reveals-main-market-driver":{"title":"Correlation between prices and quantities reveals main market driver","links":["heuristic","Old-Blue-Ocean-Draft","Supply-expansion-sucks-for-suppliers-and-vice-versa","@HowMuchSupply","tags/e","tags/online"],"tags":["e","online"],"content":"Correlation between prices and quantities reveals main market driver\nIf prices and quantities are positively correlated, the market is most likely to be demand-driven, which is to say variation in demand over time is the biggest driver of price movements, inflation, etc. Shifts in demand move the market equilibrium along the supply curve, which is upwards sloping. Thus, price and demand tend to be positively correlated when demand moves around a lot.\nOn the other hand, a negative correlation between prices and quantities suggest supply forces are the primary market mover. These shifts in supply move the market along the demand curve, which is downward sloping. This makes prices and quantities move in opposite directions.\nThis heuristic is a nice way to distinguish between demand and supply driven markets:\n\nFor example, the venture capital market is clearly demand-drive, as prices and financing activity tend to move in the same direction\n\n\nReferences\nOld Blue Ocean Draft\nSupply expansion sucks for suppliers and vice versa\n@HowMuchSupply\neonline"},"Delta-method":{"title":"Delta method","links":["@cochraneAssetPricing2005","tags/e","tags/online"],"tags":["e","online"],"content":"Delta method\nThe asymptotic variance of f(x) is related to it’s first derivative and the variance of x by\n\\text{var}[f(x)] = f&#039;(x)^2 \\text{ var}(x)\n\nReferences\n@cochraneAssetPricing2005\neonline"},"Diffusion-models":{"title":"Diffusion models","links":["Autoregressive-models","@kolloviehPredictRefineSynthesize2023","@harveyFlexibleDiffusionModeling2022","tags/e","tags/online"],"tags":["e","online"],"content":"Diffusion models\nSetup\n\nq(y): true sampling distribution of observed data\np_\\theta(y): learned sampling distribution\n\\text{x}^{1:T}: noisy latent variables at each step t\n\nForward process\nIn the forward process, noise is continuously added to the data across multiple steps, converging toward white noise \\text{x}^T \\sim N(0, 1):\nq(\\text{x}^t|\\text{x}^{t-1}) := N(\\sqrt{1-\\beta_t}\\text{x}_{t-1}, \\beta_t\\text{I})\nThe fixed Gaussian forward process means we can directly calculate/sample from q(\\text{x}^t|y) without calculating all the intermediate steps (similar to Autoregressive models):\n\\text{x}^t = \\sqrt{\\bar\\alpha_t}y + \\sqrt{(1-\\bar\\alpha_t)}\\epsilon\nWhere \\alpha_t = 1-\\beta_t, \\bar\\alpha_t = \\prod_{i=1}^t \\alpha_i, and \\epsilon is white noise. Again, this is 100% analogous to the forecasting equation for an AR(1).\nIn practice, \\alpha_t is chosen to be close to 1, as this yields the best results.\nDenoising\nIn the denoising or reverse diffusion process, noise is progressively removed over multiple steps, modeling the inverse process:\np_\\theta(\\text{x}^{t-1}|\\text{x}^t) := N(\\mu_\\theta(\\text{x}^t,t), \\sigma_t\\text{I})\nWhere \\sigma_t = \\frac{1-\\bar\\alpha_{t-1}}{1-\\bar\\alpha_t}\\beta_t.\n\\mu_\\theta is parameterized using a denoising network, \\bf\\epsilon_\\theta (of which many exist, including the popular U-Net):\n\\mu_\\theta(\\text{x}^t, t) = \\frac{1}{\\sqrt{\\alpha_t}}(\\text{x}^t-\\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\epsilon_\\theta(\\text{x}^t, t))\nThis model is trained using the objective function:\nE_{y,\\epsilon,t}[||\\epsilon_\\theta(\\text{x}^t,t)-\\epsilon||^2]\nIntuition\nOne intuitive way of thinking about diffusion models that I came up with while watching Advancing Diffusion Models for Text Generation – think about diffusion modeling as picking up a handful of sand (sampled random noise) and throwing the grains of sand against a canvas (manifold of the data distribution you care about, e.g. images). If you throw the sand in a particularly skillful way, the patterns that end up on the canvas will look like recognizable images. The training process in diffusion models is essentially learning this motion of skillfully throwing a bunch of sand so as to end up with an interesting pattern on the canvas. This is obviously somewhat complicated and a bit of miracle that this even works / is possible!\n\nReferences\n@kolloviehPredictRefineSynthesize2023\n@harveyFlexibleDiffusionModeling2022\nwww.youtube.com/watch\neonline"},"Don't-detrend-random-walks-with-linear-trends":{"title":"Don't detrend random walks with linear trends","links":["Spectral-analysis","@cochraneHowBigRandom1988","Random-walk","@nelsonSpuriousPeriodicityInappropriately1981","@hodrickExplorationTrendCycleDecomposition2020","Beats-and-Misses-Are-Forever","The-Universal-Law-of-SaaS-Growth","tags/e","tags/online"],"tags":["e","online"],"content":"Don’t detrend random walks with linear trends\nLeads to spurious dynamics, as the linear trend isn’t the real “trend,” it’s merely what happened to take place. It could have been quite different and things wouldn’t look so linear if random shocks had played out differently.\nOne good way to determine the plausibility of trend stationarity around a linear trend is to examine the spectral density of the first difference of the time series around frequency zero. Per @cochraneHowBigRandom1988, trend stationary series will have a spectral density of zero at frequency zero. More realistically, trend stationary series will have low power spectrum at frequency zero, while series with significant random walk components will have a high power spectrum at frequency zero.\n\nReferences\nSpurious Periodicity in Inappropriately Detrended Time Series\nAn Exploration of Trend-Cycle Decomposition Methodologies in Simulated Data\nBeats and Misses Are Forever\nThe Universal Law of SaaS Growth\n@cochraneHowBigRandom1988\n\nany test for trend stationarity (\\sigma_{\\Delta z}^2 = 0 or S_{\\Delta y}(e^{-i0}) = 0)\n\neonline"},"Don't-rely-on-inspiration-to-write":{"title":"Don't rely on inspiration to write","links":["When-in-doubt,-write","Quantity-creates-quality","Searching-for-Outliers","Sasha-Chapin-–-Awaken-the-Writer-Within","Notes-Against-Note-Taking-Systems---by-Sasha-Chapin","tags/e","tags/online"],"tags":["e","online"],"content":"Don’t rely on inspiration to write\nDon’t wait for inspiration to strike before you put words on the page. You really don’t need inspiration, you only need to write. When in doubt, write.\nWorried that your writing isn’t any good. Well, Quantity creates quality, so don’t overoptimize over make the best possible piece each time you publish (per Searching for Outliers).\n\nReferences\nSasha Chapin – Awaken the Writer Within\nTransclude of Sasha-Chapin-–-Awaken-the-Writer-Within#4e63cd\nNotes Against Note-Taking Systems - by Sasha Chapin\nTransclude of Notes-Against-Note-Taking-Systems---by-Sasha-Chapin#4c76cf\neonline"},"Dynamic-factor-model":{"title":"Dynamic factor model","links":["Vector-autoregression","Principal-component-analysis","Autoregressive-models","@stockDynamicFactorModels2016","@kirkerWhatDrivesCore","tags/e","tags/online"],"tags":["e","online"],"content":"Dynamic factor model\nDFMs are VARs that operate over unobserved factors rather than observed variables. The unobserved factors are derived via PCA. Idiosyncratic deviations from the common component are modeled via autoregressive models.\n\nReferences\n@stockDynamicFactorModels2016\n@kirkerWhatDrivesCore\nATSA19 Lecture 8: Introduction to Dynamic Factor Analysis\nMetran\nLarge dynamic factor models, forecasting, and nowcasting\nDynamic Factor Analysis\neonline"},"Exogeneity":{"title":"Exogeneity","links":["omitted-variable-bias","Vector-autoregression","Exogeneity-2025-02-09-12.42.12.excalidraw","tags/e","tags/online"],"tags":["e","online"],"content":"Exogeneity\nThe easiest way to think about exogeneity is in terms of correlations with error terms. To be exogenous is to be uncorrelated with the error term. There are other definitions as well but I find them to be confusing in various scenarios. The correlation with the error term definition is the only one I’ve found to be universally applicable in a non-confusing way. If you can come up with a plausible story for how your regressor of interest is uncorrelated with the error term, you’re fine.\nAnd by error term – you really don’t need to think of anything more complicated than “other things that effect the outcome variable that aren’t included among my regressors”. Could be literally anything. Once you frame it this way it becomes clear how difficult of a standard this really is and how flawed most research must therefore be. Nothing correlates with your regressor of interest than correlates with the outcome variable? Damn – must be a very special variable (or just total noise)!\nExogeneity is often written as:\nE(x\\epsilon) = 0\nwhich I used to find super confusing. An easy way of thinking about this is to remember the formula for covariance:\n\\text{cov}(x, y) = E(xy) - E(x)E(y)\nNotice that the second term multiplies the two respective expected values. If either is zero, that term goes to zero, and the covariance is simply the expectation of the product of the two variables. In an OLS context, the residual has an expectation of zero by definition, so the formula for exogeneity is equivalent to the covariance between the regressor and the error term. So, exogeneity quite literally means that the covariance of the variable and the error term is zero.\nNote that the exogeneity condition can also be expressed on conditional terms – so it’s fine if the raw regressor is not exogenous, as long as it is after conditioning on controls.\nNow, some people will write exogeneity this way:\nE(\\epsilon|w) = 0\nWhere w is a set of controls. This is called the conditional mean assumption. What this is saying is that the expectation of the error term does not depend on the conditioning set. If it did, those variables would appear somewhere on the RHS. Instead, the expectation is a constant (zero in this case, as you’d estimate in a regression), which under linearity by definition means it doesn’t depend on these other variables.\nAnother thing to note about all these definitions – they deal with the actual (sometimes unobserved), true values of all the variables in question, not their estimated quantities, which is why you don’t see any “hats” above the variables. This is of course not verifiable in practice when the variables are unobserved, so exogeneity much of the time is an assumption you are making, and these are ways of being explicit about what that assumption entails. They are not necessarily testable assumptions.\nIt’s interesting to note that exogeneity is particular to the dependent variable in question. The same RHS variable can be exogenous or endogenous depending on the context, i.e. depending on the outcome variable. Thus exogeneity is “context dependent”. This is important because it means that if if you have a random variable that it is difficult to forecast or that seems to evolve independently of most other things, then by definition most RHS variables you might want to use will be exogenous.\nIt’s interesting to think about exogeneity in the time series context. On some level, not controlling for lags of a outcome variable that as temporal autocorrelation creates an omitted variable bias issue to the degree that those lags are correlated with the independent variables in your regression.\nOne way of thinking about exogeneity that I find quite intuitive is think about two variables that are causally influenced by various structural shocks. Consider one particular structural shock – if that single shock affects both variables, then those variables are endogenous to one another. Why? They both share underlying structural causes (other than themselves). Whenever two variables share underlying causes, they are endogenous. When you think about it this way, it becomes quite clear that most variables you’d think to analyze will be endogenous, because most things that you would ever think to compare share some underlying causes. It’s very similar to a SVAR, where you have a matrix mapping structural shocks to the observed variables in the system. If that matrix is such that at least one structural shock affects multiple variables in the system, then those variables are endogenous. The only exception to this is when one of the variables in question is itself a structural / exogenous shock, in which case it’s fine that it affects multiple variables (itself plus others). It just needs to be the case that no other structural shock affect it.\n\nAnother way to think about exogeneity is in terms of systems. Things that are exogenous are those that come from outside the system and are not affected by the system itself. Otherwise, that object is endogenous – it comes from the system itself, is affected by the system, it is determined within the system, etc.\nWe can think about any collection of objects as being clustered into two categories – one broad system of related variables where there are direct or indirect two-way cause and effect relationships, and another cluster which doesn’t have these characteristics either between themselves or to the variables in the first cluster. Any relationships between an objects in the first cluster (“the system”) and the second cluster are unidirectional – things outside the system affect the things inside the system, but not the other way around. And once again, things outside the system also do not affect each other.\nTransclude of Exogeneity-2025-02-09-12.42.12.excalidraw\nWhen we write down a regression equation, we are essentially defining and separating out the system from the things outside the system (assuming we want accurate coefficient estimates):\nY = X\\beta + \\epsilon\nThe X\\beta portion represents the system, the \\epsilon portion represents the things outside the system. If we set things up this way, with everything endogenous represented in X and everything exogenous represented in \\epsilon , then we get good estimates of \\beta via OLS (which is just a simple manipulation of the regression equation, solving for \\beta). When we fail to do this, we get poor estimates.\nNote that this is relevant when the X_i in question is itself endogenous. If we want to estimate the effect of an exogenous variable we can do so straightforwardly because by definition that variable is exogenous to everything else (both things within the system and things outside it). We don’t actually need to control for anything in the simplest case, though there can be advantages to precision by doing so.\nThis also relates the notion of deterministic vs stochastic. In some sense the system itself is deterministic – with precise enough calculations, its evolution over time could be predicted perfectly, in the same way that the movement of a collection of gravitational objects could hypothetically be perfectly predicted based on the laws of physics. There exists, from the the perspective of the system, random sources of noise, variation, inputs to the system that perturb it and make its eventual dynamics not totally predictable ex ante. That these things can be separated is to some extent an assumption, most obviously laid out and explicit in the linear context (the X\\beta term, which yields E(Y|X), the “deterministic” portion of Y).\n\nReferences\neonline"},"Exponentials-drive-asymmetry":{"title":"Exponentials drive asymmetry","links":["Nassim-Taleb","tags/e","tags/inbox/write","tags/online"],"tags":["e","inbox/write","online"],"content":"Exponentials drive asymmetry\nPer Nassim Taleb, we should make asymmetric bets with long right tails. Even though he is very focused on the idea of (non)ergodicity, these discussions usually implicitly assume a stationary distribution, which is to say they lack a time component. It is assumed that the distribution of outcomes doesn’t change over time – it’s the same bet every single time. But in the real world the distribution of outcomes may shift over time.\nIt strikes me that one way of achieving asymmetry that may not be present in the current moment is to find intertemporal asymmetries. If you can find a bet where the expected value and the right tail of the distribution are growing fast, continually betting on that thing may yield a form of asymmetry through time. This could justify large expenditures in the present moment that might seem foolish to others but that could have high payoffs in a future, much more positively-skewed distribution. The best sort of thing to bet on along these lines is something growing exponentially in compounding fashion.\n\nReferences\newriteonline"},"Fat-tails-preclude-ergodicity":{"title":"Fat tails preclude ergodicity","links":["Sample-means-change-with-sample-size-in-fat-tailed-distributions","Interaction-generates-non-normality","Fat-Tails","Ergodicity","tags/e","tags/online"],"tags":["e","online"],"content":"Fat tails preclude ergodicity\nFat tails preclude ergodicity because estimation of the various moments of the distribution will depend upon the specific time period and will tend not to converge to stable values even with increasingly large time periods.\nFor example, a sliding window view of the realizations of a random variable over time will observe very different means across windows. Without a large realization, the mean will tend to decay, then jump up when a large value is realized. Sample means change with sample size in fat-tailed distributions\n\nThis jumpiness precludes ergodicity — the mean will be unstable through time, which eliminates the timelessness of ergodic processes.\n\nTime is what prevents everything from happening at once. To simply assume that economic processes are ergodic and concentrate on ensemble averages – and a fortiori in any relevant sense timeless – is not a sensible way for dealing with the kind of genuine uncertainty that permeates open systems such as economies. – Randomness, Fat Tails and Ergodicity – A Keynesian Perspective on Knightian Uncertainty\n\nFat tails should be expected because Interaction generates non-normality and interactions are quite common in the real world.\nIf Fat Tails should be expected and fat tails preclude ergodicity, then it follows that ergodicity should not be expected. In the real world, ergodicity is the exception, not the rule.\n\nTails should not be unexpected, for they are the rule. As the world becomes increasingly integrated – financially, economically, socially – interactions among the moving parts may make for potentially fatter tails. Catastrophe risk may be on the rise. – Randomness, Fat Tails and Ergodicity – A Keynesian Perspective on Knightian Uncertainty\n\nDespite the empirical rarity of true Ergodicity, much of financial and economic theory implicitly imply Ergodicity in its assumptions and conclusions.\n\nReferences\neonline"},"First-impression-are-high-variance":{"title":"First impression are high variance","links":["Information-is-surprisal","Volatility-is-information","tags/e","tags/online"],"tags":["e","online"],"content":"First impression are high variance\nThe idea that first impression matter can be interpreted in information theoretic terms. If Information is surprisal, variance and deviation from expectations are informative. Volatility is information, thus information sources with high variance are thus more informative than those with low variance, assuming the variance represents true signal rather than noise.\nFor first impressions to matter as much as people claim, they must be high variance. Otherwise, they wouldn’t be informative. So the statement that “first impressions matter” is simply a statement that first impressions are high variance.\nThis variance represents risk. If you want to control what people think of you, relying on making a great first impression is a very risky strategy, as it’s inherently a high variance information channel. This is partly why I write so much – I’ve found first impressions to be too unreliable and risky, and I’d rather tamp down the variance and exert more control over the impression I make on others.\n\nReferences\neonline"},"Focus-on-the-residuals":{"title":"Focus on the residuals","links":["Information-is-surprisal","Highly-likely-events-do-not-yield-much-information","residuals","Autoregressive-models","RBC","tags/e","tags/online"],"tags":["e","online"],"content":"Focus on the residuals\nFocus on what your models of the world get wrong. This is where the signal lies, and this signal can be used to update your mental models. Information is surprisal, so do not pay much attention to information sources whose output you could predict a priori. They do not yield much information (Highly likely events do not yield much information), nor are they useful in updating your models.\nFurther, many models can only be applied to residualized data, representing deviations from some expectation or trend (e.g. Autoregressive models, Real business cycles, etc).\n\nReferences\neonline"},"For-the-effectual-reasoner,-residuals-are-inputs-rather-than-outputs":{"title":"For the effectual reasoner, residuals are inputs rather than outputs","links":["Nassim-Taleb","Effectual-reasoning","Effectual-reasoning-starts-immediately-with-action-based-on-the-means-available-without-significant-planning-or-forethought","Planning-is-prediction","Why-Greatness-Cannot-Be-Planned","Errors-light-the-path","What-makes-entrepreneurs-entrepreneurial","Antifragile","The-Greatest-White-Pill-of-All","tags/e","tags/online"],"tags":["e","online"],"content":"For the effectual reasoner, residuals are inputs rather than outputs\nPer Nassim Taleb, in standard prediction contexts, errors are are exactly that: errors, problems, not good stuff, etc.\nThe effectual reasoner treats errors in the opposite way, as inputs rather than outputs, as valuable resources rather than wasteful exhaust.  Effectual reasoning starts immediately with action based on the means available without significant planning or forethought, thus they avoid prediction, which is effectively a form of planning (Planning is prediction) and just let the errors be what they are. It’s almost as if they are collecting errors (the stepping stones of Why Greatness Cannot Be Planned) along the path. The errors themselves light the path (Errors light the path).\n\nReferences\nWhat makes entrepreneurs entrepreneurial\n\nTransclude of What-makes-entrepreneurs-entrepreneurial#d5d76c\n\nAntifragile\n\nTransclude of Antifragile#7d4a7c\n\nThe Greatest White Pill of All\neonline"},"Generalized-Method-of-Moments":{"title":"Generalized Method of Moments","links":["Maximum-likelihood-estimation","data-generating-process","variance","estimator","OLS","@cochraneAssetPricing2005","@ackerbergIdentificationPropertiesRecent2015","@olleyDynamicsProductivityTelecommunications1996","@wooldridgeIntroductoryEconometricsModern2012","John-Cochrane","@jordaLocalProjections","Delta-method","tags/e","tags/online"],"tags":["e","online"],"content":"Generalized Method of Moments\nGMM is a method for estimating unknown parameters. GMM is often compared with Maximum likelihood estimation, another method for estimating parameters. In stark contrast to MLE, GMM does not require taking a stance on the probability distributions governing the parameters in question or the DGP. This makes it a much more flexible and widely applicable method, at the cost of lower precision.\nGMM estimation sets out to identify parameters by leveraging various “moments,” as in the statistical sense. These moments may not be merely statistical; they could come from credible economic theory or any other source. These moments are functions of the desired parameters and assumed to equal zero in population, suggesting that the correct parameters values are those that most closely get the chosen moments to zero or near zero.\nE[g(x)] = 0\nThe constraints setting these moment conditions to particular parameter values are called moment conditions. The sum of squared deviations from zero across the moment conditions is a common choice for minimization.\n\\underset{x}{\\text{min}} \\sum_m g(x)^2\nAs a general rule, one needs at least as many moments as there are parameters to estimate (“X equations, \\leq X unknowns”). If the number of moments and parameters are exactly equal, this is referred to as Method of Moments. If there are more moments than parameters, then we are in the realm of Generalized Method of Moments.\nWith more moments than parameters, some scheme must be devised to weigh the various moment conditions. In such cases, rather than minimizing the simple sum of squares (implying a identity matrix as the weighting matrix), we can choose a weighting matrix, the optimal of which would assign high weights to low variance moment conditions and low weights to high variance conditions, so as to balance their influence on the minimization objective:\n\\underset{x}{\\text{min}} \\sum_m g(x)^TWg(x)\nSince the optimal weighting matrix is unknown / infeasible (since it requires knowing the population variances), we instead estimate it via various methods:\n\nWeight matrix = Identity matrix (suboptimal, but easy)\nTwo-step estimator: Assume weight matrix is the identity matrix, minimize objective function, use sample variances of the moment conditions to calculate new weight matrix as W = (g(x)g(x)^T)^{-1}\nIterated estimator: Same two-step, but keep iterating until parameters stop moving\n\nMany alternative methods of estimation are effectively special cases of GMM, including OLS and MLE.\nStandard errors for GMM can be calculated as follows:\n\\text{var}(x) = \\frac{1}{T}(d&#039;Wd)^{-1}\nwhere d is the partial derivative of the moment condition with respect to a particular parameter:\nd = \\frac{\\delta g(x)}{\\delta x}\nThis implies that parameters that have a large impact on the moment conditions have lower standard errors / more precision.\nAsset pricing\nIn asset pricing, GMM can be interpreted as:\n\nThe moment conditions are pricing errors, which naturally should be as close to zero as possible for as many assets as possible. These errors are proportional to alpha\nGMM picks parameters to minimize the weighted sum of squared pricing errors\nThe second stage picks the combination of pricing errors (asset prices) that are the best measured / have the least variance\n\nNote that per Asset Pricing, variables in GMM must all be stationary.\n\nReferences\nIdentification Properties of Recent Production Function Estimators\nThe Dynamics of Productivity in the Telecommunications Equipment Industry\nIntroductory Econometrics: A Modern Approach\nGMM Notes from John Cochrane\nAsset Pricing\n@jordaLocalProjections\nDelta method\neonline"},"Get-leverage-on-the-fixed-cost-of-pain":{"title":"Get leverage on the fixed cost of pain","links":["Operating-leverage","tags/e","tags/online"],"tags":["e","online"],"content":"Get leverage on the fixed cost of pain\nTreat the difficulties you run into as fixed costs over which you can gain Operating leverage. If you are going to pay the cost, why not get the most out of the situation?\nA complementary idea is that you shouldn’t embark on endeavors with significant upfront costs unless you plan to exploit them to the maximum possible extent. Otherwise you miss out on the operating leverage opportunity. For so many things, the biggest gains are in the “out years” so to speak, often due to compounding.\n\nReferences\nOperating leverage\neonline"},"Goals-should-be-binomial":{"title":"Goals should be binomial","links":["variance","Discarding-details-opens-up-possibilities","tags/e","tags/online"],"tags":["e","online"],"content":"Goals should be binomial\nPrefer binary, discrete goals that don’t depend on the opinions of others.\nBinary: Two possibilities are better than many, so aim for binomial goals. Heads or tails, that’s it. This limits the variance of outcomes. Binomials are nice because they have bounded variance. Variance creates risks and uncertainty, which humans don’t do well with and can only drive manic stress. You will live happier and more stress-free if you limit your goals to things that can be distilled into binary efforts.\nDiscrete: Better to have goals with strict success criteria – you either do the thing or you don’t do the thing. Something happens or it doesn’t. Don’t attempt to achieve a particular “level” of success along some continuous spectrum. Focus on discrete objectives or tasks. “Write the essay”. “Send the email”. “Host the dinner”. This doesn’t mean quality doesn’t exist – it does – you’re just choosing not to focus on it explicitly. This doesn’t mean there aren’t intermediate steps to accomplish – there are, but they’re obvious. Discarding details opens up possibilities.\nDon’t depend on the opinions of others: Goals that depend on the opinions of others are unrealistic. Such goals are outside of your control, and create stress and worry. They require you to model the minds of others – a fraught exercise. Modeling yourself and the non-sentient world is a big enough task on its own. Limit your goals to those within your zone of control.\n\nReferences\nBinomial Distribution\n\nthe binomial distribution with parameters n and p is the discrete probability distribution of the number of successes in a sequence of n independent experiments, each asking a yes–no question, and each with its own Boolean-valued outcome: success (with probability p) or failure (with probability q = 1-p\n\neonline"},"Good-ideas-are-testable-ideas":{"title":"Good ideas are testable ideas","links":["Antifragile","Maximize-your-output-of-testable-ideas","Just-do-more","Iterative-tinkering-enhances-effective-IQ","Philosopher's-Stone","Bootstrapping-confidence","Sasha-Chapin-–-Awaken-the-Writer-Within","tags/e","tags/online"],"tags":["e","online"],"content":"Good ideas are testable ideas\nAn idea can be good in two ways. It can be good in the traditional sense, in that it comports well with reality or generates some useful output. It can also be good in the Antifragile sense - it may or may not turn out to work, but it yields some non-linear asymmetry when a certain function is applied to it. That function is testing.\nIt is in the testing of the idea that its benefits are identified. Realistically, it’s hard to know beforehand whether or not an idea is “good” in that traditional sense. Therefore, we should think of ideas as being good if they are testable, since that is the process by which their usefulness is identified. It is therefore much more robust to focus on coming up with testable ideas rather than ideas that are “good”, as that would require prediction, which is fragile.\nOnce we understand that goodness is implied by testability, it becomes obvious we should Maximize your output of testable ideas. By maximizing your output of testable ideas, you increase your odds of finding a good idea. Therefore, measure your productivity in terms of testable idea output, rather than some subjective a priori measure of idea quality. Just do more.\nTo analogize, Iterative tinkering enhances effective IQ, as does iterative idea generation and testing of those ideas. We don’t need to be “smart” in the sense of having the ability to pull good ideas out of thin air, fully-formed. We increase effective intelligence via iteration. This is the Philosopher’s Stone.\nBy focusing on testable ideas, we also Bootstrapping confidence. The goal isn’t to identify the solution to the problem in one shot. It’s to come up with enough testable hypotheses that we can have some statistical confidence in eventually finding a solution. If we trust the process and earnestly carry it out, we can reach a point of confidence much sooner in our intellectual journey.\n\nReferences\nSasha Chapin – Awaken the Writer Within\nTransclude of Sasha-Chapin-–-Awaken-the-Writer-Within#b4fff0\neonline"},"Good-representations-reduce-sample-complexity-of-downstream-tasks":{"title":"Good representations reduce sample complexity of downstream tasks","links":["sample-complexity","tags/e","tags/online"],"tags":["e","online"],"content":"Good representations reduce sample complexity of downstream tasks\n\nGood, useful representations reduce sample complexity on downstream supervised tasks. sample complexity is the number of training examples a machine learning model needs to successfully learn a target function. Therefore, good representations reduce the amount of labeled training data needed for downstream tasks to achieve a given level of performance.\nSolving a downstream supervised task with good pre-trained representations only requires samples on the order of the dimensionality of output. So binary classification tasks will only require ~2 examples during fine-tuning to “solve” the task.\n\n\nReferences\neonline"},"Granular-IV":{"title":"Granular IV","links":["Principal-component-analysis","The-Dark-Matter-of-Software-Valuations","Olley-Pakes-decomposition","@gabaixGranularInstrumentalVariables2020","@melitzDynamicOlleyPakesProductivity2015","Granular-IV","@gabaixSearchOriginsFinancial","tags/e","tags/online"],"tags":["e","online"],"content":"Granular IV\nHow to calculate granular instrumental variables\n\nRun regression of endogenous variable on exogenous variable(s) while controlling for time and unit fixed effects (alternatively, just unit effects).\nRun Principal component analysis on the residuals from the regression in (1) to extract common factors (time x components) and unit-specific loadings (components x unit), using time as the rows (samples) and the units as the columns (features) of the matrix. Can use any number of components, though two is usually good enough.\nSubtract from the residuals the dot product of factor and loadings to get the idiosyncratic shocks\nCalculate weighted average of idiosyncratic shocks, optionally excluding (weighted) shocks below certain threshold\n\nThe reason to run PCA is that you almost certainly have some omitted variables issues in the prior regressions and PCA lets you do a slightly better job in isolating the truly idiosyncratic component. I do a similar procedure in The Dark Matter of Software Valuations.\nConnection to Olley-Pakes decomposition\nGranular IV approach of Granular Instrumental Variables is quite related to decompositions of @melitzDynamicOlleyPakesProductivity2015.\nIn granular IV, size-weighted shocks are subtracted from equal weighted shocks:\nZ_t = u_{St} - u_{Et}\nIn Olley-Pakes decomposition, where overall weighted average is decomposed into equal-weighted average and covariance between the weights and things being averaged:\n\\Phi_t = \\bar{\\phi_t} + cov(s_{it}, \\phi_{it})\nThis can be re-arranged:\n\\Phi_t - \\bar{\\phi_t} = cov(s_{it}, \\phi_{it})\nThus the difference in shocks from granular IV is effectively the covariance of the shocks and the size weights. This is why there needs to be some skew in the size distribution in order for the granular IV to work — if not, the covariance between size and shocks will be zero, as there would be no variation in size weights.\nOne minor difference — when dealing with changes over time, Granular IV uses the average change while Olley-Pakes decomposition uses the change in the average.\nReferences\nGranular Instrumental Variables\nIn Search of the Origins of Financial Fluctuations: The Inelastic Markets Hypothesis\nGranular IV code per Romain Lafarguette\nScikit-learn PCA\nStatsmodels PCA\neonline"},"Granularity":{"title":"Granularity","links":["@carvalhoLargeFirmDynamics2019","Statistical-Consequences-of-Fat-Tails","Enterprise-Software-Monetization-is-Fat-Tailed","Sample-means-change-with-sample-size-in-fat-tailed-distributions","Mean-reversion","@gaubertGranularComparativeAdvantage2021","@koijenDemandSystemApproach","variance","@GranularOriginsAggregate2011","@koijenWhichInvestorsMatter2021","@FirmsDestinationsAggregate2014","@carvalhoGreatDiversificationIts2013","@gabaixGranularInstrumentalVariables2020","@gabaixSearchOriginsFinancial","@grassiWhyRiskySectors","tags/e","tags/online"],"tags":["e","online"],"content":"Granularity\nWhat it means to be granular\nGranularity can mean multiple things:\n\nHigh concentration or skewness ⇒ individual units matter\nFinite or small number of units ⇒ we live in world of pre-asymptotics\nHigh idiosyncratic volatility ⇒ units do their own thing\n\nPhenomena associated with high granularity\n\nFaster growth\n\nFat-tailedness is associated with faster growth because positive idiosyncratic shocks to large firms make the size distribution more skewed and lead to faster growth\nSee Large Firm Dynamics and the Business Cycle\n\n\nIncreasing returns to scale\n\nIn granular, fat-tailed variables, aggregating over larger quantities of underlying units increases the sample mean. Thus, a doubling of units leads to a more than doubling of the variable of interest\nSee Statistical Consequences of Fat Tails, Enterprise Software Monetization is Fat-Tailed, Sample means change with sample size in fat-tailed distributions\n\n\nMean reversion\n\nGranular/idiosyncratic shocks tend to mean revert much more quickly and forcefully\nSee Granular Comparative Advantage, A Demand System Approach to Asset Pricing\n\n\nCross-sectional variance\n\nGranularity drives larger variance across the cross-section\nSee Granular Comparative Advantage\n\n\nVolatility over time\n\nIdiosyncratic dynamics account for large share of the evolution of a variable associated with a unit over time\nSee Granular Comparative Advantage, Large Firm Dynamics and the Business Cycle\n\n\nPredictability\n\nLagged idiosyncratic shocks can be used to forecast economic variables\nSee The Granular Origins of Aggregate Fluctuations, A Demand System Approach to Asset Pricing\n\n\nConcentrated, attributable growth\n\nIf the variable in question is concentrated, growth or changes in that variable will be attributable to particular subcomponents\nSee Which Investors Matter for Equity Valuations and Expected Returns?, The Granular Origins of Aggregate Fluctuations\n\n\n\n\nReferences\nLarge Firm Dynamics and the Business Cycle\nTransclude of @carvalhoLargeFirmDynamics2019#b2f874\nFirms, Destinations, and Aggregate Fluctuations\nThe Granular Origins of Aggregate Fluctuations\nThe Great Diversification and its Undoing\nGranular Instrumental Variables\nIn Search of the Origins of Financial Fluctuations: The Inelastic Markets Hypothesis\nGranular Comparative Advantage\nWhy Risky Sectors Grow Fast\nTransclude of @gaubertGranularComparativeAdvantage2021#d2e50b\neonline"},"Hamilton-filter":{"title":"Hamilton filter","links":["@quastReliableRealTimeOutput2022","@canovaFAQHowExtract","tags/e","tags/online"],"tags":["e","online"],"content":"Hamilton filter\nProblems\nThe Hamilton filter suffers from a number of issues\n\nIt amplifies cycles of various frequencies unevenly\nThe extracted trend is noisy, rather than smooth\n\nThese flaws are made most clear via spectral analysis, a la @quastReliableRealTimeOutput2022 and @canovaFAQHowExtract:\nSpectral analysis clarifies a number of issues\n\nWithin the typical business cycle frequency range (6-32 quarters), cycles of various lengths are treated very differently\nVarious high frequencies (cycles of less than 6 quarters) are allowed through the filter\n\n\n\nInteresting idea: multivariate Hamilton filter\n\nSame as standard filter, but use multiple variables to forecast\n”Trend” is simply the forecasted outcome, cycle is the residual\nEssentially a local projection\n\n\nReferences\n@canovaFAQHowExtract\n@quastReliableRealTimeOutput2022\neonline"},"Highly-likely-events-do-not-yield-much-information":{"title":"Highly likely events do not yield much information","links":["There-is-no-alpha-in-predicting-highly-probable-events","tags/e","tags/online"],"tags":["e","online"],"content":"Highly likely events do not yield much information\nThe realization of a high probability event does not reveal much net-new information about the world. Going from 90% probability to 100% is a pitiful gain. Little uncertainty is resolved by the realization of high probability events. As such, high probability events are not terribly valuable. There is no alpha in predicting highly probable events\n\nReferences\neonline"},"Hodrick-Prescott-Filter":{"title":"Hodrick-Prescott Filter","links":["tags/e","tags/inbox/write","tags/online"],"tags":["e","inbox/write","online"],"content":"Hodrick-Prescott Filter\n\nReferences\newriteonline"},"Ideas-generate-increasing-returns-because-they-are-nonrivalrous":{"title":"Ideas generate increasing returns because they are nonrivalrous","links":["@jonesGrowthIdeas2005","tags/e","tags/Economics","tags/Economics/Growth","tags/Economics/Macro","tags/online"],"tags":["e","Economics","Economics/Growth","Economics/Macro","online"],"content":"Ideas generate increasing returns because they are nonrivalrous\nIdeas are nonrivalrous, which means they can be used multiple times by multiple parties without fighting over some limited stock of them. Naturally, this means that when we double our other inputs, we do not need to also double ideas in order to generate at least double the output. If we do happen to increase our ideas as well, then we will get more than double the output and, therefore, increasing returns. Thus, the ability of ideas to generate increasing returns is linked to their nonrivalry.\nReferences\n\nGrowth and Ideas\n\nTransclude of @jonesGrowthIdeas2005#54c28e\nTransclude of @jonesGrowthIdeas2005#9320a7\n\n\n\neEconomicsGrowthMacroonline"},"If-you're-not-sure-something-is-getting-better,-it's-not":{"title":"If you're not sure something is getting better, it's not","links":["Fat-Tails","Today,-@Foundersfund-Led...","The-Guerrilla-Guide-to-Interviewing-(version-3.0)-–-Joel-on-Software","tags/e","tags/online"],"tags":["e","online"],"content":"If you’re not sure something is getting better, it’s not\nGood heuristic, especially in fat-tailed domains where small differences in performance don’t really matter. In such cases, if it’s not obvious you’ve made a step function improvement, you either haven’t improved or haven’t improved sufficiently for it to matter.\n\nReferences\nToday, @Foundersfund Led…\n\nTransclude of Today,-@Foundersfund-Led...#6fb3f7\n\nThe Guerrilla Guide to Interviewing (version 3.0) – Joel on Software\nTransclude of The-Guerrilla-Guide-to-Interviewing-(version-3.0)-–-Joel-on-Software#461c9c\neonline"},"In-the-long-run,-all-costs-are-variable":{"title":"In the long-run, all costs are variable","links":["MIT-14.01-Principles-of-Microeconomics,-Fall-2018","tags/e","tags/online"],"tags":["e","online"],"content":"In the long-run, all costs are variable\nReferences\nMIT 14.01 Principles of Microeconomics, Fall 2018\neonline"},"Information-is-relative":{"title":"Information is relative","links":["Nassim-Taleb","tags/e","tags/online"],"tags":["e","online"],"content":"Information is relative\nWhat is informative to someone else is not necessarily informative to you. The informativeness of a information source depends on your a priori beliefs about that source, i.e. your priors. Different priors, different information from realization.\nThis is related to the point Nassim Taleb says, that Black Swans are relative. The Black Swan for the turkey is not necessarily a Black Swan for the butcher.\n\nReferences\neonline"},"Information-is-surprisal":{"title":"Information is surprisal","links":["Highly-likely-events-do-not-yield-much-information","tags/e","tags/online"],"tags":["e","online"],"content":"Information is surprisal\nSurprises inform. Non-surprises do not. Predictable events do not yield much information via their realization. Highly likely events do not yield much information via their realization, as little uncertainty is resolved via their realization.\n\nReferences\neonline"},"Interaction-generates-non-normality":{"title":"Interaction generates non-normality","links":["correlation","i.i.d.","Fat-tails-preclude-ergodicity","tags/e","tags/online"],"tags":["e","online"],"content":"Interaction generates non-normality\nWhen there are multiple agents, reacting to each other’s behaviors, the overall behavior will not be normally distributed. This is true even if the behavior of each of the individual agents is normal.\n\nThe interactions which generate non-normalities in children’s games repeat themselves in real world systems – natural, social, economic, financial. Where there is interaction, there is non-normality. – Randomness, Fat Tails and Ergodicity – A Keynesian Perspective on Knightian Uncertainty\n\nAn extension — another way to conceive of interaction is correlation. Correlated random variables effectively “interact” with one another. If this doesn’t make sense, consider the extreme opposite — two perfectly uncorrelated random variables clearly do not “interact” in any way.\nSo an equivalent statement would be that correlation generates non-normality.\nIt turns out that, in the real world, variables are rarely i.i.d. That is to say, variables in a system are rarely independent of one another. They frequently interact and are hence dependent. This dependency generates fat tails.\nFat tails are therefore common, much more common that they would be if these interactions did not occur\n-   Tails should not be unexpected, for they are the rule. As the world becomes increasingly integrated – financially, economically, socially – interactions among the moving parts may make for potentially fatter tails. Catastrophe risk may be on the rise.\nThis is closely related to the idea that Fat tails preclude ergodicity\n\nReferences\neonline"},"Invest-in-companies-like-you-invest-in-your-career":{"title":"Invest in companies like you invest in your career","links":["heuristic","Tweets-From-Saar-Gur","tags/e","tags/online"],"tags":["e","online"],"content":"Invest in companies like you invest in your career\nA simple heuristic for testing your conviction in a potential investment is whether or not you’d be willing to join the company yourself. Are you excited enough about the opportunity that you’d bet your career on it? The answer will of course be no for the vast majority of companies.\n\nReferences\nTweets From Saar Gur\nTransclude of Tweets-From-Saar-Gur#c6e255\neonline"},"Iterative-tinkering-enhances-effective-IQ":{"title":"Iterative tinkering enhances effective IQ","links":["Just-do-more","Sasha-Chapin-–-Awaken-the-Writer-Within","tags/e","tags/online"],"tags":["e","online"],"content":"Iterative tinkering enhances effective IQ\nWith rational trial and error, you don’t need to be very smart. Experimenting via tinkering can yield strong solutions to problems, even in areas where you are unskilled or that are extremely opaque, impenetrable to logic or analysis.\nOne could think of machine learning as an example of this. Even a highly performant model is “dumb” in some fundamental way, yet it can do amazing things as a result of many iterations of trial and error. By throwing more training data at the problem (aka Just do more), one can improve a model substantially without any idea of how it works or what the optimal solution looks like a priori.\n\nReferences\nSasha Chapin – Awaken the Writer Within\nTransclude of Sasha-Chapin-–-Awaken-the-Writer-Within#b4fff0\neonline"},"Just-do-more":{"title":"Just do more","links":["When-you-don't-know-what-to-do,-either-do-a-lot-or-nothing-at-all","Sasha-Chapin-–-Awaken-the-Writer-Within","tags/e","tags/online"],"tags":["e","online"],"content":"Just do more\nWhen you don’t know what to do, either do a lot or nothing at all. Assuming you want to do something, don’t worry about exactly how much to do it. That is secondary and not something you can come to a robust viewpoint on.\n\nReferences\nSasha Chapin – Awaken the Writer Within\nTransclude of Sasha-Chapin-–-Awaken-the-Writer-Within#9336dd\neonline"},"Labor-relationships-have-debt-like-features":{"title":"Labor relationships have debt-like features","links":["tags/e","tags/inbox/write","tags/online"],"tags":["e","inbox/write","online"],"content":"Labor relationships have debt-like features\nLabor relationships have a debt-like feature in the sense that they are difficult to get rid of or reduce. Businesses are generally reluctant to lay off workers and even more reluctant to cut wages. They end up reducing hiring instead, which in a sense is like consumers choosing to avoid taking on further debt after reaching a critical level of indebtedness. This leads to economic fluctuations with some not-so-nice features, in the same way that debt leads to relatively disastrous bank failures. These are “hard” failures – things suddenly break, and drastic action must be taken to stay afloat. It’d be great if there were some other way of navigating these situations. In the banking it probably involves higher capital requirements, with an emphasis on funding assets with equity rather than debt. In the labor market, it would require forging more flexible labor relationships, even more so than the “at will” stuff that we’re so used to in certain industries.\n\nReferences\nwww.wsj.com/articles/the-fall-of-a-trucking-giant-why-yellow-is-on-the-verge-of-collapse-3724f662\newriteonline"},"Learned-representations-are-more-important-than-what-you-do-with-them":{"title":"Learned representations are more important than what you do with them","links":["Machine-Learning","NLP","TPUs","Peak","Augmenting-Long-term-Memory","With-the-right-representation,-judgement-is-cheap","tags/e","tags/online"],"tags":["e","online"],"content":"Learned representations are more important than what you do with them\nIt strikes me that most of the latest advancements in Machine Learning and specifically NLP seem to depend much more on pre-training than on fine-tuning. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding is just one example of this, going so far as to use the word “pre-training” in the title of the paper, highlighting its relative importance. The “PT” in GPT stand for pre-training, another famous example.\n\nPre-training BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding takes 4 days on 16 TPUs whereas fine-tuning takes 1 hour on a single TPU. This is a 4x16x24/1 = 1,536x difference in training time. \n\nThis suggests that coming up with a generic, globally useful representation of an input is a much more important part of the learning process than learning exactly how to leverage that representation for judgement in a particular context. Pre-training takes more time and a much larger dataset than fine-tuning.\nPeak corroborates this and notes:\nTransclude of Peak#94e758\nIn this analogy, the forests are lower dimensional representations of the trees, which are the unprocessed raw data.\nResearch into the idea of “chunking” has found that memorization of chunks of information within a specific domain can act as IQ enhancement in that domain. Someone with otherwise low horsepower can achieve the performance of a high horsepower person in that field with the right knowledge chunks.\nTransclude of Augmenting-Long-term-Memory#80aa23\nTransclude of Augmenting-Long-term-Memory#d1ba99\nThis suggests that, once the right representation of an input is achieved, most of the hard work is complete. Fine-tuning is cheap. With the right representation, judgement is cheap.\n\nReferences\neonline"},"Linearly-separable-problems-are-easy-to-solve":{"title":"Linearly separable problems are easy to solve","links":["With-the-right-representation,-judgement-is-cheap","tags/e","tags/online"],"tags":["e","online"],"content":"Linearly separable problems are easy to solve\nLinearly separable problems can be solved with relatively crude classifiers, trained with very few examples. This is because linearly separable spaces can be divided by a single hyperplane, and the number of points of data required to categorize regions around this hyperplane scales with the dimensionality of the hyperplane. So a binary classifier only requires two data points to train if the representation linearly separates the downstream task. This is why, With the right representation, judgement is cheap\n\nReferences\neonline"},"Local-projections-vs.-VARs":{"title":"Local projections vs. VARs","links":["Local-projections","Vector-autoregression","local-projections","bias","@plagborg-mollerLocalProjectionsVARs2021","@liLocalProjectionsVs2021","@miranda-agrippinoBayesianLocalProjections","@barnichonImpulseResponseEstimation2019","Impulse-responses","Hodrick-Prescott-Filter","@phillipsBoostingWhyYou2021","tags/e","tags/inbox/write","tags/online"],"tags":["e","inbox/write","online"],"content":"Local projections vs. VARs\nI’ve read quite a bit on the relative advantages of local projections and VARs. I thought I’d summarize how to select between the two methods based on their respective strengths and weaknesses.\nRun local projections when:\n\nYou care a lot about bias\nYou have lots of data (at this helps mitigate its variance weakness)\nYou have many different series (since a VAR would have too many parameters)\nYou want to control for many lags (same rationale as above)\nFor whatever reason a you don’t have access to a good VAR package\n\nIt should be noted upfront that Local Projections and VARs Estimate the Same Impulse Responses in population. So the object being estimated is exactly the same in either case, at least in theory. What differs is the characteristics of the estimator itself. Practically, the two can differ after the horizon length exceeds the lag length. The two can also differ in sample.\nIn either case it’s much better to run penalized or Bayesian versions of these estimation methods, as per Local Projections vs. VARs: Lessons From Thousands of DGPs. Bayesian VARs can be estimated with a few different publicly available packages. Bayesian Local Projections are trickier, as there doesn’t seem to be any widely available packages for running them. As an alternative, use Impulse Response Estimation by Smooth Local Projections, for which there is code available. Worst case scenario – run vanilla local projections, then smooth the impulse response with a simple smoother like the one the ships with ggplot or the Hodrick-Prescott Filter (rather, its boosted alternative: Boosting: Why You Can Use the Hp Filter).\n\nReferences\newriteonline"},"Local-projections":{"title":"Local projections","links":["@jordaLongerRunEconomicConsequences2022","@rameyGovernmentSpendingMultipliers","@rameyMacroeconomicShocksTheir2016","@oleaLocalProjectionInferencea","stationarity","autocorrelation","Vector-autoregression","VARs-in-levels-vs-differences","Impulse-responses","@chenMasteringPanelMetrics2019","@meiNickellBiasPanel2023","Local-projections-vs.-VARs","tags/e","tags/online"],"tags":["e","online"],"content":"Local projections\nGood example papers\n\nLonger-Run Economic Consequences of Pandemics\nGovernment Spending Multipliers in Good Times and in Bad: Evidence from U.S. Historical Data\nMacroeconomic Shocks and Their Propagation\n\nI’m generally finding that despite the work of Local Projection Inference is Simpler and More Robust Than You Think that for local projections you’re much better off simply using a shock that is already stationary rather than including lags as controls in order to get to effective stationarity. Relying on lags of the independent variables just hasn’t worked very well for me. A quick survey of papers shows that most of the time the shock is already stationary or is made stationary via first differencing rather than via extra lags. They do include lags, but they are lags of an already stationary variable, and I think the intent there is mainly to control for autocorrelation rather than force stationarity. \nOne thing I’ve grappled with is whether to use lags of the first difference of non-stationary variables as controls or lags of the variables in levels. The vast majority of the literature is either silent on the question or uses first differences. But similar to this discussion in the context of VARs (VARs in levels vs differences),  everything should go through just fine in levels as long as you include lags. In fact, there should hypothetically be even less concern in the context of local projections than in VARs because with LP you are only concerned with the coefficient on a single variable. Unlike a VAR, where the impulse response is a complicated function of multiple parameters, in an LP it’s just a single parameter. To be fair it’s the same parameter of a series of separate regressions, but it’s effectively just one parameter. Further, first differencing is inappropriate if the variables do not in fact have unit roots.\nSpurious local projections\nHave to be careful when running local projections to ensure that the outcome and impulse variable are stationary. If they aren’t you can get some very weird results. The issue is that the typical transformations people do are not necessarily foolproof in a wide array of situations. With macro data taking first differences or long differences is generally enough to make data stationary, so that’s what people do. But in some cases even the first differences data can trend, in which case this won’t be enough. In those situations you must detrend the data or equivalently include a time trend among the regressors\nPanel LP\nNickell bias is a pervasive issue in panel local projections and one often not accounted for, leading to results that shade toward zero. The bias is especially problematic when the cross-sectional dimension is large. Some papers that cover this problem and provide a solution include\n\n@chenMasteringPanelMetrics2019\n@meiNickellBiasPanel2023\n\nThe solution is an easy modification of the fixed effects estimator that involves running the same specification on two subsets of the data, averaging them, and the subtracting this average from two times the uncorrected fixed effects estimator.\n\nReferences\nLocal projections vs. VARs\neonline"},"Look-for-high-talent-and-high-agency":{"title":"Look for high talent and high agency","links":["Focus-on-people","Agency","Elon-Musk","Spikiness","Day-479-and-High-Agency-–-Julie-Fredrickson","Tweets-From-Julie-Fredrickson","Tweets-From-Roon","The-Guerrilla-Guide-to-Interviewing-(version-3.0)-–-Joel-on-Software","tags/e","tags/inbox/write","tags/online"],"tags":["e","inbox/write","online"],"content":"Look for high talent and high agency\nOne way to Focus on people is to identify people with both high talent and high agency.\nHigh talent is in fact reasonably common. There are lots of smart people out there, people who have some high level of skill in some area. Superstar developers, 10X employees, etc. Big tech is full of these people. There are lots of people out there with high agency, as defined as ability and orientation toward making things happen. Those people are not always working on high impact projects, but they exist nonetheless.\nHowever, the intersection of the two – high talent and high agency – is relatively rare. I know this because I meet people every day with loads of talent but without that special, Elon Musk style agency – a go-getter attitude, a willingness to break through walls, etc.\nOf all the ways in which someone can spike, talent and agency are probably the two most important.\n\nReferences\nDay 479 and High Agency – Julie Fredrickson\nTransclude of Day-479-and-High-Agency-–-Julie-Fredrickson#362d52\nTweets From Julie Fredrickson\nTransclude of Tweets-From-Julie-Fredrickson#8c23cf\nTweets From Roon\nTransclude of Tweets-From-Roon#d8f257\nThe Guerrilla Guide to Interviewing (version 3.0) – Joel on Software\nTransclude of The-Guerrilla-Guide-to-Interviewing-(version-3.0)-–-Joel-on-Software#d94133\newriteonline"},"Maximize-the-entropy-of-your-information-sources":{"title":"Maximize the entropy of your information sources","links":["Information-is-surprisal","Maximize-your-return-on-attention","Residualize-your-information-sources","Focus-on-the-residuals","tags/e","tags/online"],"tags":["e","online"],"content":"Maximize the entropy of your information sources\nInformation sources which are predictable are not very valuable because the maximum information gain from reading such sources is limited.\nThink about the information gain or surprise (Information is surprisal) from a certain information source as the ratio of certainty to your a priori prediction:\n\\text{information gain} = \\frac{\\text{certainty}}{\\text{prediction}}\nFor example, if I can predict what the New York Times is going to say on some particular issue with 90% accuracy, then I don’t gain much from finding out what they actually said. That news source is not surprising. I go from being 90% sure to 100% sure, a mere 11% gain.\nIn information theory, surprise is quite literally the inverse of probability:\nS[p(x)] = \\log \\frac{1}{p(x)} = -\\log p(x)\nNotice the very similar form to the information gain formulation above.\nOne can think about the information gained from a news source as the return on your attention for that news source. If the information gain is low, then your return on attention for that source is low. You are rarely surprised by what this news source has to say. Information is surprisal, so by maximizing your surprisal, you maximize your information gain, and Maximize your return on attention.\nThis is related to the idea of Residualize your information sources. Imagine you have a model for certain news sources that can be framed as below:\ny = F(x) + \\varepsilon\nwhere x represents everything you know about this news source, F(\\cdot) is a function representing your internal mental model of that news source which converts what you know about it to your prediction of it, y is the actual information generated by the news source, and \\varepsilon is the error, or residual, of your predictions.\nIf you have a good model for a news source, you don’t need to actually take the time to find out what it actually said. You could guess ahead of time, take comfort in the accuracy of your predictions, and move on with your life.\nFocus on the sources of information for which your internal model can only generate poor predictions with large errors or residuals. Focus on the residuals\n\nIn general, information gain correlates with large residuals, as it implies you’re bad at predicting this source and would benefit significantly from the truth. Information is surprisal\n\n\nReferences\neonline"},"Maximize-your-output-of-testable-ideas":{"title":"Maximize your output of testable ideas","links":["Good-ideas-are-testable-ideas","Just-do-more","Philosopher's-Stone","Sasha-Chapin-–-Awaken-the-Writer-Within","tags/e","tags/online"],"tags":["e","online"],"content":"Maximize your output of testable ideas\nGood ideas are testable ideas. By maximizing your output of testable ideas, you increase your odds of finding a good idea.* Therefore, measure your productivity in terms of testable idea output, rather than some subjective a priori measure of idea quality. Just do more.\nThe best way to answer to answer a question is to come up with as many testable answers as possible, then go off and test them.\nBy doing this, we get maximum leverage out of the Philosopher’s Stone.\n\nReferences\nSasha Chapin – Awaken the Writer Within\nTransclude of Sasha-Chapin-–-Awaken-the-Writer-Within#b4fff0\nTransclude of Sasha-Chapin-–-Awaken-the-Writer-Within#30a817\neonline"},"Moving-average-model":{"title":"Moving average model","links":["Impulse-responses","Autoregressive-models","@stockIdentificationEstimationDynamic2018","tags/e","tags/inbox/write","tags/online"],"tags":["e","inbox/write","online"],"content":"Moving average model\nIn a moving average model, past shocks impact the current value of the variable directly, rather than having an indirect iterated effect that flows through past values of the observed variable. Thus you can read off the impulse response of a shock directly from the moving average coefficients.\nNotably, in a moving average model only the last q shocks affect the current value of the variable. This is in contrast to an autoregressive model where a shock impacts the variable forever.\nAs noted in Identification and Estimation of Dynamic Causal Effects in Macroeconomics Using External Instruments, the moving average model can be reached in a somewhat roundabout way by running a regression of the current value of the observed variable against current and past shocks:\n\nThus, a moving-average model is conceptually a linear regression of the current value of the series against current and previous (observed) white noise error terms or random shocks. (Wikipedia)\n\n\nReferences\nMoving-average model - Wikipedia\newriteonline"},"Network-egress-is-a-lock-in-tactic-for-public-cloud-providers":{"title":"Network egress is a lock-in tactic for public cloud providers","links":["An-App-Distribution-Network-in-Practice","Public-cloud-economics","Vendor-lock-in","tags/e","tags/online"],"tags":["e","online"],"content":"Network egress is a lock-in tactic for public cloud providers\nPublic cloud providers charge an extra fee for moving data out of their respective networks. This means that, once data is in, it becomes uneconomical to apply any services to that data that are not of the local cloud provider, as you would incur the egress cost as the data moves out. This also makes one-time migrations extremely expensive, to the point where the cost of data egress can overwhelm whatever near or long-term cost advantages the new platform might have.\nQuotes\nTransclude of An-App-Distribution-Network-in-Practice#1e0de9\nSource\nAn App Distribution Network in Practice\nPublic cloud economics // Vendor lock-in\neonline"},"Not-all-spurious-correlation-is-random":{"title":"Not all spurious correlation is random","links":["spurious-correlation","correlation","Bet-on-non-stationarity","tags/e","tags/online"],"tags":["e","online"],"content":"Not all spurious correlation is random\nPeople tend to talk about spurious correlation that drives from pure luck/randomness. Two unrelated timeseries end up correlated out of pure luck. The funny part is that this actually is a very low probability event for any two random timeseries that you pick, if they were both stationary. Sure the correlation might be somewhat positive or negative, but a 90%+ R^2 is actually quite unlikely for two completely random, stationary timeseries.\nThe thing that is actually more at fault a lot of the time is non-stationarity. Non-stationary timeseries have a tendency to be correlated between they are often trending in some direction, which tends to pull correlation away from zero mathematically. Non-stationarity is extremely common (Bet on non-stationarity).\n\nReferences\neonline"},"Observe-before-you-analyze":{"title":"Observe before you analyze","links":["With-the-right-representation,-judgement-is-cheap","Reading-Notes-The-Inner-Game-of-Tennis","tags/e","tags/online"],"tags":["e","online"],"content":"Observe before you analyze\nFocus on maximizing your skills of observation, not your skills of analysis.\nThere are two reasons for this:\n\nYou are already quite good at analysis and it tends to come naturally\nObservation is a more robust paradigm for achieving deep understanding\n\nOn the second point, remember that With the right representation, judgement is cheap, so it’s better to ensure you have the right representations of the situations you face before ruminating on the exact best next step or action to take. The best actions or solutions are often obvious once the situation is properly understood. Simply being able to accurately describe a situation is invaluable, and in many instances the process of doing so itself will illuminate nuggets of valuable information.\n\nReferences\nReading Notes The Inner Game of Tennis\nTransclude of Reading-Notes-The-Inner-Game-of-Tennis#15cac7\neonline"},"Perfect-competition-is-not-optimal-under-increasing-returns-to-scale":{"title":"Perfect competition is not optimal under increasing returns to scale","links":["@jonesGrowthIdeas2005","tags/e","tags/online"],"tags":["e","online"],"content":"Perfect competition is not optimal under increasing returns to scale\nIncreasing returns to scale causes classic models of equilibrium in perfect competition to break down. Normally, in perfect competition, factors of production are paid their marginal products. If there are increasing returns to scale,\nReferences\nGrowth and Ideas\n\nTransclude of @jonesGrowthIdeas2005#17fe4e\n\neonline"},"Planning-your-time-is-like-choosing-to-dance-on-beat":{"title":"Planning your time is like choosing to dance on beat","links":["Planning-your-time-is-scary","Deep-Habits-The-Importance-of-Planning-Every-Minute-of-Your-Work-Day","tags/e","tags/online"],"tags":["e","online"],"content":"Planning your time is like choosing to dance on beat\nPlanning your time means choosing to do all or some portion of the things you want to do on a specific schedule or cadence. The regular 15 minute, 30 minute, 1 hour, 1 day, etc. time increments are like the regular beats of a song. Every chunk of time that passes is another beat that drops. You’re not obligated to dance on beat, but it looks and feels a lot better when you do.\nMore flexibility, not less\nWhile it can initially feel constraining and even scary (Planning your time is scary), like it reduces degrees of freedom, what you invariably find is that there’s actually still a lot of flexibility, even conditional on being on beat:\nTransclude of Planning-your-time-is-scary#49b6ff\nIf anything, ingraining the idea that you should dance on beat, getting so good that it becomes natural and routine, and then mostly forgetting about it, expands the range of possibilities. Once you commit to being on beat, it doesn’t actually cost you much at “runtime”. Likewise, committing to doing things on a schedule doesn’t cost much once you get over the initial activation energy and skill required to create and stick to the schedule.\nWhat initially feel constraining eventually just becomes the air you breathe. You look with puzzled amazement at people who can’t dance on beat – they look strange and alien to you. Something is clearly “off”. Likewise, once you successful achieve intentional planning of your time, you’l look back on your prior self with amazement and wonder how you ever lived that way.\nGetting good\nIt does take work initially – it’s not a free lunch in the beginning. Ironically, what eventually will feel totally natural initially feels quite unnatural! Never fear – this too will pass. Just keep doing it, and eventually it will lock in place.\nAs with dancing, if you fall off beat, recognize that this has occurred, pause, find the beat again, and then get back on it. This cycle of losing the beat and finding it again is itself a form of training. Likewise with your work – falling off the plan, failing to plan at all, these are things that will happen with some regularity in the early days. Gently “get back on beat” – there’s no shame in it. If anything, noticing that you are off beat and coming to that conclusion yourself without the help of others is one of the greatest wins you can have in the early days. Not everyone can even do that.\n\nReferences\nDeep Habits The Importance of Planning Every Minute of Your Work Day\nTransclude of Deep-Habits-The-Importance-of-Planning-Every-Minute-of-Your-Work-Day#1fe3b8\nTransclude of Deep-Habits-The-Importance-of-Planning-Every-Minute-of-Your-Work-Day#653eb6\neonline"},"Pre-train-your-representations-before-learning-judgement":{"title":"Pre-train your representations before learning judgement","links":["Representation-Learning","Learned-representations-are-more-important-than-what-you-do-with-them","Ben-Franklin's-Autoencoder","With-the-right-representation,-judgement-is-cheap","Augmenting-Long-term-Memory","tags/e","tags/online"],"tags":["e","online"],"content":"Pre-train your representations before learning judgement\nLearning how to compress the world and represent its relevant information in the most useful way is an important skill (this is the essence of Representation Learning). Learned representations are more important than what you do with them, so focus most of your time on training your representation encoder. This is how Ben Franklin’s Autoencoder worked. This will pay dividends later, because With the right representation, judgement is cheap. Judgement will come much more naturally once you’ve pre-trained.\nWith regards to chunking per Augmenting Long-term Memory, this means that one should learn chunks first before trying to learn judgement. For example, don’t try to learn correct chess plays directly from raw information about the board state. First, learn how to better represent the state of the board. What to do will be much more obvious once this basic information has been correctly encoded. \n\nReferences\neonline"},"Prediction-is-compression,-compression-is-expression":{"title":"Prediction is compression, compression is expression","links":["tags/e","tags/online"],"tags":["e","online"],"content":"Prediction is compression, compression is expression\nRelating some ideas that I’ve been thinking about lately regarding the relationship between one’s mental representations, skill development, and expression.\nPrediction allows you to express a seemingly large amount of information in a smaller way via the parameters of whatever model you are using to predict.\nPerfect prediction is analogous to lossless compression, while imperfect prediction corresponds to lossy compression.\n\nSo whenever you have any sort of way of better predicting the future than chance, it corresponds to a better data compression algorithm for that type of data, where you store the same amount of information in your bits. – Prediction = Compression\n\n\nReferences\neonline"},"Principal-component-analysis":{"title":"Principal component analysis","links":["@baiPanicAttackUnit","@barigozziLargedimensionalDynamicFactor2021","@barigozziInferenceHeavyTailedNonstationary2022","@covarrubiasGoodBadConcentration2019","tags/e","tags/online"],"tags":["e","online"],"content":"PCA\nNon-stationary data\nWhen dealing with non-stationary data, you can take multiple approaches to PCA\n\nDifference the data, run PCA, then accumulate the estimated factors (@baiPanicAttackUnit)\nDifference the data, run PCA, apply the loading matrix to linearly detrended data (@barigozziLargedimensionalDynamicFactor2021)\nLinearly detrend the data, run PCA on the covariance matrix of the detrended data, apply the loading matrix to the detrended data (@barigozziInferenceHeavyTailedNonstationary2022)\n\nIn any case, you must standardize the data first before running PCA.\n\nReferences\n@covarrubiasGoodBadConcentration2019\nPrincipal Components Analysis and Factor Analysis\neonline"},"RLHF":{"title":"RLHF","links":["tags/e","tags/online"],"tags":["e","online"],"content":"RLHF\n\nReferences\neonline"},"Random-walk":{"title":"Random walk","links":["Stochastic-trend","tags/e","tags/online"],"tags":["e","online"],"content":"Random walk\nHelpful to think about a random walk as the ultimate version of a stochastic trend, in the sense that any shocks to the time series are fully incorporated into the “trend” of the series. The trend is effectively the running cumulative sum of the shocks.\n\nReferences\neonline"},"Trend-filtering":{"title":"Trend filtering","links":["stationarity","Hodrick-Prescott-Filter","@hamiltonWhyYouShould2017","@crumpSparseTrendEstimation2023","Normal-gamma-prior","@bhadraHorseshoeEstimatorUltraSparse2015","@bhadraLassoMeetsHorseshoe2019","tags/e","tags/online"],"tags":["e","online"],"content":"Trend filtering\nTrend filtering is the process of extracting a signal from a noisy time series.\ny_t = \\text{signal}_t + \\text{noise}_t\nHow “signal” and “noise” are defined is inherent to the particular trend filtering method in question. Any given method implicitly defines some notion of trend, which is to say the deterministic mean of the time series at a given point, and cycle, typically modeled as an autoregressive process in the general case and gaussian white noise in the zero autocorrelation limit. Importantly, this means the cycle will be stationary, while the trend may or may not be. Most trend filtering methods provide some way of trading off various concerns, primarily the degree of flexibility in the trend, summarized by the magnitude of its second differences, and the magnitude of the noise terms.\nIt’s helpful to interpret the trend filtering process in Bayesian terms.\np(\\text{signal}_{1:T}|y_{1:T}) \\approx p(y_{1:T}|\\text{signal}_{1:T}) \\times p(\\text{signal}_{1:T})\nWe can think of the tradeoffs highlighted above as being analogous (in fact, equivalent to) the tradeoff between the likelihood and prior in calculating the Bayes posterior, where the likelihood represents the noise term (where the data is modeled as being generated by the trend plus autoregressive cycle and where maximum likelihood → small noise), the prior represents our beliefs about the characteristics of the signal (primarily its shape / flexibility), and the posterior represents our belief about the location of the trend after observing the data (the trend is the “parameter” for which we are conducting inference). By enforcing strong priors about the signal, we can extract signals that “look like” what we’d expect based on those prior beliefs.\nIn econometrics, researchers often reach for the Hodrick-Prescott Filter which assumes a white noise cycle and offers a penalty parameter that can be used to trade off between the size of the squared second differences and the squared cycle terms:\n\\min_{\\text{signal}_{1:T}} \\sum_{t=1}^T\\text{noise}_t^2 + \\lambda (\\Delta^2 \\text{signal}_t)^2\nIt would perhaps be more accurate to say that the HP filter doesn’t necessarily assume a white noise cycle but rather is indifferent to the autocorrelation structure of the cycle and instead focuses merely on its magnitude.\nHowever, this “non-assumption” has important implications for the trend extracted by the HP filter. By assuming that the cycle is white noise when in fact it may not be, the filter implicitly penalizes the whole value of the cycle rather than accounting for the autocorrelation, which would increase the deterministic component of the cycle and reduce the noise component. Penalizing the entire cycle pushes the model to estimate a small cycle, leaving more of the data to be explained by the trend. This has the effect of increasing the variance of the trend, making it less smooth and less stable. This leads to the common artifact of HP filtered trends where the trend seems to lightly “wobble” up and down, even during periods of relative linearity in the source time series, creating the risk of spurious cycles (covered from a different angle in @hamiltonWhyYouShould2017). Increasing the penalty can help, but this runs the risk of introducing non-stationarity into the cycle, which is undesirable.\nIt should also be noted that the HP filter implicitly assumes normally distributed noise terms and second differences of the signal, as can be inferred from the “sum squared error” structure of the optimization problem. In that sense it is ridge regression in the time dimension, shrinking the second differences of the trend toward zero.\nNow, it is well-known that the ridge penalty shrinks towards zero but not all the way, leaving many non-zero terms. It is also known that the shrinkage happens fairly uniformly, essentially acting as a global shrinkage prior, which can lead to overshrinkage of truly large parameters and undershrinkage of small or zero magnitude parameters. The effect of this in the context of trend filtering is that the second differences will nearly always have some magnitude, even when zero may be more appropriate and will tend to be too small when large values are in fact more fitting.\nWhy does this matter? If one takes the slope of the trend to be some measure of the “long-run” growth rate of the time series and expects this long-run growth rate to change infrequently (i.e. be relatively constant), then the second differences of the time series should be zero most of the time, i.e. sparse. The normal prior on second differences unfortunately fails to achieve this. A Laplace prior / lasso penalization is also insufficient because although it is more likely to assign exactly zero, its tails are not heavy enough, leading to overshrinkage of large values. Further, a fully Bayesian approach won’t yield exact zeros, as the prior assigns infinitesimal probability to an exact zero value.\n@crumpSparseTrendEstimation2023 suggests alternative priors, opting for a spike-and-slab approach. Alternatives include the Normal-gamma prior (for which Laplace is a special case) and the horseshoe prior (@bhadraHorseshoeEstimatorUltraSparse2015, @bhadraLassoMeetsHorseshoe2019). Any will do – the main requirement is a large spike in probability mass at zero and reasonably heavy tails.\n\nReferences\neonline"},"Under-fat-tails,-look-for-quick-reasons-to-say-yes":{"title":"Under fat tails, look for quick reasons to say yes","links":["Searching-for-Outliers","Maybe-Your-Dating-Preferences-Are-Stupid","Fat-Tails","Bet-on-positively-fat-tailed-opportunities","Looking-for-the-Spike-in-a-Haystack","You-likely-haven't-seen-the-best-or-worst","Product-Market-Fit-is-Lindy","Craftspeople-How-to-Create-Good-Work","The-Case-for-American-Seriousness---by-Katherine-Boyle","tags/e","tags/online"],"tags":["e","online"],"content":"Under fat tails, look for quick reasons to say yes\nThis note summarizes my thoughts and learnings from a number of interesting articles:\n\nSearching for Outliers\nMaybe Your Dating Preferences Are Stupid\n\nFat Tails are weird in the sense that the flip the standard logic of optimal evaluation on its head, often necessitating the exact opposite behavior from what one might typically default to. Evaluation routines that make complete sense in a thin-tailed world are completely wrong in the realm of fat tails.\nFor example, in the realm of thin tails, it’s possible to, with a moderate amount of data, reach a fairly precise estimate of the importance of various characteristics to some outcome. With fat tails, there’s no amount of data within reason that will get you to precise parameter values.\nFurther, we must be vary careful about over-relying on these estimates, as the cost of saying no to a potentially very good thing could be quite high. One should Bet on positively fat-tailed opportunities, which necessitates suspension of disbelief for a moment. Humility is key – we don’t really know what we think we know, so don’t bother trying to use what we don’t actually know that well to eliminate potentially incredible options.\nWe should look for reasons to say yes, rather than reasons to say no. Avoid checklists. Almost any amazing opportunity will fail some number of requirements on a naive checklist. Overuse of checklist leads to only picking “well-rounded” options, which might have a decent chance of being better than average but have a poor chance of being exceptional. I see this in so many of my peers, as one example, but it applies in many places.\nIt’s fine to have dealbreakers, but they should be few in numbers. Use them sparingly, and only use the ones you have incredible confidence in. Be quick in your decisions, but don’t be fickle.\nFocus more of your efforts on finding reasons to say yes. What are the characteristics that, if present, could lead to an exceptional outcome? Look for the presence of these characteristics. The best option will spike in some particular way (Looking for the Spike in a Haystack), but may be mediocre in many other ways. Pass on opportunities because they lack these spiky characteristics, rather than because they have (or don’t have) some other, moderately bad (good) characteristics. Rule things in, rather rule things out.\nDon’t evaluate each option in isolation, from a blank page. This is likely to be error prone and exhausting at a minimum. Place each decision in the context of past decisions made by yourself and the collective experiences of others. Does this item look like the others that have done well? What are the shared, potentially exceptional characteristics?\nOnce you know what you want, be ruthless and quick in evaluating opportunities. Don’t dwell. You need to take many samples to arrive at a potential “high draw” from a positively fat-tailed distribution. Overindexing or optimizing over a single or small numbers of draws is a waste of time, as measured in terms of the opportunity cost of what you could have if you were only to draw more samples.\nRemember, You likely haven’t seen the best or worst, so if something is clearly far from the optimal outcome, you should cut bait quickly and feel no guilt in doing so. Similar to product-market fit, “fit” that isn’t found quickly is likely to never materialize (Product-Market Fit is Lindy), so don’t beat a dead horse. Get a move on!\n\nReferences\nYou likely haven’t seen the best or worst\nCraftspeople How to Create Good Work\nTransclude of Craftspeople-How-to-Create-Good-Work#23bc81\nThe Case for American Seriousness - by Katherine Boyle\nTransclude of The-Case-for-American-Seriousness---by-Katherine-Boyle#abc2eb\neonline"},"Under-fat-tails,-outliers-drive-movements-in-the-mean":{"title":"Under fat tails, outliers drive movements in the mean","links":["Fat-Tails","@GranularOriginsAggregate2011","Volatility-is-information","2022-01-10","Weighted-ACV","tags/e","tags/online"],"tags":["e","online"],"content":"Under fat tails, outliers drive movements in the mean\nThe law of large numbers fails when dealing with Fat Tails. Thus the influence of outliers is insufficiently diluted by their probably of occurrence. This means they move the mean when they occur. This is important, as many important economic variables aggregate fat tailed components (which is not always obvious, as in the case of inflation). The idiosyncratic fluctuations (The Granular Origins of Aggregate Fluctuations) of these components is often large enough that most of the period to period movement is driven by the outliers rather than the median component. The median could be flat over time, yet the weighted mean will bounce around. Thus, Volatility is information about the underlying components.\n\nReferences\n2022-01-10\nTransclude of 2022-01-10#406215\nWeighted ACV\neonline"},"Under-fat-tails,-unbiasedness-is-overrated":{"title":"Under fat tails, unbiasedness is overrated","links":["Fat-Tails","tags/e","tags/online"],"tags":["e","online"],"content":"Under fat tails, unbiasedness is overrated\nUnder fat tails, or situations with extremely high variance, being unbiased isn’t that helpful. The underlying variance will destroy you on a MSE basis. Better to be somewhat biased if it helps you significantly reduce variance.\n\nReferences\neonline"},"Use-TK-liberally":{"title":"Use TK liberally","links":["Write-furious-first-drafts","Writing-Wednesdays-The-Magic-of-TK","tags/e","tags/online"],"tags":["e","online"],"content":"Use TK liberally\nUse TK whenever possible during the drafting phase of writing, which will help you keep momentum and get out the first draft as quickly as possible (Write furious first drafts). Whenever it isn’t clear how to proceed, simply drop in a “TK” and move on with haste. This can even be used for evergreen notes, which can always be finished later and certainly do not need to be finished in one sitting.\n\nReferences\nWriting Wednesdays The Magic of TK\nTransclude of Writing-Wednesdays-The-Magic-of-TK#d717c1\neonline"},"Vector-autoregression":{"title":"Vector autoregression","links":["Dealing-with-nonstationarity","tags/e","tags/online"],"tags":["e","online"],"content":"Vector autoregression\nA structural VAR is simply one where the errors are uncorrelated with one another across the various included time series.\nDealing with nonstationarity\n\n”VAR models are usually guided by theory and are dynamic in nature, so you do not have to worry about the nonstationarity.” (Source)\n“There is an issue of whether the variables in a VAR need to be stationary. Sims (1980) and Sims, Stock and Watson (1990) recommend against differencing even if the variables contain a unit root. They argued that the goal of a VAR analysis is to determine the interrelationships among the variables, not to determine the parameter estimates. The main argument against differencing is that it “throws away” information concerning the comovements in the data (such as the possibility of cointegrating relationships). Similary, it is argued that the data need not be detrended. In a VAR, a trending variable will be well approximated by a unit root plus drift. However, majority view is that the form of variables in the VAR should mimic the true data-generating process. This is particularly true if the aim is to estimate a structural model.” (Source)\n\nOptions for Bayesian VAR\n\nPyFlux\npybvar\nPyMC\nLargeBvarPython\n\n\nReferences\nImpulse-Response Functions for VARs\nBayesian Vector Autoregression in PyMC\nBayesian vector autoregression models\nSome good notes:\n\n\nwww.eco.uc3m.es/~jgonzalo/teaching/timeseriesma/watsonvarnotes-reading.pdf\n\nVECM\n\neonline"},"Volatility-is-information":{"title":"Volatility is information","links":["Nassim-Taleb","Antifragile","Volatility-is-information","Information-is-surprisal","Surprise-as-general-notion-of-volatility","Effectual-reasoning","For-the-effectual-reasoner,-residuals-are-inputs-rather-than-outputs","tags/e","tags/online"],"tags":["e","online"],"content":"Volatility is information\nI’ve definitely seen this referred to somewhere other than my own notes.\nNassim Taleb is very likely a good reference source here. If Antifragile is things that gain from volatility, then if Volatility is information, Information is surprisal, and Surprise as general notion of volatility, then things that gain from information or surprise are Antifragile.\nIf you think of volatility as being related to entropy, high entropy is in some sense more informative than low entropy.\nEffectual reasoning is Antifragile because For the effectual reasoner, residuals are inputs rather than outputs, thus Effectual reasoning gains from residuals/surprises, as they lead to better output in the form of better goals or lightweight plans for reaching those goals.\n\nReferences\nAntifragile\n\nTransclude of Antifragile#7d4a7c\n\neonline"},"What-makes-entrepreneurs-entrepreneurial":{"title":"What makes entrepreneurs entrepreneurial","links":["Saras-Sarasvathy","tags/online","Effectual-reasoning","supervised-learning","unsupervised-learning","self-supervised-learning","Taylor-Pearson","Effectual-reasoning-starts-immediately-with-action-based-on-the-means-available-without-significant-planning-or-forethought","Don't-plan","Confidence-grows-with-action-and-shrinks-with-idleness","Effectual-reasoning-targets-low-cost-to-action-rather-than-high-ROI","It's-Impossible-To-Size-a-Market","Effectual-reasoning-ignores-competition","For-the-effectual-reasoner,-residuals-are-inputs-rather-than-outputs","readwise/Books/Why-Greatness-Cannot-Be-Planned","Nassim-Taleb","Peter-Thiel","Invest-in-unknowns"],"tags":["online"],"content":"What makes entrepreneurs entrepreneurial\nAuthors: Saras Sarasvathy\nSource: www.khoslaventures.com/wp-content/uploads/What_makes_entrepreneurs_entrepreneurial.pdf\n#literatureonline\nHighlights\nEffectual reasoning is the inverse of causal reasoning.\nCausal reasoning takes a chosen goal and available means and attempts to map the two to one another via some set of strategy, tactics, and plans. In this sense, causal reasoning is like trying to figure out the functional form and parameters of a model for turning known X into chosen Y (a la supervised learning).\nConversely, effectual reasoning assumes only a given set of means (X) and then allows the goals (Y) to organically emerge out of the primordial soup of means (like in unsupervised or self-supervised learning). Thus known X is mapped to unknown Y, and the mapping function and it’s parameters are jointly determined with the Y itself.\n\nSaid differently, causal reasoning is a way to figure out how to get what you want, while effectual reasoning is a way to determine what you want in the first place (along with how to get it), as articulated by Taylor Pearson in Effectual Reasoning: The Little Known Method Entrepreneurs Use to Figure Out What They Want.\nThe set of means available can be roughly bucketed as:\n\nWho you are\nWhat you know\nWhom you know\n\nEffectual reasoning starts immediately with action based on the means available without significant planning or forethought. Plans are fluid (Don’t plan): emerging and withering away over time, revised as needed based on actions taken and interaction with the outside world. (Confidence grows with action and shrinks with idleness) \nEffectual reasoning targets low cost-to-action rather than high ROI\nInstead of trying to analyze and identify large markets with high ROI (as critiqued in It’s Impossible To Size a Market), effectual reasoning merely goes after the first plausible, addressable market opportunity in the most direct fashion, in some cases selling the product before the product is even built! The emphasis is on low, affordable costs rather than the ROI of those costs. \nThe low cost of effectual reasoning is similar to the low cost of unsupervised learning and self-supervised learning. No labeled data is needed; you learn purely off of the unlabeled Xs, making the learning process much cheaper. It is much cheaper to identify and take action on emergent goals than figure out the mapping to pre-existing ones. \nEffectual reasoning ignores competition\nGiven that effectual reasoning doesn’t focus on identifying pre-existing markets, it naturally follows that it also doesn’t waste time analyzing the competitive landscape. With no defined market, there are no defined competitors. Rather than focus on competitors, effectual reasoning identifies strategic partners who can help the founder get to market with low cost-to-action. \nFor the effectual reasoner, residuals are inputs rather than outputs\nFor the effectual reasoner, action generates effects, and with enough time these effects generate achievable and desirable goals, the “stepping stones” of Why Greatness Cannot Be Planned. They also generate surprises. Importantly, these surprises are not deviations from the “correct” path but rather they are features of the landscape, and you should leverage these features to determine the path itself. They are landmarks which highlight the path, guiding you forward. They are the input to the next iteration or step of the algorithm. \nThe logic of effectual reasoning\nIn causal reasoning, the purpose of predicting the future is to control it. This is the entire point of causal inference. Presumably we want to infer causality so that the next time around we can leverage it to achieve desired aims, armed with the knowledge of the likely impact of our actions.\nIn effectual reasoning, the ability the control the future obviates the need to predict it in the first place. This avoidance of prediction echoes advice from Nassim Taleb. Rather than attempt to predict the future, effectual reasoning believes that action creates the future in the first place. There is not a future that exists as a concept separate from our actions, that could be discovered without any action. Peter Thiel has made a similar point on numerous occasions.\nAcknowledging this agency, effectual reasoners create games they are likely to win, rather than try to predict the outcome or come up with optimal strategies for existing games. In fact, the predictability of existing games makes them poor targets, as they are likely competitive, escalating conflicts. Unpredictable games are more easily shaped. Invest in unknowns.\nPDF\n\nReferences\nWhy Greatness Cannot Be Planned"},"When-in-doubt,-don't-difference":{"title":"When in doubt, don't difference","links":["stationarity","Hamilton-filter","Hodrick-Prescott-Filter","@canovaFAQHowExtract","tags/e","tags/online"],"tags":["e","online"],"content":"When in doubt, don’t difference\nIn my experience, differencing data to achieve stationarity is a fraught exercise. It destroys a significant amount of information, and only makes sense when the data is a true random walk or close to it. If you have any doubt that the data is in fact a random walk, I would be extremely careful about differencing it.\nThis is especially true in the case of multivariate series in which there may be cointegration. Differencing such series destroys the long-run equilibrium / correction behavior in the data, leading to poor inference and inferences.\nI would also be careful about differencing when you either don’t have much data or you know there’s likely a lot of measurement noise in the data. When you difference such data, you only amplify whatever noise is already present, to the point where the noise can overwhelm the signal. I don’t know a good rule of thumb, but I’d be wary of differencing when you have fewer than 100 datapoints.\nWhen in doubt, use another method, like polynomial detrending or a more sophisticated method like the Hamilton filter or HP filter. Per @canovaFAQHowExtract, polynomial detrending is actually quite robust.\n\nReferences\n\nwhen cointegration may be present, simply getting rid of nonstationarity by differencing individual series so that they are all stationary throws away vast amounts of information and may distort inference. – Christopher Sims, Comments and discussion: Disentangling the channels of the 2007–2009 recession\n\neonline"},"When-unskilled,-complicate-the-game-and-add-randomness":{"title":"When unskilled, complicate the game and add randomness","links":["The-Greatest-White-Pill-of-All","Tweets-From-vitalik.eth","tags/e","tags/online"],"tags":["e","online"],"content":"When unskilled, complicate the game and add randomness\nBy complicating the game, you create situation where a strong opponent has to compete with you on multiple overlapping dimensions. While your competitors might be strong along one or a few lines of skill, they are unlikely to be extremely competent at a complex combination of many skills. Thus, the skill distribution for multivariable competition is much narrower, giving you better odds of coming out on top.\nRandomness helps you when you have a high probability of losing anyway. Since the probability of losing is bounded at 100%, additional randomness generally only helps you, as it opens you up to positive tails that can lead to a win that you wouldn’t have otherwise been exposed to.\n\nReferences\nThe Greatest White Pill of All\nTransclude of The-Greatest-White-Pill-of-All#604d0a\nTransclude of The-Greatest-White-Pill-of-All#c52699\nTweets From vitalik.eth\nTransclude of Tweets-From-vitalik.eth#85c681\neonline"},"With-the-right-representation,-judgement-is-cheap":{"title":"With the right representation, judgement is cheap","links":["Learned-representations-are-more-important-than-what-you-do-with-them","Chunks-are-low-dimensional-representations","Cal-Newport","Deep-Work","Exploring-the-Limits-of-Transfer-Learning-with-a-Unified-Text-to-Text-Transformer","Pre-train-your-representations-before-learning-judgement","Prediction-Machines","tags/e","tags/online"],"tags":["e","online"],"content":"With the right representation, judgement is cheap\nWe can think about representation learning as the pre-training phase of model development whereas “judgement learning” corresponds to the fine-tuning phase.\nIt appears that pre-training of NLP models to achieve proper representations for words and sequences is more important to the the overall training regimen than fine-tuning on specific tasks. Learned representations are more important than what you do with them.\nA tell-tale sign of mastery is increasingly complex chunks. These chunks enable higher-level thinking with a fixed amount of compute horsepower, as one only needs to reason about the higher-level chunks rather than the raw information. This reduces perceived complexity and compresses information.\n\nA beginner would see “a pawn here, a rook there”, and so on, a series of individual pieces. Masters, by contrast, saw much more elaborate “chunks”: combinations of pieces that they recognized as a unit, and were able to reason about at a higher level of abstraction than the individual pieces.\nSimon estimated chess masters learn between 25,000 and 100,000 of these chunks during their training, and that learning the chunks was a key element in becoming a first-rate chess player. Such players really see chess positions very differently from beginners.\n\nChunks are analogous to representations. Chunks are low-dimensional representations. Memorizing chunks is thus similar to learning representations.\nCal Newport considers Deep Work to be tasks that you couldn’t train a college-educated person to do in a short period of time. In this sense he relates necessary training time directly to the value of the work. If we think about training time as a “cost” then it could be said that learning good judgement (fine-tuning) is cheap relative to learning useful representations (pre-training). The most valuable work or training regimen should be that which is most costly in terms of training time. If we extend this to the machine learning context, learning representations is the “Deep Work” of machine learning, whereas learning judgement is relatively shallow work (once representations have been learned).\nTransclude of Learned-representations-are-more-important-than-what-you-do-with-them#757870\nThe notion that judgement is cheap is corroborated by the architectures of pre-trained NLP models (BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding), which oftentimes only require a single layer or shallow feed forward network to turn the input representation into some required task output. This is often overlooked but in some ways is an amazing result — with the right representation, you need almost no additional computation to successfully complete a wide range of difficult tasks.\n\nOur task-speciﬁc models are formed by incorporating BERT with one additional output layer, so a minimal number of parameters need to be learned from scratch. – BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n\nAccording to Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, fine-tuning a model from scratch (which isn’t really fine-tuning) results in much worse performance than with pre-training before fine-tuning. Fine-tuning is thus only cheap when the model has learned the right representation first. Judgement is cheap when equipped with the right representation, otherwise it’s quite expensive.\nExpanding this outside the machine learning context, this idea would imply that attempting to learn judgement directly from the data itself (yours or others’ experiences) without first pre-processing the data or “pre-training” yourself is inefficient and unlikely to work. Pre-train your representations before learning judgement. In doing so, be careful not to fool yourself into thinking something is valuable simply because it takes a long time.\nTransclude of Pre-train-your-representations-before-learning-judgement#6bef51\nThis differs markedly from the main thesis of Prediction Machines, which is that machine learning models mainly concern themselves with prediction and that judgement remains within the domain of humans. The authors also predict that judgement will rise in value given its complementary with prediction.\n\nBut if judgement is in fact cheap once you are good at generating representations, I’m not so sure that humans will remain valuable in this way.\nI could be misreading this though. Perhaps they don’t think of being able to generate Q&amp;A answers as judgement but really prediction. There still needs to be a human in the loop to know what to do with those predictions and make decisions. I’m not sure about this.\n\n\n\nReferences\neonline"},"boosting":{"title":"boosting","links":["tags/e","tags/online"],"tags":["e","online"],"content":"boosting\n\nReferences\neonline"},"published":{"title":"published","links":[],"tags":[],"content":"Published Online Content\nTABLE \n    round(file.size / 1024, 2) + &quot; KB&quot; AS &quot;Size&quot;,\n    length(file.text) AS &quot;Words&quot;,\n    file.mtime AS &quot;Modified&quot;\nFROM #online\nSORT file.mtime DESC\n\n/* Base table styles */\n.dataview.table-view-table {\n    width: 100%;\n    table-layout: fixed;\n}\n\n/* Column widths */\n.dataview.table-view-table td:nth-child(1) {\n    width: 40%;\n    max-width: 300px;\n}\n\n.dataview.table-view-table td:nth-child(2) {\n    width: 15%;\n    max-width: 100px;\n}\n\n.dataview.table-view-table td:nth-child(3) {\n    width: 15%;\n    max-width: 100px;\n}\n\n.dataview.table-view-table td:nth-child(4) {\n    width: 30%;\n    max-width: 200px;\n}\n\n/* Cell content handling */\n.dataview.table-view-table td {\n    white-space: nowrap;\n    overflow: hidden;\n    text-overflow: ellipsis;\n    padding: 4px 8px;\n}\n\n/* Allow cell content to wrap on hover */\n.dataview.table-view-table td:hover {\n    white-space: normal;\n    overflow: visible;\n    background: var(--background-primary);\n}\n\nI’ve created a new file called published.md with:\n\nAn H1 header describing the content\nThe Dataview query that will generate a table of all #online tagged files\nThe same columns and sorting as previously discussed\n\nThe table will automatically populate when viewed in Obsidian with the Dataview plugin installed."},"reduced-form":{"title":"reduced form","links":["tags/e","tags/online"],"tags":["e","online"],"content":"reduced form\nReduced form equations frame an endogenous variables in terms of purely exogenous variables.\n\nReferences\neonline"},"reinforcement-learning":{"title":"reinforcement learning","links":["tags/e","tags/online"],"tags":["e","online"],"content":"reinforcement learning\nThe main thing to keep in mind with reinforcement learning, the main thing that distinguishes it from other kinds of model training is that that model is only rewarded for producing the right final answer. How to get to that final answer is left to the model to figure out. The model may explore or search around initially to find behaviors that consistently lead to rewards. But again, the reward is for the final answer, not for any particular kind of behavior that might lead to that. This means that novel behaviors can be discovered by the model itself rather than needing to be pre-programmed by the researcher.\nIn contrast, standard supervised fine-tuning tells the model directly how to behave, i.e., the behaviors are directly “rewarded” in some sense. This is partly why fine-tuning is very powerful but also fragile – it’s a very dense and pure signal, which strongly affects how the model behaves. If you aren’t careful, you’ll overtune the model, making it overfit to a particular behavior pattern.\n\nReferences\neonline"},"self-supervised-learning":{"title":"self-supervised learning","links":["self-supervised-learning","unsupervised-learning","Representation-Learning","Transfer-learning","tags/e","tags/online"],"tags":["e","online"],"content":"self-supervised learning\nself-supervised learning is a form of unsupervised learning where the data itself provides the supervision without access to handmade training labels.\nIn general, withhold some part of the data and task the model with predicting the masked portion. The model generates a useful representation (Representation Learning) of the input data via its parameters which can then be used in some downstream task. This is an example of Transfer learning.\nThe loss of the self-supervised learning task serves as a proxy for the downstream task that the model will be fine-tuned on.\n\nReferences\neonline"},"shock":{"title":"shock","links":["tags/e","tags/inbox/write","tags/online"],"tags":["e","inbox/write","online"],"content":"shock\n\nReferences\newriteonline"}}