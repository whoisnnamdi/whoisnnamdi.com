{"@adamekLassoInferenceHighDimensional2022":{"title":"Lasso Inference for High-Dimensional Time Series","links":["LASSO","@belloniHighDimensionalMethodsInference2014","@vandegeerAsymptoticallyOptimalConfidence2014","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Lasso Inference for High-Dimensional Time Series\nRobert Adamek, Stephan Smeekes, Ines Wilms\nLink\nAbstract\n\nIn this paper we develop valid inference for high-dimensional time series. We extend the desparsified lasso to a time series setting under Near-Epoch Dependence (NED) assumptions allowing for non-Gaussian, serially correlated and heteroskedastic processes, where the number of regressors can possibly grow faster than the time dimension. We first derive an error bound under weak sparsity, which, coupled with the NED assumption, means this inequality can also be applied to the (inherently misspecified) nodewise regressions performed in the desparsified lasso. This allows us to establish the uniform asymptotic normality of the desparsified lasso under general conditions, including for inference on parameters of increasing dimensions. Additionally, we show consistency of a long-run variance estimator, thus providing a complete set of tools for performing inference in high-dimensional linear time series models. Finally, we perform a simulation exercise to demonstrate the small sample properties of the desparsified lasso in common time series settings.\n\nThe key aim of the authors is to do inference in a lasso context, which is complicated by the fact that the variable selection feature of lasso renders standard inference invalid. Basically, when you do variable selection on the outcome variable, you potentially drop relevant covariates for properly measuring the coefficient on a “treatment” variable of interest. This makes inference on the parameters invalid. Therefore one needs post-selection inference methods. @belloniHighDimensionalMethodsInference2014 is an example of this, leveraging orthogonalization via Frisch-Waugh partialling out (post-double-selection) which ensures that covariates relevant to the treatment and/or outcome variable are kept in the regression.  The debiased/desparsified lasso of @vandegeerAsymptoticallyOptimalConfidence2014 is another approach.\nIn the desparsified lasso approach, you effectively add back a small amount of “noise” to the zeros from the original lasso estimates. This undoes model selection, but in return, you get valid inference, normally distributed uncertainty, etc. By definition the estimator is no longer sparse, but now you can make scientific statements about the coefficients of interest. One can think of this as “weak sparsity”.\nWe also need to address the IID issue. Another key tension that these authors address is the fact that lasso cannot be applied off the shelf to time series data, as the data are not IID. There is other research that also explores this, so this is not the first entry in the literature. They extend the desparsified lasso to the time series context. Their approach also allows the errors to be non-Gaussian and heteroskedastic.\nUnfortunately my sense is that the methods in this paper only apply to stationary data, or at least that is an assumption of the method, per these quotes (non-stationary data does not have constant or even finite moments). One may be able to get around this by first regressing non-stationary variables on a lag and using the residuals as regressors:\n\nAssumption 1(i) ensures that the error terms are contemporaneously uncorrelated with each of the regressors, and that the process has finite and constant unconditional moments.\n…\nWe provide a complete set of tools for uniformly valid inference in high-dimensional stationary time series settings, where the number of regressors N can possibly grow at a faster rate than the time dimension T .\n\nFrom some example DGPs they provide:\n\nAlso assume that the vector of exogenous variables wt is stationary and geometrically β-mixing as well with finite 2  ̄ m moments.\n…\nthe K × K matrices Φi satisfy appropriate stationarity and 2  ̄ m-th order summability conditions.\n\nHere’s my attempt at explaining desparsified lasso in simple terms:\n\nThe idea is to unsparsify the original lasso estimates, with the goal of removing the bias generated by standard lasso, which enables valid inference\nTake the initial lasso estimates and add to them some “noise” equal to the covariance of the residualized covariate in question with the residualized outcome variable (i.e. the regression scores) scaled by a factor, where the residualization is done via standard lasso\n\n\nReferences\npaperreadonline"},"@atheyGeneralizedRandomForests2018":{"title":"Generalized Random Forests","links":["local-projections","impulse-responses","random-forests","@chernozhukovDoubleDebiasedMachine2017","Local-projections","Impulse-responses","instrumental-variables","@stockIdentificationEstimationDynamic2018","@rameyMacroeconomicShocksTheir2016","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Generalized Random Forests\nSusan Athey, Julie Tibshirani, Stefan Wager\nLink\nAbstract\n\nWe propose generalized random forests, a method for non-parametric statistical estimation based on random forests (Breiman, 2001) that can be used to fit any quantity of interest identified as the solution to a set of local moment equations. Following the literature on local maximum likelihood estimation, our method considers a weighted set of nearby training examples; however, instead of using classical kernel weighting functions that are prone to a strong curse of dimensionality, we use an adaptive weighting function derived from a forest designed to express heterogeneity in the specified quantity of interest. We propose a flexible, computationally efficient algorithm for growing generalized random forests, develop a large sample theory for our method showing that our estimates are consistent and asymptotically Gaussian, and provide an estimator for their asymptotic variance that enables valid confidence intervals. We use our approach to develop new methods for three statistical tasks: non-parametric quantile regression, conditional average partial effect estimation, and heterogeneous treatment effect estimation via instrumental variables. A software implementation, grf for R and C++, is available from CRAN.\n\nWhen you have an estimator with low bias but high variance, averaging multiple noisy instances of that predictor helps reduce variance. It’s worth thinking about how to apply this in other arenas, such as local projections, which are known for their low bias but high variance. This logic would suggest estimating multiple impulse responses via local projections and then averaging them to get a lower variance estimate. How to generate these estimates is the key question. I’d imagine two potential options: (1) estimate the same model on different subsamples of the data, (2) estimate different models with different controls included on the same data:\n\nbecause individual trees \\hat\\mu_{b}(x) have low bias but high variance, such averaging meaningfully stabilizes predictions\n\nReminds me that it’s worth using random forests as the estimator for @chernozhukovDoubleDebiasedMachine2017 style estimates of local projections. It still surprises me that more people haven’t considered this, especially given all the work around deriving more “causal” impulse responses via instrumental variables variants of local projections (@stockIdentificationEstimationDynamic2018, @rameyMacroeconomicShocksTheir2016).\n\nReferences\npaperreadonline"},"@axlerLinearAlgebraDone":{"title":"Linear Algebra Done Right","links":["tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Linear Algebra Done Right\n**Sheldon Axler – **\nChapter 1: Vector Spaces\nComplex Numbers\nComplex numbers were invented so we could take square roots of negative numbers.\nComplex numbers are defined as an ordered pair of real numbers a and b, and are typically written as a +bi. One can even think of the set of real numbers as being a subset of the set of complex numbers, where b=0 for all real numbers. Note that a complex number is considered a single number, even though it involves an ordered pair of real numbers.\nAll the typical arithmetic operations and properties apply to complex numbers (commutativity, associativity, etc), so you can work with them fairly similarly.\nMultiply complex number as so:\n(a + bi)(c+di) = (ac-bd) + (ad+bc)i\nThe calculation for multiplying complex numbers doesn’t need to be memorized, as it can always be re-derived as needed. That said, if you need some intuition, notice that i^2=-1, so when you multiply bi by di, that becomes -bd.\nFields\nA field is a set of at least two distinct elements along with various operations of addition and multiplication. Examples of fields include the set of all real numbers and the set of all complex numbers.\nVector Spaces\nA vector space is a set of numbers with addition and multiplication that have the typical properties we associate with those operations. Elements of a vector space are called vectors.\nWe can define addition or scalar multiplication as the set as functions that respectively maps pairs of numbers within the set to a new numbers also within the set.\n\nReferences\npaperreadonline"},"@baiPrincipalComponentsEstimation2013":{"title":"Principal components estimation and identification of static factors","links":["@stockDynamicFactorModels2016","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Principal components estimation and identification of static factors\nJushan Bai, Serena Ng – 2013\n\n\n                  \n                  Abstract \n                  \n                \n\nIt is known that the principal component estimates of the factors and the loadings are rotations of the underlying latent factors and loadings. We study conditions under which the latent factors can be estimated asymptotically without rotation. We derive the limiting distributions for the estimated factors and factor loadings when N and T are large and make precise how identification of the factors affects inference based on factor augmented regressions. We also consider factor models with additive individual and time effects. The asymptotic analysis can be modified to analyze identification schemes not considered in this analysis. © 2013 Elsevier B.V. All rights reserved.\n\n\nAnother description of the named factor normalization covered in @stockDynamicFactorModels2016. Think I get it at this point, though I somewhat question its utility\n\nReferences\npaperreadonline"},"@bardesVICRegVarianceInvarianceCovarianceRegularization2021":{"title":"VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning","links":["Yann-LeCun","self-supervised-learning","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning\nAdrien Bardes, Jean Ponce, Yann LeCun\nAbstract\n\nRecent self-supervised methods for image representation learning maximize the agreement between embedding vectors produced by encoders fed with different views of the same image. The main challenge is to prevent a collapse in which the encoders produce constant or non-informative vectors. We introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with two regularizations terms applied to both embeddings separately: (1) a term that maintains the variance of each embedding dimension above a threshold, (2) a term that decorrelates each pair of variables. Unlike most other approaches to the same problem, VICReg does not require techniques such as: weight sharing between the branches, batch normalization, feature-wise normalization, output quantization, stop gradient, memory banks, etc., and achieves results on par with the state of the art on several downstream tasks. In addition, we show that our variance regularization term stabilizes the training of other methods and leads to performance improvements.\n\nself-supervised learning method for training embeddings from images (though could probably be applied to other modalities) where the model generates outputs from two transformed version of the input and tries to ensure that the output embeddings are similar (invariance), the individual dimensions of the embeddings vary enough within a batch (variance), and the covariances among the dimensions are minimized (covariance).\nLoss function is weighted average of the three terms, with the covariance term being constrained to the smallest weight (which makes sense given it’s units are likely large) and the other weights forced to be equal (variance and invariance “matter” equally).\nBenefits of the approach are that you don’t need contrastive examples or normalization.\n\nThe most important features of the approach are variance and invariance, without which the embeddings simply collapse. The covariance feature enhances performance to near-SOTA levels but isn’t required to get the approach at least working to some extent.\n\n\nReferences\npaperreadonline"},"@bardsenForecastingLevelsLog2011":{"title":"Forecasting levels of log variables in vector autoregressions","links":["Nassim-Taleb","tags/literature/paper","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Forecasting levels of log variables in vector autoregressions\nGunnar Bårdsen, Helmut Lütkepohl\nLink\nAbstract\n\nSometimes forecasts of the original variable are of interest, even though a variable appears in logarithms (logs) in a system of time series. In that case, converting the forecast for the log of the variable to a na¨ıve forecast of the original variable by simply applying the exponential transformation is not theoretically optimal. A simple expression for the optimal forecast under normality assumptions is derived. However, despite its theoretical advantages, the optimal forecast is shown to be inferior to the na¨ıve forecast if specification and estimation uncertainty are taken into account. Hence, in practice, using the exponential of the log forecast is preferable to using the optimal forecast.\n\nMain point here is that the common recommendation that you modify the predictions of a log-linear model when moving to log levels units may not actually be correct. Rather, you can just naively exponentiate and do just fine:\n\nthe common practice of forecasting the logs of a variable and then obtaining a forecast of the original variable by applying the exponential function is a useful strategy in practice.\n…\nfor typical economic variables, gains in forecast precision from using the optimal rather than the na ̈ıve forecast are not likely to be substantial. In fact, in practice the optimal forecast may well be inferior to the na ̈ıve forecast.\n\nThey arrive at this conclusion by first deriving a simpler form of the predicted value, then showing in simulation that you get smaller RMSE if you use the naive estimator instead of a post hoc adjustment:\n\nfor variables which have typical features of some economic variables, using the optimal forecast is likely to result in efficiency losses if the forecast precision is measured by the root mean square error (RMSE).\n\nThe standard recommended adjustment is the following, which is straightforward to derive if you assume normal forecast errors:\nE(\\exp(x)) = \\exp(\\mu + \\frac{1}{2}\\sigma^2)\nThere are three main reasons for this counterintuitive result:\n\nThe standard adjustment assume normal forecast errors, which may not be true in fact\nThe adjustment factor itself (the variance of the forecast errors) must itself be estimated, introducing error\nStationary variables that are log transformed have bounded error distributions even out to infinity, and those errors are small relative to the level of the variable, so the adjustment doesn’t do much\n\nWhile one should be careful with non-stationary variables, errors driven by misspecification / estimation error will tend to drown out everything else, making the naive forecast perform relatively well vs. the “optimal” adjusted forecast:\n\nfor integrated variables, the naıve forecasts generally perform better than the optimal forecasts, with the relative gains increasing with the forecast horizon.\n\nThis seems like a classic case where being too confident about the DGP / model leads to serious issues. Sometimes, simpler is better, especially when operating under substantial uncertainty, which we tend to be most of the time even when we don’t acknowledge it, per Nassim Taleb.\n\nReferences\npaperonline"},"@barigozziInferenceHeavyTailedNonstationary2022":{"title":"Inference in Heavy-Tailed Nonstationary Multivariate Time Series","links":["Stochastic-trend","stationarity","Fat-Tails","@onatskiSpuriousFactorAnalysis2021","Principal-component-analysis","@penaNonstationaryDynamicFactor2006","@zhangIdentifyingCointegrationEigenanalysis2019","tags/literature/paper","tags/online"],"tags":["literature/paper","online"],"content":"Inference in Heavy-Tailed Nonstationary Multivariate Time Series\nMatteo Barigozzi, Giuseppe Cavaliere, Lorenzo Trapani – 2022\n\n\n                  \n                  Abstract \n                  \n                \n\nWe study inference on the common stochastic trends in a non-stationary, N -variate time series yt, in the possible presence of heavy tails. We propose a novel methodology which does not require any knowledge or estimation of the tail index, or even knowledge as to whether certain moments (such as the variance) exist or not, and develop an estimator of the number of stochastic trends m based on the eigenvalues of the sample second moment matrix of yt. We study the rates of such eigenvalues, showing that the first m ones diverge, as the sample size T passes to infinity, at a rate faster by O (T ) than the remaining N −m ones, irrespective of the tail index. We thus exploit this eigen-gap by constructing, for each eigenvalue, a test statistic which diverges to positive infinity or drifts to zero according to whether the relevant eigenvalue belongs to the set of the first m eigenvalues or not. We then construct a randomised statistic based on this, using it as part of a sequential testing procedure, ensuring consistency of the resulting estimator of m. We also discuss an estimator of the common trends based on principal components and show that, up to a an invertible linear transformation, such estimator is consistent in the sense that the estimation error is of smaller order than the trend itself. Importantly, we present the case in which we relax the standard assumption of i.i.d. innovations, by allowing for heterogeneity of a very general form in the scale of the innovations. Finally, we develop an extension to the large dimensional case. A Monte Carlo study shows that the proposed estimator for m performs particularly well, even in samples of small size. We complete the paper by presenting two illustrative applications covering commodity prices and interest rates data.\n\n\n\n\n                  \n                  Summary \n                  \n                \n\nMethod for estimating the number of common stochastic trends in nonstationary, fat-tailed data that doesn’t require knowing the tail index.\n\n\nwww.youtube.com/watch\nThe key idea of this paper analytically is to scale the covariance matrix of the data in levels by the covariance matrix of the data in differences. Doing so ensures that eigen-analysis is scale-free and independent of the fat-tailedness of the data. The scaling is important, otherwise you’re simply in the same realm analyzed by @onatskiSpuriousFactorAnalysis2021 where the first few eigenvalues are destined to be large even if there are no true factors.\nPCA is always superconsistent in the presence of integrated processes, regardless of how many individual time series you have.\nInteresting implicit test of stationarity – if the test cannot find at least one common trend, the data is by definition stationary.\nThe one uncertainty I have about this paper is whether or not you need to standardize the variables before taking the eigenvalues. The paper doesn’t say that you have to, perhaps because multiplying by the inverse of the covariance of the differences takes care of this? Also have some questions about detrending the variables.\n\nReferences\n@penaNonstationaryDynamicFactor2006\n@zhangIdentifyingCointegrationEigenanalysis2019\n@onatskiSpuriousFactorAnalysis2021\npaperonline"},"@barigozziLargedimensionalDynamicFactor2021":{"title":"Large-dimensional Dynamic Factor Models: Estimation of Impulse–Response Functions with I ( 1 ) cointegrated factors","links":["tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Large-dimensional Dynamic Factor Models: Estimation of Impulse–Response Functions with I ( 1 ) cointegrated factors\nMatteo Barigozzi, Marco Lippi, Matteo Luciani – 2021\n\n\n                  \n                  Abstract \n                  \n                \n\nWe study a large-dimensional Dynamic Factor Model where: (i) the vector of factors Ft is I(1) and driven by a number of shocks that is smaller than the dimension of Ft ; and, (ii) the idiosyncratic components are either I(1) or I(0). Under (i), the factors Ft are cointegrated and can be modeled as a Vector Error Correction Model (VECM). Under (i) and (ii), we provide consistent estimators, as both the cross-sectional size n and the time dimension T go to infinity, for the factors, the loadings, the shocks, the coefficients of the VECM and therefore the Impulse–Response Functions (IRF) of the observed variables to the shocks. Furthermore, possible deterministic linear trends are fully accounted for, and the case of an unrestricted VAR in the levels Ft , instead of a VECM, is also studied. The finite-sample properties the proposed estimators are explored by means of a MonteCarlo exercise. Finally, we revisit two distinct and widely studied empirical applications. By correctly modeling the long-run dynamics of the factors, our results partly overturn those obtained by recent literature. Specifically, we find that: (i) oil price shocks have just a temporary effect on US real activity; and, (ii) in response to a positive news shock, the economy first experiences a significant boom, and then a milder recession.\n\n\n\n\n                  \n                  Summary \n                  \n                \n\nCommon factors among economic time series are often non-stationary and cointegrated, suggesting they should be jointly modeled with a VECM.\n\n\nImportant to keep in mind here that what’s new is the not estimator of the factor loadings (eigenvectors of covariance matrix of differed data, which is standard) but the estimator of the factors themselves, in which the loadings are applied to de-trended data rather than to differenced data. This gives you the factors in levels directly and doesn’t require cumulation at the end.\nNote that this method can potentially have issues if every series in the dataset has linear trends.\n\nReferences\npaperreadonline"},"@barnichonImpulseResponseEstimation2019":{"title":"Impulse Response Estimation by Smooth Local Projections","links":["Local-Projections","identification","@jordaEstimationInferenceImpulse2005","@plagborg-mollerEstimationSmoothImpulse","tags/online"],"tags":["literature/paper","online"],"content":"Impulse Response Estimation by Smooth Local Projections\nRegis Barnichon, Christian Brownlees\nAbstract\n\nLocal Projections (LP) is a popular methodology for the estimation of Impulse Responses (IR). Compared to the traditional VAR approach, LP allow for more flexible IR estimation by imposing weaker assumptions on the dynamics of the data. The nonparametric nature of LP comes at an efficiency cost and in practice the LP estimator may suffer from excessive variability. In this work we propose an IR estimation methodology based on B-spline smoothing called Smooth Local Projections (SLP). The SLP approach preserves the flexibility of standard LP, can substantially increase precision and is straightforward to implement. A simulation study shows that SLP can deliver substantial gains in IR estimation over LP. We illustrate our technique by studying the effects of monetary shocks where we highlight how SLP can easily incorporate commonly employed structural identification strategies.\n\nOne way to think about the large, single regression that is done in this paper is to analogize it to panel regression\n\nYou stack the variables for each of the units. Here, the “units” are the different horizons. So you end up with roughly N * H data points in a single regression rather than N data points in H different regressions. You can think about the outcome variable as y_{it}, where i is the unit and t is the time point, except here you effectively have y_{th}.\n\nSteps of smooth local projections\n\nTransform the impulse variable X from a Nx1 dimensional vector to a NxK dimensional matrix, via the transformation X @ B, where B is a vector of K values coming from K basis functions\nResidualize the outcome variable and the transformed impulse variable by the set of controls for each impulse horizon, appropriately lagged\nStack the residualized outcome variable and the NxK impulse variable by the horizon, yielding (NxH)x1 and (NxH)xK dimensional matrices for each\nRun a penalized regression, where the r-th difference between subsequent splined Xs is penalized by \\lambda. Note you can do this in one-shot via standard least squares by stacking the r-th difference matrix multiplied by \\lambda on to the X matrix. This is effectively finds the weights for the basis functions that work best across all horizons\nGet the final IRF by multiplying the matrix of basis functions (splines) by the weights vector\n\nReferences\n\nEstimation and Inference of Impulse Responses by Local Projections\nEstimation of Smooth Impulse Response Functions\n\nonline"},"@bayerLiquidityChannelFiscal2020":{"title":"The Liquidity Channel of Fiscal Policy","links":["Local-projections","time-trend","@blanchardEmpiricalCharacterizationDynamic2002","residuals","tags/literature/paper","tags/online"],"tags":["literature/paper","online"],"content":"The Liquidity Channel of Fiscal Policy\nChristian Bayer, Benjamin Born, Ralph Luetticke\nAbstract\n\nWe provide evidence that expansionary fiscal policy lowers return differences between public debt and less liquid assets—the liquidity premium. We rationalize this finding in an estimated heterogeneous-agent New-Keynesian model with incomplete markets and portfolio choice, in which public debt affects private liquidity. This liquidity channel stabilizes fixed-capital investment. We then quantify the long-run effects of higher public debt and find little crowding out of capital, but a sizable decline of the liquidity premium, which increases the fiscal burden of debt. We show that the revenue-maximizing level of public debt is positive and has increased to 60 percent of GDP post-2010.\n\nLocal projection specification:\nx_{t+h}=\\beta_0 + \\beta_1t + \\beta_2t^2 + \\psi_h\\log{g_t} + \\Gamma(L)Z_{t-1} + u_{t+h}\nZ includes vector of controls, including four lags of everything. Note the use of a quadratic time trend.\nMakes the point that including lags means that the variable in question represents a shock and is therefore plausibly causal in its influence, a point also made by @blanchardEmpiricalCharacterizationDynamic2002:\n\n“Under the Blanchard and Perotti (2002)-predeterminedness assumption, the coefficient \\psi_h provides a direct estimate of the impulse response at horizon h to the government spending shock in t.” (Bayer et al., 2020, p. 6) (pdf)\n“This is equivalent to a two-step approach, where g_t is first regressed on lags of itself and additional covariates and the residual is then included in step 2 as the shock measure.” (Bayer et al., 2020, p. 6) (pdf)\n\nThis work uses levels of all the variables along with their lags and seems to arrive at sensible results, which I’ve generally found to be harder to achieve:\nTransclude of Local-projections#c91898\n\n\n\nReferences\npaperonline"},"@budaShortVariableLags2023":{"title":"Short and Variable Lags","links":["Impulse-responses","local-projections","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Short and Variable Lags\nGergely Buda, Vasco M. Carvalho, Giancarlo Corsetti, João B. Duarte, Stephen Hansen, Álvaro Ortiz, Tomasa Rodrigo, José V. Rodríguez Mora – 2023\n\n\n                  \n                  Abstract \n                  \n                \n\nWe study the transmission of monetary policy shocks using daily consumption, corporate sales and employment series. We find that the economy responds at both short and long lags that are variable in economically significant ways. Consumption reacts in one week, reaches a local trough in one quarter, recovers, and declines again after three quarters. Sales follow a similar pattern, but the initial drop, while delayed (one month), is deeper. In contrast, employment falls monotonically for five quarters albeit with a smaller impact reaction. We show that these short lags are masked by time aggregation at lower —quarterly— frequencies.\n\n\n\n\n                  \n                  Summary \n                  \n                \n\nTK\n\n\nDemonstrates cool way to calculate level impulse responses from Y/Y growth IRFs.\nThere’s a lot of smoothing/filtering here – they use a 90-day moving average on the daily data, and then in addition use y/y growth rates in the local projections. So in effect you have extremely smoothed data, which might be necessary given the inherent noisiness of daily data. Something to potentially consider for my purposes as an alternative to LOESS.\n\nReferences\npaperreadonline"},"@canovaFAQHowExtract":{"title":"FAQ: How do I extract the output gap?","links":["Lindy-effect","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"FAQ: How do I extract the output gap?\n**Fabio Canova – **\n\n\n                  \n                  Abstract \n                  \n                \n\nI investigate the properties of gaps and potentials in a variety of DSGE models and their relationship with estimates obtained with standard approaches. Gaps display low frequency variations, have similar frequency domain representation as potentials, and are correlated with them. Transitory and permanent fluctuations display similar features, but are uncorrelated. I use a number of procedures to estimate the latent components. All approaches generate distortions. Gaps are best estimated with a polynomial filter; transitory fluctuations with a differencing approach. Explanations for the outcomes are given. I design a procedure that reduces the biases of existing methods.\n\n\n\n\n                  \n                  Summary \n                  \n                \n\nTK\n\n\nyoutu.be/nWeL1lKugik\nTypical statistical approaches for extracting or removing trends make several assumptions that don’t necessarily hold in the underlying DGP:\n\nTrend isn’t necessarily uncorrelated with cycle\nCycles are not necessarily transitory\nTrue cycles do not necessarily happen at business cycle frequencies\n\nPolynomial detrending / filtering is Lindy: it is the oldest procedure and arguably one of the best.\n”Gap” isn’t a meaningful concept without some sort of structural theory for “potential”. Given that I mostly avoid deep theoretical models in my work, I should be careful about any suggestion of “potential” in venture capital data.\n\nReferences\npaperreadonline"},"@cochraneHowBigRandom1988":{"title":"How Big Is the Random Walk in GNP?","links":["random-walk","stationarity","Impulse-responses","Don't-Discount-Interest-Rates","Random-walk","Beats-and-Misses-Are-Forever","RBC","@cochranePermanentTransitoryComponents1994","@hodrickExplorationTrendCycleDecomposition2020","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"How Big Is the Random Walk in GNP?\nJohn H. Cochrane\nLink\n\n\n                  \n                  Abstract\n                  \n                \n\nThis paper presents a measure of the persistence of fluctuations in GNP based on the variance of its long differences. That measure finds little long-term persistence in GNP. Previous research on this question found a great deal of persistence in GNP, suggesting models such as a random walk. A reconciliation of this paper’s results with previous research shows that conventional criteria for time series model building can produce misleading estimates of persistence.\n\n\nIt’s very difficult to distinguish between permanent shocks and merely transitory shocks with high persistence.\n\nThis paper reexamines the long-run properties of GNP and argues that GNP does, in fact, revert toward a “trend” following a shock. However, that reversion occurs over a time horizon characteristic of business cycles-several years at least. Therefore, the short-run properties of GNP are consistent with a model with very persistent shocks,\n\nFor stationary time series, long-term forecasts do not change in response to shocks. In other words, the impulse response function should fade to zero over a long enough horizon. This seems to be roughly the case for my analysis in Don’t Discount Interest Rates.\n\nIf the variance of the shocks to the random walk component is zero, the series is trend-stationary, and long-term forecasts do not change in response to shocks. If the variance of the shocks to the random walk component is equal to the variance of first differences, the series is a pure random walk.\n\nFor a time series with permanent fluctuations, like a random walk, lower values today imply lower forecasts out into the indefinite future. This seems to be what I found in Beats and Misses Are Forever – lower revenue today forecasts lower revenue one, two, and three years out.\n\nFluctuations in a random walk are permanent in the following sense: suppose that \\epsilon_t = - 1, so that y_t falls one unit below last period’s expected value. Then, since y_{t+j} = y_t + j\\mu + \\epsilon_{t+1} +  . . . + \\epsilon_{t+p} forecasts E_t(y_{t+j}) fall by one unit for the indefinite future. Also, a low or negative growth rate today implies nothing about growth rates in the future, and there is no tendency for future levels of GNP to revert to a trend line.\n\n\nHow much does a one-unit shock to GNP affect forecasts in the far future? If by one unit, it finds a random walk; if by zero, it finds a trend-stationary process like (1). It can also find numbers between zero and one, characterizing a series that returns toward a “trend” in the far future,\n\n\nThe size of the random walk component seems to have implications for the plausibility of various economic models. In particular, if the economy has a large random walk component, that is favorable of RBC style models of the economy, where fluctuations originate from real productivity shocks. If the random walk component is small or non-existant, that militates in favor of Keynesian or monetary theories of the business cycle, where the economy fluctuates around an exogenous trend due to monetary or fiscal policy.\n\nThe size of a random walk in GNP has been cast as a direct test between competing models of the economy. For example, Nelson and Plosser (1982) interpreted their result that GNP has a large random walk component as evidence for stochastic equilibrium models over traditional monetary or Keynesian business cycle models. They argued that traditional models produce only temporary deviations from trend, while models that find the ultimate source of GNP variability in technology shocks can produce permanent fluctuations.\n\n\nThis suggests that software companies are better modeled with an RBC-like approach, which is what I would have guessed (refer to Beats and Misses Are Forever)\nVenture capital seems to be better modeled by a Keynesian or monetarist approach, which is also quite intuitive given the whole premise of Don’t Discount Interest Rates is exactly that monetary policy has a large effect on venture activity\n\n\nReferences\nPermanent and Transitory Components of GNP and Stock Prices\nAn Exploration of Trend-Cycle Decomposition Methodologies in Simulated Data\npaperreadonline"},"@crumpSparseTrendEstimation2023":{"title":"Sparse Trend Estimation","links":["@phillipsBoostingWhyYou2021","@parkBayesianLasso2008","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Sparse Trend Estimation\nRichard K. Crump, Nikolay Gospodinov, Hunter Wieman – 2023\n\n\n                  \n                  Abstract \n                  \n                \n\nThe low-frequency movements of many economic variables play a prominent role in policy analysis and decision-making. We develop a robust estimation approach for these slow-moving trend processes which is guided by a judicious choice of priors and is characterized by sparsity. We present some novel stylized facts from longer-run survey expectations that inform the structure of the estimation procedure. The general version of the proposed Bayesian estimator with a slab-and-spike prior accounts explicitly for cyclical dynamics. The practical implementation of the method is discussed in detail and we show that it performs well in simulations against some relevant benchmarks. We report empirical estimates of trend growth for U.S. output (and its components), productivity and annual mean temperature. These estimates allow policy makers to assess shortfalls and overshoots in these variables from their economic and ecological targets.\n\n\nThe authors examine the evolution of long-horizon forecasts of the US economy and note that these forecast change quite slowly over time. Thus the first differences of these forecasts are usually zero, with rarer deviations in either direction when the forecast does change. The same is true of the second differences of these forecasts. These distributions don’t appear to be consistent with continuous distributions like the normal distribution. They are more consistent with mixture distributions.\nThis feels like a relatively “doable” paper to replicate\nThe March 2024 revision of the paper is definitely cleaner and easier to understand, though I’m biased from having read the first version of the paper.\nOne thing to notice is that though they omit any multivariate designation for the normals that define the distributions of the observed data and trend, you can tell they are multivariate by the fact that they are parameterized with vectors for the means and matrices for the variance.\nReplication\nSome interesting reflections after attempting to replicate the paper multiple times, with the most success this was recent attempt:\n\nWhen people say that a Laplace distribution is a scale mixture of normals with exponential mixing density, what they literally mean is that the variances are exponentially distributed, meaning they get less frequent the larger they are. How the infrequency scales with the variances depends on the \\lambda parameter, i.e. the variance of the original Laplace. The higher the variance of the original Laplace, the lower the \\lambda, the higher variance the exponential distribution, the less high variance normals are effectively “penalized”.\nYou are effectively sampling various normals from a single exponential distribution. It’s not that there are multiple different exponentials – there is only one, characterized by \\lambda, from which you sample all the variances, i.e. all the normals. It just so happens that when you sample a bunch of numbers from an exponential, call those things “variances” and use them to parameterize a bunch of normals centered around the same mean, you get a Laplace. That just happens by definition, that’s just the math. The rarity of higher variances comes for free via the exponential distribution. When combined with those relative frequencies, you get a Laplace.\nThe normals are integrated over to get the final Laplace distribution. The exact way this is done is fairly complicated in its full form, but it’s basically integrating over the normals conditional on their variances, then integrating over the variances themselves. So if the normals are f(x|s)  and the exponentials are p(s|a) where s is the variance of the normals and a is the scale parameter of the exponentials, then the integral for the density is \\int_0^\\infty f(x|s)p(s|a)ds which evaluates out to the Laplace density \\frac{a}{2}e^{-a|x|}\nYou need to use a mixture to represent the spike and slab density, since the whole point is that a single parameterized Laplace distribution can’t accommodate the stylized facts we see in the data – lots of zeros and a small number of larger magnitude changes. This was originally confusing because I thought there was only one mixture, the one generating the Laplace if you use the hierarchical setup. But no, you need to setup a mixture, because although there are “2 Laplaces” you are only pulling a single sample for each time period. It’s not a separate sample for each Laplace for each time period. The only way to set that up is via a mixture distribution, which gives you one effective distribution. Now, because you use a Bernoulli to pick between them, any given time period is only effectively using one Laplace or the other.\nI’m still not sure if its critical to use the mixture of normals approach. My sense is that they use that so that they can use a Gibbs sampler, but I don’t know enough about Gibbs sampling to know if I’m right on that. For my purposes it’s much simpler to just use Laplace distributions directly, though now with my new understanding maybe it wouldn’t be so bad to rewrite using normals and exponentials… I need to make sure that the “scale mixture of normals” can be represented in PyMC using the NormalMixture function or if there’s some subtle distinction there. I think the weights in that function are different than what comes out of an exponential distribution. I think the weights for the NormalMixture should actually be the relative frequency of the variances you are sampling from the exponential, not the variances themselves which is what I was previously doing.\nSomething seems to just completely blow up when you expand the sample size. The time to take to run the routine grows dramatically if you add just 5 more years of data, something like 50x. Strange.\n\n\nReferences\n@phillipsBoostingWhyYou2021\nThe king must die\n\nReally good discussion of why The Bayesian Lasso is not very good in practice\nSimilar to why this paper even exists, the Bayesian Lasso can’t properly balance between big and small values, so it ends up trying to compromise which shrinks the larger values too much and doesn’t shrink the small values enough\nThe core reason for this is that a single Laplace prior distribution doesn’t place enough probability mass near the spike at zero relative to the mass in the tails to ensure that you get a reasonable level of sparsity\n\npaperreadonline"},"@dubeLocalProjectionsApproach":{"title":"A Local Projections Approach to Difference-in-Differences Event Studies","links":["Difference-in-Differences","difference-in-differences","Local-projections","Fixed-effects","@hoyosTariffsGrowthHeterogeneous","tags/literature/paper","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"A Local Projections Approach to Difference-in-Differences Event Studies\nArindrajit Dube, Daniele Girardi, Alan M Taylor\nLink\nVideo\nVideo\nAbstract\n\nRecent applied microeconometrics research proposes various difference-in-differences (DiD) estimators for the problem of dynamic heterogeneous treatment effects. We show that the problem can be resolved by the local projection (LP) estimators of the sort used in applied macroeconometrics. Our proposed LP-DiD estimator provides an overarching toolkit with several advantages. First, the method is clear, simple, easy to compute, and transparent and flexible in its handling of treated and control units. Second, it is quite general, including its ability to control for pre-treatment values of the outcome and of other covariates, as under conditional common trends. Third, the LP-DiD can nest other estimators, providing a framework that is not only rigorous but also encompassing. The LP-DiD estimator does not suffer from the negative weighting problem, and indeed can be implemented with any weighting scheme the investigator desires. Simulations demonstrate the good performance of the LP-DiD estimator in common settings. Two empirical applications illustrate how LP-DiD addresses the bias of conventional fixed effects estimators, leading to potentially different results.\n\nThe LP-DiD estimator is implemented via OLS via either sample restriction or including interactions against whether or not a particular unit is an unclean control at each point in time. Unclean controls are those units which are treated at some point in time other than the current period.\nThe LP-DiD estimator regresses future outcomes on a \\Deltatreatment indicator, \\Deltaoutcome lags,  \\Delta covariate lags, and time fixed effects. When the full sample is used, lagged outcomes, covariates, and time fixed effects are interacted with the unclean control indicator and the indicator is additional included as a regressor.\nMakes the note referred to in @hoyosTariffsGrowthHeterogeneous that you don’t need unit fixed effects for panel local projections with long differenced outcome variables:\n\nIn applications of the local projections estimator, it is common to employ the long difference \\Delta_ky_{it} ≡ y_{i,t+k} – y_{i,t–1} on the left side of the estimating equation, especially when y_{it} is expressed in logs. A reason is that 100 \\times \\beta_k can then be interpreted as an approximate percentage change in the outcome at time t + k due to treatment at time t, facilitating interpretation of effect sizes. ==This transformation also has the advantage of mechanically removing unit-specific fixed effects.==\n\nThe dropping of unclean controls doesn’t lead to as much data loss as you might think. It’s not that those future periods aren’t included. They are included as the dependent variable in the samples where the treatment originally happened.\n\n\nReferences\npaperonline"},"@fernaldDisappointingRecoveryOutput2017":{"title":"The Disappointing Recovery of Output after 2009","links":["@hodrickExplorationTrendCycleDecomposition2020","@hallWhyHasUS","stationarity","Dynamic-factor-model","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"The Disappointing Recovery of Output after 2009\nJohn Fernald, Robert Hall, James Stock, Mark Watson\nLink\nAbstract\n\nU.S. output has expanded only slowly since the recession trough in 2009, even though the unemployment rate has essentially returned to a precrisis, normal level. We use a growth-accounting decomposition to explore explanations for the output shortfall, giving full treatment to cyclical effects that, given the depth of the recession, should have implied unusually fast growth. We find that the growth shortfall has almost entirely reflected two factors: the slow growth of total factor productivity, and the decline in labor force participation. Both factors reflect powerful adverse forces that are largely unrelated to the financial crisis and recession—and that were in play before the recession.\n\nSome weird arbitrary choices in this paper but per @hodrickExplorationTrendCycleDecomposition2020 this is supposedly an example of “gold standard” econometric research, so I’ll try to follow the logic. I do think the use of unemployment to measure the state of the cycle is interesting and potentially applicable to other domains.\nNote that they are decomposing the growth rate itself, not the level of the series.\nWhen measuring growth rates they use the annualized Q/Q growth rate, which I’m not used to seeing but perhaps worth thinking about for my purposes. I’d imagine that this would tend to be highly seasonal (EDIT: Likely using seasonally adjusted data so this point is moot). Seems like the key to this is that changes in unemployment period to period have an expected value of zero. Thus a cycle that is a linear combination of various leads and lags of the change in unemployment will also have a zero expected value, though it might fluctuate up and down as we’d hope a cycle would.\nUnemployment recovers fairly consistently across recessions, as per @hallWhyHasUS, but output seemed to recover much more slowly in the financial crisis than in prior recessions. This paper posits that the reason lies in the trend level of GDP having slowed substantially during this time, such that the contemporaneous output gap was less pronounced than commonly believed.\nKey assumption is that the capital-output ratio should be stationary and reasonably stable over time. Thus increases and decreases can be interpreted as strong signals of either capital surplus and shortfall relative to the “ideal” level.\nGood example of a DFM, estimated on detrended growth rates of various economic variables. Here the DFM is used to forecast the cyclical component of each series, while the trend in each series is assumed to be constant at wherever it ended at the 2009 trough:\n\nThe 123 series are transformed into growth rates (for activity variables; see the online appendix for the details of other series); low-frequency trends are extracted, as discussed above; and six factors are then estimated using principal components.\n…\nIn the notation of equation 6, the factor model forecast of y_t is the sum of the trend projection μt and the projection of c_t computed using the detrended factors. Thus, the forecast error is an estimate of the irregular part z_t; subtracting this forecast error measures the growth shortfall of y_t.\n\n\n\nStandard deviation of GDP components  \n\n\nReferences\npaperreadonline"},"@gorodnichenkoForecastErrorVariance2020":{"title":"Forecast Error Variance Decompositions with Local Projections","links":["Local-projections","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Forecast Error Variance Decompositions with Local Projections\nYuriy Gorodnichenko, Byoungchan Lee\nLink\nAbstract\n\nWe propose and study properties of an estimator of the forecast error variance decomposition in the local projections framework. We find for empirically relevant sample sizes that, after being bias-corrected with bootstrap, our estimator performs well in simulations. We also illustrate the workings of our estimator empirically for monetary policy and productivity shocks.\n\nA simple method for estimating forecast error decompositions for local projections.\n:\n\nCalculate forecast errors for the endogenous variable of interest at each horizon h based on the information set at t-1 as the residuals of a regression of the change in the endogenous variable from t-1 \\rightarrow h  on lags of all variables\nFor each horizon h, regress the forecast errors at that horizon onto the shock innovations up through that horizon\nThe R^2 of those regressions is the forecast error variance due to the shock\n\n\nThis quantity can be understood as an R2 of the population projection of ft+h|t−1 on Zh t , or the probability limit of sample R2’s. This observation suggests a natural estimator of sh. First, the forecast errors for each horizon h are estimated using local projections. Second, the estimated forecast errors for the horizon h at time t are regressed on shocks that happen between t and t + h.The R2 in this regression is an estimate of sh.\n\nThe forecast errors can be interpreted as the change in the endogenous variable that couldn’t have be forecasted based on available information. The size (variance) of these errors will of course tend to grow with the horizon. Some portion of this variance is due to the shock variable, while the rest could be due to any number of factors (including direct shocks to the endogenous variable itself).\nIt might be easiest to simply include the future values of the shock of interest when running the forecast error regression above and see how much of a difference they make to the R-squared, in other words, identify the partial R^2 of those shocks.\n\nNote that one may implement this estimator by augmenting Equation (5) with shocks z_t, …, z_{t+h} and calculating the partial R^2.\n\n\nReferences\npaperreadonline"},"@gouletcoulombeNeuralPhillipsCurve2022":{"title":"A Neural Phillips Curve and a Deep Output Gap","links":["tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"A Neural Phillips Curve and a Deep Output Gap\nPhilippe Goulet Coulombe\nLink\nVideo\nAbstract\n\nMany problems plague the estimation of Phillips curves. Among them is the hurdle that the two key components, inflation expectations and the output gap, are both unobserved. Traditional remedies include creating reasonable proxies for the notable absentees or extracting them via some form of assumptions-heavy filtering procedure. I propose an alternative route: a Hemisphere Neural Network (HNN) whose peculiar architecture yields a final layer where components can be interpreted as latent states within a Neural Phillips Curve. There are benefits. First, HNN conducts the supervised estimation of nonlinearities that arise when translating a high-dimensional set of observed regressors into latent states. Second, computations are fast. Third, forecasts are economically interpretable. Fourth, inflation volatility can also be predicted by merely adding a hemisphere to the model. Among other findings, the contribution of real activity to inflation appears severely underestimated in traditional econometric specifications. Also, HNN captures out-of-sample the 2021 upswing in inflation and attributes it first to an abrupt and sizable disanchoring of the expectations component, followed by a wildly positive gap starting from late 2020. HNN’s gap unique path comes from dispensing with unemployment and GDP in favor of an amalgam of nonlinearly processed alternative tightness indicators – some of which are skyrocketing as of early 2022.\n\nHard to say how interesting this is without diving into the architecture. At a high level doesn’t sound particularly innovative. It’s basically a GAM where each component is a neural net. Could be wrong though, I haven’t read the paper, just watched the video above.\nThe separation of the network into different “hemispheres” is really what gives the model structure and allows you to interpret the various components. If you don’t do this then the output neurons would be effectively meaningless since they would each combine information from various inputs in complicated, black-box ways. Instead, by separating the pieces you end up with separate components which map to particular input information. Adding these components in simple linear fashion is also a form of structure, ensuring that their contributions remains linearly additive and separable.\n\nReferences\npaperreadonline"},"@greenwaldHowWealthWas":{"title":"How the Wealth Was Won: Factor Shares as Market Fundamentals","links":["Vector-autoregression","reduced-form","shock","Autoregressive-models","random-walk","unit-root","The-Universal-Law-of-SaaS-Growth","Backing-into-ARR","@greenwaldOriginsStockMarket2014","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"How the Wealth Was Won: Factor Shares as Market Fundamentals\nDaniel L Greenwald, MIT Sloan, Martin Lettau, Sydney C Ludvigson\nLink\nVideo\nVideo\n\n\n                  \n                  Abstract \n                  \n                \n\nWhy does the stock market rise and fall? From 1989 to 2017, the real per-capita value of corporate equity increased at a 7.5% annual rate. We estimate that 44% of this increase was attributable to a reallocation of rewards to shareholders in a decelerating economy, primarily at the expense of labor compensation. Economic growth accounted for just 25% of the increase, followed by a lower risk price (18%), and lower interest rates (14%). The period 1952 to 1988 experienced less than one third of the growth in market equity, but economic growth accounted for more than 100% of it.\n\n\nThe related work section of this paper is a good discussion of some of the different trade-offs of different methods within the context of economic modeling. On the one hand, VAR models are highly flexible but entirely statistical, thus lacking economic content outside of the rotation of the reduced form residuals to get the structural shocks. On the other hand, structural modeling is less flexible but add economic theory, aiding interpretation of various drivers and forces. While it’s not discussed directly in the paper, I would guess that an additional benefit of structural modeling is that it more easily let’s you work with non-stationary variables, which are difficult to deal with directly with purely statistical approaches for obvious reasons.\nIf I want to advance my ability to make economically meaningful claims within my essays, all roads seems to eventually lead to structural modeling.\n\n\n                  \n                  Quote\n                  \n                \n\nOur work also relates to the literature estimating log-affine SDFs in reduced form.12 These studies describe the evolution of the state variables and the SDF in purely statistical terms, for example using an estimated vector autoregression (VAR) for state dynamics. While less statistically flexible, our work features more economic structure, using separate and mutually uncorrelated fundamental components, as well as parametric restrictions on the SDF exposures obtained from theory, such as the leverage risk effect. This structure allows a much clearer interpretation of the drivers of asset prices. For example, unlike VAR-based models, which face the difficult task of transforming reduced-form residuals into identified structural shocks, our model allows us to directly read off the contribution of each latent state. We thus complement this literature by providing economic insight on the economic sources of market fluctuations, particularly the role of factor shares.\n\n\nHas a kind of cool, natural definition of operating leverage nested within their model – whenever one line item grows faster than another in response to the same core impulse, you can think of that as operating leverage. As with debt, which causes returns to grow faster than the thing that is driving the returns, operating leverage means your outputs grow faster than your inputs. Here, the input is a change in the earnings share and the output is a change in payouts to shareholders as a proportion of output (revenue).\n\n\n                  \n                  Quote\n                  \n                \n\nImportantly, (3) implies that the volatility of cash flow growth is amplified relative to earnings share growth — a form of operating leverage. For example, if ω = 6%, then an increase in the earnings share St from 12% to 18% increases the cash flow share from 6% to 12%. As a result, proportional growth in the cash flow share (100%) is twice as large as in the earnings share (50%), a phenomenon that we call the leverage effect. We note that this leverage effect should hold on average even if the reinvestment share is not exactly constant, so long as investment at long horizons is proportional to output rather than earnings.\n\n\nInteresting to think about variables having low frequency and high frequency components, where the main distinguishes characteristic is the persistence of the components. In a simple AR(1) autoregressive setup, this implies that the coefficient on the lag is higher for the low frequency component than the high frequency. In the most extreme case, the low frequency component could be model as a “permanent component” that evolves as a random walk (unit root) whereas the high frequency component could be modeled as a “transitory” white noise process. (Relevant to my work on The Universal Law of SaaS Growth, Backing into ARR)\n\n\n                  \n                  Quote\n                  \n                \n\nWe choose a two-component mixture for each process to allow the model to flexibly capture both high and low frequency variation in the latent states. Since equity gives its owners access to profits for the lifetime of the firm, it is a heavily forward-looking asset that is much more influenced by persistent rather than transitory fluctuations. Our mixture specification allows the model to accurately capture both low frequency movements that have greater impact on equity prices, as well as higher frequency movements that have a smaller impact on equity prices but may nonetheless drive much of the variation in the observable series. Correspondingly, we refer to the components of each latent state vector as the high or low frequency component\n\n\n\nReferences\n@greenwaldOriginsStockMarket2014\npaperreadonline"},"@greenwaldOriginsStockMarket2014":{"title":"Origins of Stock Market Fluctuations","links":["Vector-autoregression","Moving-average-model","Don't-Discount-Interest-Rates","tags/literature/paper","tags/online"],"tags":["literature/paper","online"],"content":"Origins of Stock Market Fluctuations\nDaniel Greenwald, Martin Lettau, Sydney Ludvigson\nLink\n\n\n                  \n                  Abstract \n                  \n                \n\nThree mutually uncorrelated economic disturbances that we measure empirically explain 85% of the quarterly variation in real stock market wealth since 1952. A model is employed to interpret these disturbances in terms of three latent primitive shocks. In the short run, shocks that affect the willingness to bear risk independently of macroeconomic fundamentals explain most of the variation in the market. In the long run, the market is profoundly affected by shocks that reallocate the rewards of a given level of production between workers and shareholders. Productivity shocks play a small role in historical stock market fluctuations at all horizons.\n\n\n\n\n                  \n                  Summary \n                  \n                \n\nOverall a very interesting paper showing that most of the growth in the detrended value of the stock market over time is attributable to changes in the labor share. Important to my own efforts, demonstrates a decomposition of a non-stationary series into components attributable to various orthogonal shocks using a Cholesky decomposition within a VAR approach. Also includes some helpful discussion of the moving average representation of the VAR.\n\n\nThis uses a very similar methodology to the one I use in Don’t Discount Interest Rates to decompose the level of stock market wealth into various drivers. Notably, the authors argue for using separate regressions when regressing the outcome variable on the shocks, using a different regression for each shock rather than stuffing them all into the same regression. This works since all the shock are uncorrelated by assumption. This is obviously an important assumption, but clearly it can be rigorously justified.\n\nSince e_{c,t}, e_{y,t} and e_{a,t} are mutually uncorrelated and i.i.d., we estimate these equations separately by OLS with L = 16 quarters.\n\nThey do the same thing I do where I get the cumulative decomposition by summing up the effects on the first differences.\n\nThe effect on the log levels of stock wealth of each disturbance is obtained by summing up the effects on the log differences\n\nThe one area where they do differ is that they include a deterministic linear trend in stock market wealth (edit: not sure this is actually different from my approach, it’s implicit since the underlying regressions are on first differences), which of course soaks up some of the non-stationarity and leaves less room to be explained by the shocks. However, it seems in reporting their results they report the explained variance of the detrended series, so it’s still possible for the shocks to “fully explain” stock market wealth over time. This feels OK – it makes sense in the context of the stock market to have a deterministic term since there does seems to be a strong force driving the market up over time that’s probably out of scope of this paper to explain.\nOne interesting chart was one showing the detrended stock market value against the cumulated factor share shock. They move very closely together, suggesting a tight relationship. Perhaps interesting for Don’t Discount Interest Rates, though I have a feeling this kind of chart would be tricky to explain to folks (cumulated shock? detrended stock market?): \n\nReferences\npaperonline"},"@haddadHowCompetitiveStock2021":{"title":"How Competitive is the Stock Market? Theory, Evidence from Portfolios, and Implications for the Rise of Passive Investing","links":["@huStatisticalArbitrageUncertain","@koijenDemandSystemApproach","tags/online"],"tags":["literature","inbox/read","online"],"content":"How Competitive is the Stock Market? Theory, Evidence from Portfolios, and Implications for the Rise of Passive Investing\nValentin Haddad, Paul Huebner, Erik Loualiche\nVideo\nAbstract\n\nWe develop a framework to theoretically and empirically analyze investor competition on financial markets. The classic view assumes that markets are very competitive: if a group of investors changes its behavior, other investors react such that nothing happens in equilibrium. Our framework quantifies the strength of the competitive response. We estimate a demand system of institutional investors in the US stock market accounting for two layers of equilibrium: how investors compete with each other in setting their strategies and how prices adjust to clear asset markets. We find that investors react to the behavior of others in the market: when an investor is surrounded by less aggressive traders she trades more aggressively. This reaction reduces the equilibrium consequences of changes in individual behavior by 50%. However, it also implies that the stock market is far from the competitive ideal. A consequence of this result is that the large increase in passive investing over the last 20 years has led to substantially more inelastic aggregate demand curves for individual stocks, by 15%.\n\nHighlights\nEach investor demands some amounts of an asset, decreasing in the price of the asset by the elasticity:\nd_i = \\underline{d}_i - \\mathcal{E}_i \\times p\nIn equilibrium, demand sums up to supply:\n\\int_{i} D_{i}(p) = S\nElasticity is determined by individual effect and response to aggregate demand elasticity:\n\\mathcal{E}_i = \\underline{\\mathcal{E}_i} - \\chi \\times \\mathcal{E}_{agg}\nIn equilibrium, aggregate elasticity is equal to average investor elasticity (weighted by demand):\n\\int_i \\mathcal{E}_i D_i / S = \\mathcal{E}_{agg}\nCan think about \\chi as representing competition. If \\chi = 0, there is no competition — each investor follows their own strategy totally independent of the rest of the market. In perfect competition, \\chi = \\infty, and any change is completely counteracted by investor reaction. Related to Statistical Arbitrage with Uncertain Fat Tails\nLarge increase in passive investors over time. In no competitive world, this lead to proportional reduction in elasticity. In perfectly competitive market, increase in passive investors has no effect on elasticity.\nAuthors estimate that \\chi = 1.7, relatively stable over time.\nFollow similar methodology as A Demand System Approach to Asset Pricing, with logit specification for portfolio shares w_{ik}. Track portfolio shares relative to price as a measure of demand, modeled as investor-specific function of observables and investor-specific elasticity:\n\\log{\\frac{w_{ik}}{w_{i0}} - p_k} = \\underline{d}_{0i} + \\underline{d}&#039;_{1i}X_k - \\mathcal{E}_{ik}p_k + \\epsilon_{ik}\nInvestor-specific elasticity modeled as investor-specific function of observables and market elasticity:\n\\mathcal{E}_{ik} = \\underline{\\mathcal{E}}_{0i} + \\underline{\\mathcal{E}}&#039;_{1i}X_k - \\chi \\mathcal{E}_{agg,k}\nInvestors respond less to price movements for assets with more aggressive investors than for assets with less aggressive investors. If all other investors are more elastic by 1, lower my elasticity by 1.7.\nElasticities in general are low — this study finds 0.3. Price elasticity in the stock market is lower for large companies, approaching zero for the largest stocks:\n\n\nonline"},"@hamiltonPrincipalComponentAnalysis":{"title":"Principal Component Analysis for Nonstationary Series","links":["Principal-component-analysis","stationarity","Spurious-correlation","OLS","Hamilton-filter","@hamiltonWhyYouShould2017","data-generating-process","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Principal Component Analysis for Nonstationary Series\nJames D Hamilton, Jin Xi\nLink\nAbstract\n\nThis paper develops a procedure for uncovering the common cyclical factors that drive a mix of stationary and nonstationary variables. The method does not require knowing which variables are nonstationary or the nature of the nonstationarity. Applications to the term structure of interest rates and to the FRED-MD macroeconomic dataset demonstrate that the approach offers similar benefits to those of traditional principal component analysis with some added advantages.\n\nMethod for conducting Principal component analysis for non-stationary data.\nAvoid standard, static PCA when working with non-stationary data, as there’s a tendency for the factors that emerge to be spurious: they won’t represent the true factor structure if there is one, and they’ll tend to invent factors where none exist like how OLS generates correlations in sample where none exist in population. This happens for the same reasons as does spurious regression with time series data:\n\nTrending data isn’t stationary.\nData that isn’t stationary doesn’t have a defined mean or variance.\nThis violates the assumption of standard estimators like OLS and destroys any guarantee of correctness for the results.\nLike OLS, PCA is a least squares estimator and thus inherits similar strengths and weaknesses.\n\n\nFor a nonstationary variable, the population mean is undefined and the sample standard deviation diverges to infinity as the number of time-series observations gets large.\n\nFocusing on the cyclical, non-trending portion of a series mitigates these issues. This requires a filter that can extract this component. There’s no shortage of filters, but an easy one proposed by the same author of this method is the Hamilton filter (Why You Should Never Use the Hodrick-Prescott Filter):\n\nThe Hamilton filter extracts a stationary cyclical component from a diverse array of time series DGPs\nIt’s simple to use, only requiring running OLS on each series controlling for lagged values of the variable (with at least some delay) and extracting the residuals from this regression, which represent the cyclical component\nThis now stationary data form the input to PCA, which will yield factors which are themselves stationary and non-spurious\n\nSome interesting notes:\n\nUse log transformation (\\log X_t) for most variables, e.g. ones you would normally first difference. If a variable has negative values, use X_t / X_{t-1} instead\nUse lags that add up to a year’s worth of observations to account for seasonality\n\n\nReferences\npaperreadonline"},"@hodrickExplorationTrendCycleDecomposition2020":{"title":"An Exploration of Trend-Cycle Decomposition Methodologies in Simulated Data","links":["Robert-Hodrick","Hodrick-Prescott-Filter","Random-walk","stationarity","Stochastic-trend","Hamilton-filter","Autoregressive-models","@hamiltonWhyYouShould2017","@fernaldDisappointingRecoveryOutput2017","@cochraneHowBigRandom1988","tags/literature/paper","tags/inbox/process","tags/online"],"tags":["literature/paper","inbox/read","inbox/process","online"],"content":"An Exploration of Trend-Cycle Decomposition Methodologies in Simulated Data\nRobert Hodrick\nAbstract\n\nThis paper uses simulations to explore the properties of the HP filter of Hodrick and Prescott (1997), the BK filter of Baxter and King (1999), and the H filter of Hamilton (2018) that are designed to decompose a univariate time series into trend and cyclical components. Each simulated time series approximates the natural logarithms of U.S. Real GDP, and they are a random walk, an ARIMA model, two unobserved components models, and models with slowly changing nonstationary stochastic trends and definitive cyclical components. In basic time series, the H filter dominates the HP and BK filters in more closely characterizing the underlying framework, but in more complex models, the reverse is true.\n\nMakes the point that the HP filter tends to dominate the Hamilton filter when dealing with data that doesn’t follow relatively simple time series models like random walk or autoregressive processes.\n\nReferences\nWhy You Should Never Use the Hodrick-Prescott Filter\nThe Disappointing Recovery of Output after 2009\nHow Big Is the Random Walk in GNP?\npaperprocessonline"},"@hoyosTariffsGrowthHeterogeneous":{"title":"Tariffs and Growth: Heterogeneous Eﬀects by Economic Structure","links":["@dubeLocalProjectionsApproach","The-Universal-Law-of-SaaS-Growth","local-projections","Difference-in-Differences","selection-bias","fixed-effects","Local-projections","Impulse-responses","tags/literature/paper","tags/inbox/process","tags/online"],"tags":["literature/paper","inbox/read","inbox/process","online"],"content":"Tariffs and Growth: Heterogeneous Eﬀects by Economic Structure\nMateo Hoyos\nLink\nAbstract\n\nThis article presents evidence that the impact of tariffs on GDP per capita is mediated by economic structure. I use a panel of 161 countries from 1960 to 2019 to study the impact of changes in average tariff rates on GDP per capita. Using a local projections difference-in-differences (LP-DiD) approach allows me to flexibly control for the surge in GDP that precedes tariff reductions, which if ignored could bias the estimates, and to estimate medium-term dynamic effects. The results, consistent with a specific strand of the trade theory literature, establish that the tariff-growth nexus is contingent on economic structure: tariff reductions led to lower GDP per capita for nonmanufacturer countries, but higher GDP per capita for manufacturers. Additionally, the effects are persistent even twenty years after tariff reductions. The validity of the baseline estimates is confirmed by several robustness checks, especially the control for relevant confounders in the tariffs-growth nexus and a clean controls analysis aimed to address biases from heterogeneity as highlighted by recent differencein-differences literature. The results seem to be driven by heterogeneous effects in productivity and capital accumulation, in turn related to changes in the manufacturing share of GDP.\n\nResults\nMain finding of this paper is that the effects of globalization (proxied for by local tariffs) aren’t not uniformly positive for developing countries. The benefits depend importantly on the manufacturing export intensity of the exporter. Countries with high manufacturing export share see strong growth in GDP per capita post lowering tariffs. However, countries with low manufacturing intensity see declines in GDP per capita after dropping tariffs. \nGood example of leveraging insights and methods from @dubeLocalProjectionsApproach. Perhaps useful methodology for my The Universal Law of SaaS Growth efforts, since the setup is quite similar (panel data, per unit impulses)\nMakes the important point that local projections can suffer from pre-trends issues reminiscent of difference-in-differences if the outcome variable exhibits a trend relative to the timing of the impulse variable. It’s a form of selection bias. For example, countries that reduce tariffs in general were seeing rising GDP per capita ahead of tariff reduction, implying that the positive trend afterward may have already been baked in the cake. This kind of chart can easily be generated by simply running the same LP estimator on negative horizons (in levels, lagged outcome variable minus level at t-1). This can be avoided by including lags of the outcome variable: \n\ncountries reducing their tariffs are on different pretrends from those not changing them. In particular, the former countries display a relative surge in GDP before tariff reductions as compared to the latter. In other words, tariff changes are endogenous to the evolution of GDP, such that countries that decide to decrease tariffs do so after GDP has been on a relative increase. Failure to control for this surge constitutes a clear violation of the parallel trends assumption and may lead to biases in the treatment effect estimates.\n\nThey also test for non-linearity in the interaction with manufacturing by using six quantiles of manufacturing intensity. \nInteresting tidbits\nNotably, they only include time fixed effects in their panel local projections because the data is already differenced. This is a good point that I hadn’t previously considered, but it’s aligned with the standard time series econometrics pedagogy:\n\nI include only time fixed effects, as the equation is already in differences\n\nI do worry about applying this too broadly – in my areas of research growth rates of companies tend to be highly variable, much more so than the growth rates of countries. Seems like even after differencing if you know that there’s large differences in growth rates across units then unit fixed effects might still be a good idea.\nSimple way to eyeball how many lags you need in your local projections  to avoid pre-trend: graph the negative horizon impulse response with various lag lengths. Keep adding lags until pre-trends are no more: \nGood example of running local projections with interactions to explore heterogeneity: y_{c, t+h}-y_{c, t-1}=\\beta_h \\Delta T A_{c, t}+\\theta_h i n t_{c, t}+\\phi_h m_{c, t}+\\sum_{j=1}^8 \\sigma_h^j g_{c, t-j}+\\alpha_t+\\epsilon_{c, t}\n\nThe initial share of manufacturing exports, mc,t, is calculated as the average of this variable in the five years before tariff reductions, to avoid contemporaneous endogeneity that may run from GDP to manufacturing exports.\n\n\nWith this specification, the impact of tariff changes on growth varies with the initial level of the manufacturing share of exports. For example, if I want to calculate the cumulative change in GDP per capita at time t + h in relation to a one standard-deviation tariff reduction for a country with an initial manufacturing share of exports of 29 percent, I estimate it by calculating (-1) * S D(\\Delta T A) *\\left(\\beta_h+29 * \\theta_h\\right).\n\nInteresting definition of a clean control – country which hasn’t experienced a &gt;1 standard deviation tariff change in the last 10 years, with the term determined by observing that local projection effects seem to stabilize after 10 years.\n\nReferences\n@dubeLocalProjectionsApproach\npaperprocessonline"},"@hsuSubsetSelectionVector2008":{"title":"Subset selection for vector autoregressive processes using Lasso","links":["LASSO","Vector-autoregression","@wangRegressionCoefficientAutoregressive2007","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Subset selection for vector autoregressive processes using Lasso\nNan-Jung Hsu, Hung-Lin Hung, Ya-Mei Chang\nLink\nAbstract\n\nA subset selection method is proposed for vector autoregressive (VAR) processes using the Lasso [Tibshirani, R. (1996). Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society, Series B 58, 267–288] technique. Simply speaking, Lasso is a shrinkage method in a regression setup which selects the model and estimates the parameters simultaneously. Compared to the conventional information-based methods such as AIC and BIC, the Lasso approach avoids computationally intensive and exhaustive search. On the other hand, compared to the existing subset selection methods with parameter constraints such as the top-down and bottom-up strategies, the Lasso method is computationally efficient and its result is robust to the order of series included in the autoregressive model. We derive the asymptotic theorem for the Lasso estimator under VAR processes. Simulation results demonstrate that the Lasso method performs better than several conventional subset selection methods for small samples in terms of prediction mean squared errors and estimation errors under various settings. The methodology is applied to modeling U.S. macroeconomic data for illustration.\n\nRelatively straightforward paper the merely makes the point that you can apply lasso to vector autoregressions in a fairly braindead way and get better forecasting results than picking the lag length via Bayesian criteria.\n\nReferences\nRegression Coefficient and Autoregressive Order Shrinkage and Selection Via the Lasso\npaperreadonline"},"@jonesGrowthIdeas2005":{"title":"Growth and Ideas","links":["Charles-I.-Jones","Scale-invariant-growth","First-Welfare-Theorem","Perfect-competition-is-not-optimal-under-increasing-returns-to-scale","Long-run-exponential-growth-requires-scale-invariant-growth-in-at-least-one-input","Ideas-generate-increasing-returns-because-they-are-nonrivalrous","tags/literature","tags/Economics","tags/Economics/Growth","tags/Economics/Macro","tags/Economics/Competition","tags/online","Imperfections-of-perfect-competition","Convexity","You-Don't-Understand-Compound-Growth","@jonesFutureEconomicGrowth"],"tags":["literature","Economics","Economics/Growth","Economics/Macro","Economics/Competition","online"],"content":"Growth and Ideas\nCharles I. Jones\nAbstract\n\nIdeas are different from nearly all other economic goods in that they are nonrivalrous. This nonrivalry implies that production possibilities are likely to be characterized by increasing returns to scale, an insight that has profound implications for economic growth. The purpose of this chapter is to explore these implications.\n\n\nLiterature Notes\nIncluding ideas in the production function leads to increasing returns to scale, as a doubling of physical inputs and ideas more than doubles output. This is a direct result of the fact that doubling physical inputs alone already doubles output, so layering in additional ideas must lead to an even larger increase. \n\n\n\nDon’t include “per worker” variables when considering the convexity/concavity of / returns to scale of a production function\n\n\n\nGrowth models with ideas in them means that factors cannot be paid their marginal products, as the marginal products add up to more than output itself. Standard competitive equilibrium runs into problems in such models.\n\n\n\nThe linearity critique makes the point that many endogenous growth models require the assumption of a linear differential equation in order to generate stable long-run exponential growth (or as I call it, Scale invariant growth). In a linear differential equation, absolute growth of a variable is proportional to scale.\n\n\n\nAny model with long-run exponential growth will involve a linearity, not just endogenous growth models\n\n\n\nLinearity / Scale invariant growth applies most naturally to population, since living beings reproduce in proportion to their number1\n\n\n\nPerfect competition will not deliver the optimal allocation of resources (First Welfare Theorem) in growth models with ideas that generate increasing returns to scale. \n\n\n\n\nPermanent Notes\n\nPerfect competition is not optimal under increasing returns to scale\nLong-run exponential growth requires scale invariant growth in at least one input\nIdeas generate increasing returns because they are nonrivalrous\n\nliteratureEconomicsGrowthMacroCompetitiononline\nImperfections of perfect competition // Scale invariant growth // Convexity\nFootnotes\n\n\nRelated to my bacteria analogy in You Don’t Understand Compound Growth, also mentioned in The Past and Future of Economic Growth: A Semi-Endogenous Perspective ↩\n\n\n"},"@jordaModelFreeImpulseResponses2003":{"title":"Model-Free Impulse Responses","links":["Vector-autoregression","local-projections","@dubeLocalProjectionsApproach","bias","Local-projections","Impulse-responses","@plagborg-mollerLocalProjectionsVARs2021","Local-projections-vs.-VARs","@liLocalProjectionsVs2023","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Model-Free Impulse Responses\nOscar Jorda – 2003\nVARs decompose the economy (or more literally, a vector of time series) into systematic/deterministic responses and random sources of variation.\nThe central benefit of local projections is that they do not require specifying and estimating an unknown multivariate dynamic system. This is especially relevant when the data may not in fact follow such a system, perhaps because certain variables are effectively exogenous to the subsystem of variables you are most interested in. Although they are most often used in macroeonomic studies, this feature makes local projections especially useful in micro settings, hence their adoption in the causal inference setting of LP-DiD (@dubeLocalProjectionsApproach).\nVARs are in fact ideal for one-period ahead forecasting. While they can be used for longer horizon forecasting as well, this put them firmly in the realm of extrapolation, leading to bias. They are global approximations to the true dynamic multivariate system, which again, doesn’t necessarily even exist in the first place. Thus in using them you are actually making a significant assumption right off the bat, whether you realize it or not.\nIf the data do follow a VAR, then a VAR will be the most efficient estimator. However, if the data do not follow such a structure, local projections can be more robust, especially as the forecast horizon increases. There is a good argument that, unless you have strong theory which suggests the data follows a VAR, you should remain agnostic to the DGP and use a local projection to estimate impulse responses. The only caveat to this is that the two methods estimate the same impulse response up to the horizon h corresponding to the chosen lag length p, i.e. while h\\leq p (@plagborg-mollerLocalProjectionsVARs2021). After that horizon, however, all bets are off – they no longer estimate the same impulse responses unless the DGP is a VAR, which you can’t know for certain in applied settings. See Local projections vs. VARs and @liLocalProjectionsVs2023.\nVARs are especially dangerous when the data is persistent or non-stationary, as the model misspecification biases imbued by the cross-horizon restrictions get amplified. These issues are less severe when the data is stationary, since no matter what the impulse responses need to trend toward zero with the horizon length. This will happen automatically in any reasonably accurate VAR estimated on stationary data.\n\nReferences\npaperreadonline"},"@karapanagiotiModelSelectionLocal":{"title":"Model Selection for Local Projections Instrumental Variable Methods - Empirical Application to Government Spending Multipliers.","links":["Local-projections","Vector-autoregression","LASSO","data-generating-process","regularization","local-projections","Impulse-responses","Principal-component-analysis","Dynamic-factor-model","@rameyMacroeconomicShocksTheir2016","@adamekLocalProjectionInference2022","OLS","@rameyGovernmentSpendingMultipliers","@chernozhukovHIGHDIMENSIONALMETRICS","@belloniInferenceHighDimensionalSparse2011a","@adamekLassoInferenceHighDimensional2022","@vandegeerAsymptoticallyOptimalConfidence2014","@zhangConfidenceIntervalsLow2014","@hsuSubsetSelectionVector2008","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Model Selection for Local Projections Instrumental Variable Methods - Empirical Application to Government Spending Multipliers.\nChrysoula Karapanagioti\nLink\nAbstract\n\nThis thesis tackles the problem of model selection for a single equation estimation method named Local Projection with Instrumental variable. Regularization techniques that choose the model and estimate the parameter concurrently are used to estimate the impulse response functions, which is especially beneficial in vector autoregressive contexts. The main focus is upon the Desparsified Lasso method. A simulation research as well as an empirical investigation to government expenditure multipliers illustrate the usefulness of the processes in terms of forecasting and model development. In addition, the main results indicate that the Desparsified Lasso produce much closer estimated impulse responses both in DGP and empirical analysis.\n\nThis paper explores how regularization methods can be used in concert with VARs and local projections to estimate impulse responses in high-dimensional contexts. Importantly, the author claims that these estimators incorporate the temporal dynamics and spatial dependence that VARs inherently provide. Thus, we’re not necessarily losing anything or estimating the wrong estimand by using these methods, which is a worry I’ve had.\n\nRecent advances in the literature have resulted in strategies that use regularized estimating techniques to choose variables in VARs. The high-dimensional VAR model is estimated using this method. Specifically, a selection of the regularization models developed by Nicholson et al. (2020) like Componentwise HLAg, Lag-Weighted Lasso, Disparsified Lasso by van de Geer et al. (2014) , Lasso by Tibshirani (1996), and Post Double Lasso by Chernozhukov, Hansen, and Spindler (2015) are used. These models incorporate relevant information (temporal dynamics and spatial dependence) that VARs inherently provide.\n\nAlternatives for reducing the dimensionality of VAR models include:\n\nBayesian VARs\nPrincipal component analysis\nDynamic factor models\n\nThe author goes right out and acknowledges that VARs with the same number of lags for every variable might not be ideal, which is something I’ve always wondered about. @rameyMacroeconomicShocksTheir2016 makes a similar point when she notes that control variables don’t need to be the same for each horizon of a local projection. Maybe that’s not exactly the same point, but there’s a similar line of thinking running through both:\nTransclude of @rameyMacroeconomicShocksTheir2016#d3cf71\nDesparsified Lasso\nThe point of desparsifying lasso estimates (as in Local Projection Inference in High Dimensions) is to obtain “uniformly reliable inference” which seems to be a fancy way of saying unbiased estimates of the coefficients. Desparsifying effectively undoes the coefficient shrinkage.\nI don’t totally understand the point of using Lasso in the first place if you then desparsify it. It must be the case that not all variables are unshrunk or rather they are not all fully unshrunk. I guess the idea is that the standard Lasso emphasizes predictive performance rather than unbiasedness of the coefficients, and so the coefficients can be far from their true values if that helps the regression. You might want to undo this effect. I would assume from the name that what happens in practice is that many formerly zeroed out coefficients come back with small absolute values.\nThe thinking seems to go – we use Lasso because we have too many covariates, but we pay the cost of bias in the coefficients. So we undo the sparsification (presumably on only the variables that weren’t shrunk all the way to zero).\nJudging from the desparsifying equation, it seems like you take the lasso betas and add back something the roughly tracks the degree to which a predictor is correlated to the residuals, which is possible because we aren’t using OLS. This correlation is then discounted for how variable the residualized predictor is.\nLag-Weighted Lasso\nStandard lasso is totally unstructured in the sense that all predictors are treated equally. One could imagine wanting to treat different coefficients differently based on prior beliefs. For example, if the regression includes lags, you might think that more recent lags are more important than further away lags, which should be more heavily penalized.\nGroup Lasso\nReally interesting note that if you apply ridge regression but group the variables in some way and then apply the penalization to the sum of the L2-norms, the estimator will actually pick or drop entire groups, even though ridge regression doesn’t normally reduce coefficients to zero. By penalizing the sum of L2-norms, you achieve parameter selection.\nResults\nDesparsified Lasso performs the best in the simulated examples, but it’s pretty hard to tell this from the tables.\n\nIt’s outperformance is more clear in the empirical example (from @rameyGovernmentSpendingMultipliers):\n\nThis guy’s write up of the results is terrible.\nThe big takeaway from this paper is that it’s probably worthwhile to improve my understanding of the desparsified / debiased lasso. It’s doing something interesting and different from both the standard lasso and the post-(double) lasso estimator (HIGH-DIMENSIONAL METRICS IN R, @belloniInferenceHighDimensionalSparse2011a). It’s clearly trying to achieve something similar to post-lasso, which is unshrinking the coefficients, but in the case of desparsified lasso it’s unshrinking potentially all the coefficients while post-lasso only unshrinks the coefficients that were selected in the first place. Standard lasso fails because it struggles to handle both bias and variable selection at the same time.\nThese would be good places to start:\n\nLasso Inference for High-Dimensional Time Series\nLocal Projection Inference in High Dimensions\nOn asymptotically optimal confidence regions and tests for high-dimensional models\nConfidence intervals for low dimensional parameters in high dimensional linear models\n\nInteresting tidbits\n\nAssuming that the variables are all already stationary, adding lags of the outcome to the RHS helps reduce standard errors.\nInvertibility in VARs implies that structural shocks can be recovered from the data, which is to say non-invertibility means that structural shocks cannot be recovered from the data. Whenever you hear “invertibility” you should simply think “the structural shocks can be identified.”\n\n\nReferences\nSubset selection for vector autoregressive processes using Lasso\npaperreadonline"},"@liLocalProjectionsVs2023":{"title":"Local Projections vs. VARs: Lessons From Thousands of DGPs","links":["local-projections","@herbstBiasLocalProjections","Vector-autoregression","@barigozziLargedimensionalDynamicFactor2021","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Local Projections vs. VARs: Lessons From Thousands of DGPs\nDake Li, Mikkel Plagborg-Møller, Christian K. Wolf – 2023\nNever use local projections in small samples unless you have overwhelming concern for bias, in which case you should really use bias-corrected LP a la @herbstBiasLocalProjections. In such scenarios, use a Bayesian VAR or a least a standard least squares VAR.\n\nReferences\n@barigozziLargedimensionalDynamicFactor2021\npaperreadonline"},"@nowakMathematicalFoundationsMachine":{"title":"Mathematical Foundations of Machine Learning","links":["tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Mathematical Foundations of Machine Learning\nRobert Nowak\nConditional probability\nDefined as the solution to the equation p(x, y) = p(y|x)p(x) or, rearranged, p(y|x) = p(x,y)/p(x)\nIn this way, we can think of conditional probabilities as a mathematical object rather than a more intuitive “words” explanation.\nMarginal probability\nTo marginalize is to sum over all possible values of a variable. Thus, if the joint density in question is p(x,y), the marginal probability of a random variable X is p(x) = \\sum_yp(x,y) = \\int_yp(x,y)dx\nMarginalization is often useful when you need to remove a random variable from consideration. By marginalizing above, y drops out of the equation.\nExpected value\nThe expected value of a function of a random variable is E[f(X)] = \\sum_xf(x)p(x) = \\int f(x)p(x)dx\nNote that this works for any function – you just evaluate the function at each potential realization and multiply that by the probability of that realization, then sum it all up. You can also use this for things like conditional expectations, where the “function” is one of the variables and the probability is the conditional probability: E[Y|X] = \\sum_yyp(y|x) = \\int yp(y|x)dx\n\nReferences\npaperreadonline"},"@parkBayesianLasso2008":{"title":"The Bayesian Lasso","links":["tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"The Bayesian Lasso\nTrevor Park, George Casella – 2008\n\n\n                  \n                  Abstract \n                  \n                \n\nThe Lasso estimate for linear regression parameters can be interpreted as a Bayesian posterior mode estimate when the regression parameters have independent Laplace (i.e., double-exponential) priors. Gibbs sampling from this posterior is possible using an expanded hierarchy with conjugate normal priors for the regression parameters and independent exponential priors on their variances. A connection with the inverse Gaussian distribution provides tractable full conditional distributions. The Bayesian Lasso provides interval estimates (Bayesian credible intervals) that can guide variable selection. Moreover, the structure of the hierarchical model provides both Bayesian and likelihood methods for selecting the Lasso parameter. Slight modifications lead to Bayesian versions of other Lasso-related estimation methods, including bridge regression and a robust variant.\n\n\n\n\n                  \n                  Summary \n                  \n                \n\nTK\n\n\nThe nice part about a fully Bayesian approach to Lasso is that you don’t have to worry about doing cross-validation. You just set your priors and you’re ready to go. Even the \\lambda parameter in the Laplace coefficient prior can be set via maximum likelihood.\nHierarchical priors\n\nPrior on coefficients,  \\beta\nPrior on variance of coefficients, \\sigma^2\nPrior on the Laplace parameter, \\lambda\n\nPriors\nPrior on coefficients, \\beta (conditional Laplace distribution):\np(\\beta|\\sigma^2)= \\prod_{j=1}^p \\frac{\\lambda}{2\\sqrt{\\sigma^2}}e^{-\\lambda|\\beta_j|/\\sqrt{\\sigma^2}}\n\\lambda here is equivalent to \\sqrt{\\sigma^2}/b, where b is the scale parameter of the Laplace distribution. So in a way \\lambda is the inverse variance / precision.\nPrior on variance of coefficients, \\sigma^2 (non-informative scale-invariant):\np(\\sigma^2) = 1/{\\sigma^2}\nNote that it’s presumed the data has been standardized, so the above is technically scale-invariant. Also note that this kind of prior is very similar to an exponential, one-sided Laplace, or Gamma distribution with shape parameter = 1.\nPrior on the Laplace parameter, \\lambda (note \\lambda^2, not \\lambda):\np(\\lambda^2) = \\frac{\\delta^2}{\\Gamma(r)}(\\lambda^2)^{r-1}e^{-\\delta\\lambda^2}\nwith r = 1 for most use cases and \\delta &gt; 1. This is a Gamma distribution. If r=1 it actually becomes an exponential distribution.\nEstimation\nThe authors propose a Gibbs sampling approach to estimating the Bayesian Lasso, which exploits the fact that the Laplace distribution can be represented as a mixture of normal distributions.\n\nReferences\npaperreadonline"},"@phillipsBoostingWhyYou2021":{"title":"Boosting: Why You Can Use the Hp Filter","links":["boosting","Hodrick-Prescott-Filter","bias","stationarity","@hamiltonWhyYouShould2017","Why-You-Should-Never-Use-the-Hodrick-Prescott-Filter","tags/literature/paper","tags/inbox/process","tags/online"],"tags":["literature/paper","inbox/read","inbox/process","online"],"content":"Boosting: Why You Can Use the Hp Filter\nPeter C. B. Phillips, Zhentao Shi\nAbstract\n\nWe propose a procedure of iterating the HP filter to produce a smarter smoothing device, called the boosted HP (bHP) filter, based on L2-boosting in machine learning. Limit theory shows that the bHP filter asymptotically recovers trend mechanisms that involve integrated processes, deterministic drifts, and structural breaks, covering the most common trends that appear in current modeling methodology. A stopping criterion automates the algorithm, giving a data-determined method for data-rich environments. The methodology is illustrated in simulations and with three real data examples that highlight the differences between simple HP filtering, the bHP filter, and an alternative autoregressive approach.\n\nIt’s important to use a lower penalty parameter for smaller timeseries, otherwise the HP filter will tend to over-penalize (over-smooth) the data.\nIn some ways the choice of lambda for the HP filter has less to do with the frequency of the data and more to do with the amount of data you have. The real reason lambda tends to align with data frequency is that similar frequency data tends to be of similar length. If you have similar frequency but significantly different length, it can make sense to use a different lambda penalization.\nProbably makes sense to bias towards setting \\lambda too high to begin with, rather than too low. Setting \\lambda too low causes the HP filter to effectively over-fit the data, meaning that the residuals/cycle is too small/low variance, which can’t really be undone by further filtering/boosting. On the other hand, if \\lambda is too high initially, the filter under-fits, meaning the trend will be biased and the errors/cycle will be too “large,” which can always be fixed via reapplication of the filter, progressively shaving off more non-stationary variation.\nRepo for bHP filter\n\nReferences\nWhy You Should Never Use the Hodrick-Prescott Filter\nWhy You Should Never Use the Hodrick-Prescott Filter\npaperprocessonline"},"@plagborg-mollerEssaysMacroeconometrics":{"title":"Essays in Macroeconometrics","links":["Impulse-responses","Local-projections","@greenwaldOriginsStockMarket2014","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Essays in Macroeconometrics\nMikkel Plagborg-Møller\nLink\n\n\n                  \n                  Abstract \n                  \n                \n\nThis dissertation consists of three independent chapters on econometric methods for macroeconomic analysis. In the first chapter, I propose to estimate structural impulse response functions from macroeconomic time series by doing Bayesian inference on the Structural Vector Moving Average representation of the data. This approach has two advantages over Structural Vector Autoregression analysis: It imposes prior information directly on the impulse responses in a flexible and transparent manner, and it can handle noninvertible impulse response functions. The second chapter, which is coauthored with B. J. Bates, J. H. Stock, and M. W. Watson, considers the estimation of dynamic factor models when there is temporal instability in the factor loadings. We show that the principal components estimator is robust to empirically large amounts of instability. The robustness carries over to regressions based on estimated factors, but not to estimation of the number of factors.In the third chapter, I develop shrinkage methods for smoothing an estimated impulseresponse function. I propose a data-dependent criterion for selecting the degree of smoothing to optimally trade off bias and variance, and I devise novel shrinkage confidence sets with valid frequentist coverage.\n\n\nSmoothness priors sharpen inference (i.e. reduce standard errors) for impulse response functions, since smoother IRFs have fewer effective free parameters. In other words, if you shrink toward a polynomial, that polynomial has fewer parameters than some other complex function you could come up with. It has to look a certain way (like a polynomial), irrespective of the data, which reduces uncertainty. There’s only so many ways to skin a cat, and likewise there’s only so many ways to draw a polynomial of a particular finite order that loosely fits the data. The coefficients of the polynomials effectively define the IRF, so reduced parameter uncertainty is equivalent to reduced IRF uncertainty.\nExplicit confirmation that if you have the correct shocks, you can just regress the outcome variable on the shocks directly a la local projections. This is what they do in Origins of Stock Market Fluctuations.\nSeems that an inherent drawback of the SVMA approach is that the parameters (which are the impulse response function) are under-identified, so it’s not just that the method makes priors easy and transparent to impose, they must be imposed. You could say that this isn’t much of a disadvantage, since in the SVAR setting you often impose the equivalent of priors as well, but in a more obtuse and less direct fashion.\n\nReferences\npaperreadonline"},"@stockDynamicFactorModels2016":{"title":"Dynamic Factor Models, Factor-Augmented Vector Autoregressions, and Structural Vector Autoregressions in Macroeconomics","links":["Dynamic-factor-model","Vector-autoregression","identification","OLS","omitted-variable-bias","Impulse-responses","Forecast-error-variance-decomposition","Moving-average-model","@baiPrincipalComponentsEstimation2013","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Dynamic Factor Models, Factor-Augmented Vector Autoregressions, and Structural Vector Autoregressions in Macroeconomics\nJ.H. Stock, M.W. Watson\nLink\nAbstract\n\nThis chapter provides an overview of and user’s guide to dynamic factor models (DFMs), their estimation, and their uses in empirical macroeconomics. It also surveys recent developments in methods for identifying and estimating SVARs, an area that has seen important developments over the past 15 years. The chapter begins by introducing DFMs and the associated statistical tools, both parametric (state-space forms) and nonparametric (principal components and related methods). After reviewing two mature applications of DFMs, forecasting and macroeconomic monitoring, the chapter lays out the use of DFMs for analysis of structural shocks, a special case of which is factor-augmented vector autoregressions (FAVARs). A main focus of the chapter is how to extend methods for identifying shocks in structural vector autoregression (SVAR) to structural DFMs. The chapter provides a unification of SVARs, FAVARs, and structural DFMs and shows both in theory and through an empirical application to oil shocks how the same identification strategies can be applied to each type of model.\n\nDynamic Factor Models\nThis article is basically the whole textbook on DFMs. It’s super helpful, though I don’t think for practical purposes one should code up the algorithms on your own.\nAt some point I might take some notes here but for now I think I can simply leverage this stuff out of the box (e.g. using the R package dfms) without worrying too much about the core details.\nStructural Vector Autoregressions\nIf shocks were observed, you could simply run OLS of the outcome variable on the current and past shock values, and you’d be good to go. If the shock is a true shock, it’s uncorrelated with other variables, so there’s no omitted variable bias. The coefficients will be unbiased and represent the impulse response of the outcome to the shock:\n\nIf a time series of shocks were observed, it would be straightforward to estimate the effect of that shock, say \\varepsilon_{1t}, on a macro variable y_t by regressing y_t on current and past values of \\varepsilon_{1t}. Because the shock \\varepsilon_{1t} is uncorrelated with the other shocks to the economy, that regression would have no omitted variable bias. The population coefficients of that regression would be the dynamic causal effect of that shock on the dependent variable, also called the structural impulse response function (SIRF). The cumulative sum of those population coefficients would be the cumulative causal effect of that shock over time, called the cumulative SIRF.\n\nSince the shocks are typically not in fact observed, the whole point of SVARs is to leverage the assumption that the forecast errors / innovations of the VAR fully represent the space of structural shocks to uncover the impact of those structural shocks on relevant variables of interest. Economists use the term “span the shocks,” but all this means in practice is that the VAR innovations are linear combinations of all the structural shocks in the economy and vice versa. Said this way, this seems like a fairly strong assumption, but accepting it at least in principle lets us do some very interesting things.\n\nThe premise of SVARs is that the space of the innovations to a vector of time series variables Y_t—that is, the one step ahead forecast errors of Y_t based on a population projection of Y_t onto its past values—spans the space of the structural shocks. Said differently, in population the econometrician is assumed to be as good at one step ahead forecasting of the economy as an agent who directly observes the structural shocks in real time. The task of identifying the structural shock of interest thus reduces to the task of finding the linear combination of the innovations that is the structural shock.\n\nHowever, if one is only interested in the effect of a subset of shock, only those shocks need to be spanned by the VAR innovations. For example, if I’m only interested in the effect of a single shock, then only that shock needs to be a linear combination of the VAR innovations in the system. This is a helpful distinction in theory, though I wonder in practice to what extent results would ever depend on this being true vs the broader assumption of full spanning needing to be true.\nIn practical terms, this tends to mean that you need at least as many variables as shocks you want to identify. Adding more variables than that helps to ensure the structural shocks of interest are in fact fully spanned, but the math works as soon as you have enough variables / reduced form equations in the VAR.\nForecast error variance decomposition (FEVD): the breakdown of how important each shock is in explaining the variation in some outcome variable. FEVDs are calculated as the relative size of the variance of a shock to the overall variance in the forecast errors in the outcome variable over future periods (the h-step ahead forecast errors). This it is calculated separately for each horizon (h), shock (j), and outcome variable triplet (i). In a way, you can think about the FEVD as simply using the idea of a system of equations to say that the innovations of one variable can be explained by the structural shocks within the system, explicitly calculating the relative contributions.\nHistorical decomposition (HD): These are literally just the moving average representation of the time series. Thus once you have the impulse response function you effectively also have the historical decomposition. Another way to think about this is that you can project the demeaned observed variable on current and past shocks and that result is the contribution of the shocks to that variable.\n\nReferences\n@baiPrincipalComponentsEstimation2013\npaperreadonline"},"@stockIdentificationEstimationDynamic2018":{"title":"Identification and Estimation of Dynamic Causal Effects in Macroeconomics Using External Instruments","links":["local-projections","vector-autoregression","Local-projections","instrumental-variables","Vector-autoregression","omitted-variable-bias","@nakamuraIdentificationMacroeconomics2018","shock","Dynamic-factor-model","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Identification and Estimation of Dynamic Causal Effects in Macroeconomics Using External Instruments\nJames H. Stock, Mark W. Watson\nLink\nVideo\nAbstract\n\nExternal sources of as-if randomness — that is, external instruments — can be used to identify the dynamic causal effects of macroeconomic shocks. One method is a one-step instrumental variables regression (local projections – IV); a more efficient two-step method involves a vector autoregression. We show that, under a restrictive instrument validity condition, the one-step method is valid even if the vector autoregression is not invertible, so comparing the two estimates provides a test of invertibility. If, however, lagged endogenous variables are needed as control variables in the one-step method, then the conditions for validity of the two methods are the same.\n\nMakes the point that there can be more shocks than economics variables in your system of equations:\n\nIf we collect all such structural shocks and measurement error together in the m \\times 1 vector \\varepsilon_t, the n \\times 1 vector of macroeconomic variables Y_t can be written in terms of current and past \\varepsilon_t\n…\nIn general, the number of shocks plus measurement errors, m, can exceed the number of observed variables, n.\n\nThe unit effect normalization – fixing the shock variable such that a one unit movement corresponds to a unit movement in the impulse variable of interest. This solves the scale ambiguity issues the arises from the fact that the true shock is unobserved. It also underpins the local projections approach,  as it enables regressions in terms of observables.\nProves that local projections with instrumental variables can be used in situations when the equivalent VAR would be invertible:\n\nwe provide conditions for instrument validity for LP-IV, and show that under those conditions LP-IV can estimate dynamic causal effects without assuming invertibility, that is, without assuming that the structural shocks can be recovered from current and lagged values of the observed data.\n…\nThe structural moving average \\Theta(\\text{L}) in (5) is said to be invertible if et can be linearly determined from current and lagged values of Y_t:\n…\nIn the linear models of this article, condition (24) is equivalent to saying that \\Theta(\\text{L})^{-1}\u0001exists.\n\n\nInvertibility implies that the VAR is fully specced out. In other words, the system contains all the data you need to uncover the true shocks and you don’t need to augment it with anything, nor would you benefit from doing so:\n\nunder invertibility, a forecaster using a VAR would find no value in augmenting her system with data on the true macroeconomic shocks, were they magically to become available.\n…\na forecaster using a VAR who magically stumbled upon the history of true shocks would have no interest in adding those shocks to her forecasting equations.\n…\nIf invertibility holds, then knowledge of the past true shocks would not improve the VAR forecast. If instead those forecasts were improved by adding the shocks to the regression – infeasible, of course, but a thought experiment – then the VAR has omitted some variables, and that omission is an indication of the failure of the invertibility assumption.\n\nCovers the omitted variable bias interpretation of invertibility as discussed in @nakamuraIdentificationMacroeconomics2018:\n\nOne interpretation provided in the literature on invertibility is that invertibility implies that there are no omitted variables in the VAR (e.g. Fernandez-Villaverde et al., 2007): because invertibility implies that the spans of \\varepsilon_T and \\nu_t are the same, there is no forecasting gain from adding past shocks to the VAR.\n\nInvertibility as meaning structural shocks can be expressed as linear combination of current and past values of the observed data:\n\nThe structural MA representation Y_t = D(\\mathrm{L}) \\varepsilon_t represents Y_t in terms of current and past values of the structural shocks \\varepsilon_t. The moving average is said to be invertible if \\varepsilon_t can be expressed as a distributed lag of current and past values of the observed data Y_t. SVARs typically assume \\varepsilon_t = H^{-1} \\eta_t = H^{-1}A(\\mathrm{L})Y_t, so an SVAR typically imposes invertibility.\n\nFinally hit me that the “moving average” representation simply means representing current variables as a linear combination of current and past innovations / shocks, while the “VAR” representation means representing residuals / shocks as a linear combination of current and past observed data.\nYou can get the moving average representation of a VAR by regressing the outcome variables on its lags, then regress the outcome variables on the residuals from the first regression. This yield the MA coefficients. It is also therefore an easy way to invert a VAR without literally inverting the coefficient matrix.\nSolutions to omitted variable bias in the context of a VAR:\n\nInclude large number of variables via DFM, FAVAR, BVAR etc\ninstrumental variables estimation\n\n\n\nReferences\npaperreadonline"},"@taylorSystematicShiftsScaling2021":{"title":"Systematic shifts in scaling behavior based on organizational strategy in universities","links":["tags/online"],"tags":["literature","inbox/read","online"],"content":"Systematic shifts in scaling behavior based on organizational strategy in universities\nRyan C. Taylor, Xiaofan Liang, Manfred D. Laubichler, Geoffrey B. West, Christopher P. Kempes, Marion Dumas\nAbstract\n\nTo build better theories of cities, companies, and other social institutions such as universities, requires that we understand the tradeoffs and complementarities that exist between their core functions, and that we understand bounds to their growth. Scaling theory has been a powerful tool for addressing such questions in diverse physical, biological and urban systems, revealing systematic quantitative regularities between size and function. Here we apply scaling theory to the social sciences, taking a synoptic view of an entire class of institutions. The United States higher education system serves as an ideal case study, since it includes over 5,800 institutions with shared broad objectives, but ranges in strategy from vocational training to the production of novel research, contains public, nonprofit and for-profit models, and spans sizes from 10 to roughly 100,000 enrolled students. We show that, like organisms, ecosystems and cities, universities and colleges scale in a surprisingly systematic fashion following simple power-law behavior. Comparing seven commonly accepted sectors of higher education organizations, we find distinct regimes of scaling between a school’s total enrollment and its expenditures, revenues, graduation rates and economic added value. Our results quantify how each sector leverages specific economies of scale to address distinct priorities. Taken together, the scaling of features within a sector along with the shifts in scaling across sectors implies that there are generic mechanisms and constraints shared by all sectors, which lead to tradeoffs between their different societal functions and roles. We highlight the strong complementarity between public and private research universities, and community and state colleges, that all display superlinear returns to scale. In contrast to the scaling of biological systems, our results highlight that much of the observed scaling behavior is modulated by the particular strategies of organizations rather than an immutable set of constraints.\n\nHighlights\n\nRevenue and expenses at universities and colleges follow power law behavior with respect to enrollment\n\n“We show that, like organisms, ecosystems and cities, universities and colleges scale in a surprisingly systematic fashion following simple power-law behavior.”\n\nMost socioeconomic metrics scale with population as a power law with exponent 1.15\n\n“from total wages and GDP to the number of social interactions and number of patents produced, scale with population size as a power law with an exponent of 1.15.”\n…\n“Consequently, on a per capita basis, socio-economic metrics increase proportionally to X0.15, implying that on a per capita basis, larger cities promote more social interactions and greater production of patents, and therefore more innovation”\n\nOn average, universities and colleges scale revenue and expenses linearly with student enrollment\n\n“financial throughput scales linearly with size, suggesting that, on average, there is no advantage to being larger at least as far as these economic indicators are concerned”\n\n\nResearch universities scale revenue and expenses superlinearly with respect to enrollment\n\n“research universities (both public and non-profit private) scale superlinearly: as they enroll more students, their revenues and expenditures increase faster than linearly”\n…\n“Research universities (public and private) scale superlinearly in all activities and sources of revenue, but sacrifice affordability. As they grow larger, they seek to attract increasingly prestigious faculty (as indicated by the superlinear scaling in faculty pay, especially in private universities) and charge higher tuition, also attracting better-resourced students, who later on enjoy higher earnings. The fact that both research and educational outcomes scale superlinearly suggest that these activities are synergistic.”\n\nCommunity colleges demonstrated sublinear scaling with respect to enrollment, leading to declining per capita spending as they scale:\n\n“community colleges and state colleges display remarkably sublinear scaling, that is, financial throughput per student decreases with size, representing strong overall economies of scale”\n\n\n“non-profit private colleges and professional schools scale roughly linearly with size, indicating little advantage in being larger”\n\nFor profit colleges demonstrate linear revenue scaling and sublinear expenditure scaling, implying rising profit margins as they scale:\n\n“for-profit colleges display linear scaling in revenue but sublinear scaling in expenditure, which implies that they are able to make aprofit by exploiting economies of scale in their costs.”\n\n\n“Maintenance and administrative costs scale slightly superlinearly, but do not systematically outpace production processes (teaching, research and student completions). This indicates that there are no apparent diseconomies of scale in maintenance function. On the contrary, efficiency in maintenance seems to support the diversification of activities”\n\n\n“average mid-career earnings increase with a school’s mean test scores and its out-of-state tuition costs”\n\nonline"},"@vivianoDynamicCovariateBalancing2023":{"title":"Dynamic covariate balancing: Estimating treatment effects over time with potential local projections","links":["local-projections","tags/literature/paper","tags/inbox/read","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"Dynamic covariate balancing: Estimating treatment effects over time with potential local projections\nDavide Viviano, Jelena Bradic – 2023\nMain difference from standard local projections\n\nYou estimate the LPs on the conditional expectation of potential outcomes rather than the conditional expectation of actual outcomes\nYou estimate the LPs recursively, which requires first estimating the furthest out horizon and then using those calculations as part of your estimates for the shorter horizons\nUses a method called dynamic covariate balancing to reweight observations to ensure covariate balance between treated and control units.\n\nWhy have this distinction\n\nThe estimand of standard local projections doesn’t explicitly take a stance of the autocorrelation of the treatment variable. This makes interpretation of the LPs ambiguous, since interpreting what effect you are measuring requires a view on to what degree future treatments are influenced by past treatments and covariates. Note that this doesn’t mean that LPs are “wrong” in any way, merely that their interpretation is tricky and they may not be estimating the thing you want. The only way to interpret an LP as representation only the effect of a singular treatment with no future treatments is if you assume treatments are independent over time (no autocorrelation). This is almost certainly not true unless you are using a properly constructed white noise shock variable with no autocorrelation.\nThis is an important distinction particularly when you have an explicit series of treatments that you want to estimate the effect of. In such situations, you need to be clear about about assigning effects to various treatments. Not so in the standard LP case, where you only care about a single, initiating treatment and whatever happens from there is fair game / not explicitly modeled\nIn effect, this method lets you explicitly estimate the effect of a single treatment followed by no future treatments and the effect of two consecutive treatments. On the other hand, LPs estimate something in the middle – the effect of a single treatment, with no control over or insight into subsequent treatments.\n\nImportant assumptions\n\nThis method takes no stance on modeling selection into treatment, i.e. propensity scores. The issue with propensity scores is that you can, of course, be wrong in your estimation an in addition they can be unstable in certain situations. The nice part about this method is that, via covariate balancing, you can achieve an as-good-as solution relative to propensity scores without literally knowing true propensity model and with lower variance.\nBecause you model potential outcomes, the intertemporal dependence of treatments doesn’t matter, since by assumption potential outcomes are not affected by treatment. This lets you retain full control over the particular treatment effects you analyze\nCan be used on imbalanced panels\n\n\nReferences\npaperreadonline"},"@zouAdaptiveLassoIts2006":{"title":"The Adaptive Lasso and Its Oracle Properties","links":["lasso","LASSO","OLS","bias","tags/literature/paper","tags/online"],"tags":["literature/paper","inbox/read","online"],"content":"The Adaptive Lasso and Its Oracle Properties\nHui Zou\nAbstract\n\nThe lasso is a popular technique for simultaneous estimation and variable selection. Lasso variable selection has been shown to be consistent under certain conditions. In this work we derive a necessary condition for the lasso variable selection to be consistent. Consequently, there exist certain scenarios where the lasso is inconsistent for variable selection. We then propose a new version of the lasso, called the adaptive lasso, where adaptive weights are used for penalizing different coefficients in the \\ell_1 penalty. We show that the adaptive lasso enjoys the oracle properties; namely, it performs as well as if the true underlying model were given in advance. Similar to the lasso, the adaptive lasso is shown to be near-minimax optimal. Furthermore, the adaptive lasso can be solved by the same efficient algorithm for solving the lasso. We also discuss the extension of the adaptive lasso in generalized linear models and show that the oracle properties still hold under mild regularity conditions. As a byproduct of our theory, the nonnegative garotte is shown to be consistent for variable selection.\n\nAdaptive LASSO is just normal lasso with some additional weights on the regressor penalization based on the inverse coefficients coming from some other initial procedure, such as OLS. Gives LASSO the oracle property. Equivalently, you can divide the regressors by their respective coefficients in the first stage.\nStandard LASSO tends to use shrinkage parameter that is tuned for prediction, making it too small for variable selection purposes.\nThe coefficients from standard LASSO tend to be too small (i.e. biased) for variable that should in fact have large coefficients (over shrinkage), and small true parameters have a tendency to be zeroed out.\nHigh multicollinearity between predictors tends to lead to poor variable selection by LASSO.\nasgl is a good Python library for adaptive LASSO (GitHub)\nSome example R code from Stack Overflow:\n# get data\ny &lt;- train[, 11]\nx &lt;- train[, -11]\nx &lt;- as.matrix(x)\nn &lt;- nrow(x)\n \n# standardize data\nymean &lt;- mean(y)\ny &lt;- y-mean(y)  \nxmean &lt;- colMeans(x)\nxnorm &lt;- sqrt(n-1)*apply(x,2,sd)\nx &lt;- scale(x, center = xmean, scale = xnorm)\n \n# fit ols \nlm.fit &lt;- lm(y ~ x)\nbeta.init &lt;- coef(lm.fit)[-1] # exclude 0 intercept\n \n# calculate weights\nw  &lt;- abs(beta.init)  \nx2 &lt;- scale(x, center=FALSE, scale=1/w)  \n \n# fit adaptive lasso\nrequire(glmnet)\nlasso.fit &lt;- cv.glmnet(x2, y, family = &quot;gaussian&quot;, alpha = 1, standardize = FALSE, nfolds = 10)\nbeta &lt;- predict(lasso.fit, x2, type=&quot;coefficients&quot;, s=&quot;lambda.min&quot;)[-1]\n \n# calculate estimates\nbeta &lt;- beta * w / xnorm # back to original scale\nbeta &lt;- matrix(beta, nrow=1)\nxmean &lt;- matrix(xmean, nrow=10)\nb0 &lt;- apply(beta, 1, function(a) ymean - a %*% xmean) # intercept\ncoef &lt;- cbind(b0, beta)\n“We have shown that the lasso cannot be an oracle procedure.” (Zou, 2006, p. 3) (pdf)\n“We now define the adaptive lasso. Suppose that \\hat{\\beta} is a root-n–consistent estimator to \\beta^*; for example, we can use \\hat{\\beta} (ols). Pick a \\gamma &gt; 0, and define the weight vector \\hat{\\text{w}} = 1/|\\hat{\\beta}|^{\\gamma} .” (Zou, 2006, p. 3) (pdf)\n“We have shown that although the lasso variable selection can be inconsistent in some scenarios, the adaptive lasso enjoys the oracle properties by utilizing the adaptively weighted \\ell_1 penalty.” (Zou, 2006, p. 8) (pdf)\n\nReferences\npaperonline"},"Autoregressive-models":{"title":"Autoregressive models","links":["tags/e","tags/online"],"tags":["e","online"],"content":"Autoregressive models\nAutoregressive models seem like a workhorse for modeling various timeseries data without having to make strong theoretical claims about the structure of the underlying variables.\nThe AR(1) model in particular is ubiquitous across economic modeling. Economists model many diverse processes using AR(1) models, especially anything where one believes disturbances eventual dissipate but have some persistence over time.\n\nReferences\neonline"},"Ben-Franklin's-Autoencoder":{"title":"Ben Franklin's Autoencoder","links":["tags/e","tags/online"],"tags":["e","online"],"content":"Ben Franklin’s Autoencoder\nBen Franklin’s method of reading a piece of writing, taking notes on it, and then trying to reproduce the original piece from his notes is identical to an autoencoder architecture in deep learning.\n\nReferences\neonline"},"Beveridge-Nelson-Decomposition":{"title":"Beveridge-Nelson Decomposition","links":["Stochastic-trend","stationarity","@cochraneHowBigRandom1988","tags/e","tags/online"],"tags":["e","online"],"content":"Beveridge-Nelson Decomposition\nThe Beveridge-Nelson (BN) Decomposition breaks down a time series, y_t, into a stochastic trend and stationary cycle. The trend, or permanent component, represents the long-run forecast of y_t at time t, after all transitory shocks have died out. In fact, the BN decomposition is defined so that this is true.\nConsider the time series\ny_t=z_t+c_t\nwhere\nz_t=\\mu+z_{t-1} + (\\sum_{j=0}^\\infty a_j) \\epsilon_t\n-c_t=(\\sum_{j=1}^\\infty a_j) \\epsilon_t + (\\sum_{j=2}^\\infty a_j) \\epsilon_{t-1} + (\\sum_{j=3}^\\infty a_j) \\epsilon_{t-2} + ...\nHere z_t is the trend and c_t is the cycle.\nz_t essentially counts the full impact of of the current shock, even future effects that have not yet occurred. Since it incorporates its own lag, it also by definition includes the full impact of all past shocks as well. In a sense, the permanent component “books all the revenue upfront”.\nc_t cumulates all the temporary impacts from past shocks that have not yet transpired in the current level of the series. In other words, it only books the “future revenue”, dropping anything that’s been booked already.\nThe net of “all impacts past, present, and future” and “impacts in the future” is “impacts past and present”, i.e. the time series itself.\n\nReferences\n@cochraneHowBigRandom1988\neonline"},"Career-success-and-career-capital-are-cointegrated":{"title":"Career success and career capital are cointegrated","links":["shock","Autoregressive-models","Random-walk","@lettauConsumptionAggregateWealth","tags/e","tags/inbox/write","tags/online"],"tags":["e","inbox/write","online"],"content":"Career success and career capital are cointegrated\nCareer success and career capital can diverge periodically but will over the medium to long-run converge. In that sense they are cointegrated – they exist in long-run equilibrium with one another. My personal belief is that career success is the mean reverting part of the system. That is – career success way above or below earned career capital will be transitory. If you get a lucky stream of “success shocks”, you’ll eventually fall back down to your actual level of career capital. Same in the reverse.\nThat said, my sense is that these deviations are somewhat persistent. I don’t have a strong sense of the exact autoregressive coefficient nor the error correction coefficient, but I don’t think this stuff dissipates within a year. It probably takes at least one job turn, which is 2+ years for most white collar professionals.\nNote that in isolation both career success and career capital will appear to be highly persistent random walks, but as in @lettauConsumptionAggregateWealth the true driver the permanent changes can be found via cointegration analysis.\n\nReferences\newriteonline"},"Careers-are-a-random-walk":{"title":"Careers are a random walk","links":["Random-walk","Stochastic-trend","deterministic-trend","data-generating-process","Career-success-and-career-capital-are-cointegrated","tags/e","tags/inbox/write","tags/online"],"tags":["e","inbox/write","online"],"content":"Careers are a random walk\nIt’s a common statement that careers only make sense in retrospect. Another version of this is the Steve Jobs quote that “you can only connect the dots looking backward”:\n\nCareers only make sense in hindsight. – What would a map of your career look like?\n\nIt strikes me that this is simply another way of saying that careers are random walks. Random walks can only be rationalized looking backward, and by definition cannot be rationalized going forward because the trend itself is stochastic (Stochastic trend). There is no deterministic trend that can be used to extrapolate forward. You can go ahead and fit one but it won’t mean anything.\nIf we take this idea seriously, attempts to rationalize careers, even after the fact, are foolish in the same way that “technical analysis” of the stock market is a waste of time. You fool yourself that there are stories in the data that might be useful for predicting the future or guiding behavior. If careers are a random walk, there are no stories. Or, said differently, there is only one story, which is the one that happened, as we can never know the DGP with any certainty or precision.\nSee also: Career success and career capital are cointegrated\n\nReferences\newriteonline"},"Change-leads-to-insight-far-more-often-than-insight-leads-to-change":{"title":"Change leads to insight far more often than insight leads to change","links":["tags/e","tags/online"],"tags":["e","online"],"content":"Change leads to insight far more often than insight leads to change\n\nReferences\neonline"},"Correlation-between-prices-and-quantities-reveals-main-market-driver":{"title":"Correlation between prices and quantities reveals main market driver","links":["heuristic","Old-Blue-Ocean-Draft","Supply-expansion-sucks-for-suppliers-and-vice-versa","@HowMuchSupply","tags/e","tags/online"],"tags":["e","online"],"content":"Correlation between prices and quantities reveals main market driver\nIf prices and quantities are positively correlated, the market is most likely to be demand-driven, which is to say variation in demand over time is the biggest driver of price movements, inflation, etc. Shifts in demand move the market equilibrium along the supply curve, which is upwards sloping. Thus, price and demand tend to be positively correlated when demand moves around a lot.\nOn the other hand, a negative correlation between prices and quantities suggest supply forces are the primary market mover. These shifts in supply move the market along the demand curve, which is downward sloping. This makes prices and quantities move in opposite directions.\nThis heuristic is a nice way to distinguish between demand and supply driven markets:\n\nFor example, the venture capital market is clearly demand-drive, as prices and financing activity tend to move in the same direction\n\n\nReferences\nOld Blue Ocean Draft\nSupply expansion sucks for suppliers and vice versa\n@HowMuchSupply\neonline"},"Diffusion-models":{"title":"Diffusion models","links":["Autoregressive-models","@kolloviehPredictRefineSynthesize2023","@harveyFlexibleDiffusionModeling2022","tags/e","tags/online"],"tags":["e","online"],"content":"Diffusion models\nSetup\n\nq(y): true sampling distribution of observed data\np_\\theta(y): learned sampling distribution\n\\text{x}^{1:T}: noisy latent variables at each step t\n\nForward process\nIn the forward process, noise is continuously added to the data across multiple steps, converging toward white noise \\text{x}^T \\sim N(0, 1):\nq(\\text{x}^t|\\text{x}^{t-1}) := N(\\sqrt{1-\\beta_t}\\text{x}_{t-1}, \\beta_t\\text{I})\nThe fixed Gaussian forward process means we can directly calculate/sample from q(\\text{x}^t|y) without calculating all the intermediate steps (similar to Autoregressive models):\n\\text{x}^t = \\sqrt{\\bar\\alpha_t}y + \\sqrt{(1-\\bar\\alpha_t)}\\epsilon\nWhere \\alpha_t = 1-\\beta_t, \\bar\\alpha_t = \\prod_{i=1}^t \\alpha_i, and \\epsilon is white noise. Again, this is 100% analogous to the forecasting equation for an AR(1).\nIn practice, \\alpha_t is chosen to be close to 1, as this yields the best results.\nDenoising\nIn the denoising or reverse diffusion process, noise is progressively removed over multiple steps, modeling the inverse process:\np_\\theta(\\text{x}^{t-1}|\\text{x}^t) := N(\\mu_\\theta(\\text{x}^t,t), \\sigma_t\\text{I})\nWhere \\sigma_t = \\frac{1-\\bar\\alpha_{t-1}}{1-\\bar\\alpha_t}\\beta_t.\n\\mu_\\theta is parameterized using a denoising network, \\bf\\epsilon_\\theta (of which many exist, including the popular U-Net):\n\\mu_\\theta(\\text{x}^t, t) = \\frac{1}{\\sqrt{\\alpha_t}}(\\text{x}^t-\\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\epsilon_\\theta(\\text{x}^t, t))\nThis model is trained using the objective function:\nE_{y,\\epsilon,t}[||\\epsilon_\\theta(\\text{x}^t,t)-\\epsilon||^2]\n\nReferences\n@kolloviehPredictRefineSynthesize2023\n@harveyFlexibleDiffusionModeling2022\neonline"},"Don't-detrend-random-walks-with-linear-trends":{"title":"Don't detrend random walks with linear trends","links":["Spectral-analysis","@cochraneHowBigRandom1988","Random-walk","@nelsonSpuriousPeriodicityInappropriately1981","@hodrickExplorationTrendCycleDecomposition2020","Beats-and-Misses-Are-Forever","The-Universal-Law-of-SaaS-Growth","tags/e","tags/online"],"tags":["e","online"],"content":"Don’t detrend random walks with linear trends\nLeads to spurious dynamics, as the linear trend isn’t the real “trend,” it’s merely what happened to take place. It could have been quite different and things wouldn’t look so linear if random shocks had played out differently.\nOne good way to determine the plausibility of trend stationarity around a linear trend is to examine the spectral density of the first difference of the time series around frequency zero. Per @cochraneHowBigRandom1988, trend stationary series will have a spectral density of zero at frequency zero. More realistically, trend stationary series will have low power spectrum at frequency zero, while series with significant random walk components will have a high power spectrum at frequency zero.\n\nReferences\nSpurious Periodicity in Inappropriately Detrended Time Series\nAn Exploration of Trend-Cycle Decomposition Methodologies in Simulated Data\nBeats and Misses Are Forever\nThe Universal Law of SaaS Growth\n@cochraneHowBigRandom1988\n\nany test for trend stationarity (\\sigma_{\\Delta z}^2 = 0 or S_{\\Delta y}(e^{-i0}) = 0)\n\neonline"},"Don't-rely-on-inspiration-to-write":{"title":"Don't rely on inspiration to write","links":["When-in-doubt,-write","Quantity-creates-quality","Searching-for-Outliers","Sasha-Chapin-–-Awaken-the-Writer-Within","Notes-Against-Note-Taking-Systems---by-Sasha-Chapin","tags/e","tags/online"],"tags":["e","online"],"content":"Don’t rely on inspiration to write\nDon’t wait for inspiration to strike before you put words on the page. You really don’t need inspiration, you only need to write. When in doubt, write.\nWorried that your writing isn’t any good. Well, Quantity creates quality, so don’t overoptimize over make the best possible piece each time you publish (per Searching for Outliers).\n\nReferences\nSasha Chapin – Awaken the Writer Within\nTransclude of Sasha-Chapin-–-Awaken-the-Writer-Within#4e63cd\nNotes Against Note-Taking Systems - by Sasha Chapin\nTransclude of Notes-Against-Note-Taking-Systems---by-Sasha-Chapin#4c76cf\neonline"},"Dynamic-factor-model":{"title":"Dynamic factor model","links":["Vector-autoregression","Principal-component-analysis","Autoregressive-models","@stockDynamicFactorModels2016","@kirkerWhatDrivesCore","tags/e","tags/online"],"tags":["e","online"],"content":"Dynamic factor model\nDFMs are VARs that operate over unobserved factors rather than observed variables. The unobserved factors are derived via PCA. Idiosyncratic deviations from the common component are modeled via autoregressive models.\n\nReferences\n@stockDynamicFactorModels2016\n@kirkerWhatDrivesCore\nATSA19 Lecture 8: Introduction to Dynamic Factor Analysis\nMetran\nLarge dynamic factor models, forecasting, and nowcasting\nDynamic Factor Analysis\neonline"},"Exponentials-drive-asymmetry":{"title":"Exponentials drive asymmetry","links":["Nassim-Taleb","tags/e","tags/inbox/write","tags/online"],"tags":["e","inbox/write","online"],"content":"Exponentials drive asymmetry\nPer Nassim Taleb, we should make asymmetric bets with long right tails. Even though he is very focused on the idea of (non)ergodicity, these discussions usually implicitly assume a stationary distribution, which is to say they lack a time component. It is assumed that the distribution of outcomes doesn’t change over time – it’s the same bet every single time. But in the real world the distribution of outcomes may shift over time.\nIt strikes me that one way of achieving asymmetry that may not be present in the current moment is to find intertemporal asymmetries. If you can find a bet where the expected value and the right tail of the distribution are growing fast, continually betting on that thing may yield a form of asymmetry through time. This could justify large expenditures in the present moment that might seem foolish to others but that could have high payoffs in a future, much more positively-skewed distribution. The best sort of thing to bet on along these lines is something growing exponentially in compounding fashion.\n\nReferences\newriteonline"},"Fat-tails-preclude-ergodicity":{"title":"Fat tails preclude ergodicity","links":["Sample-means-change-with-sample-size-in-fat-tailed-distributions","Interaction-generates-non-normality","Fat-Tails","Ergodicity","tags/e","tags/online"],"tags":["e","online"],"content":"Fat tails preclude ergodicity\nFat tails preclude ergodicity because estimation of the various moments of the distribution will depend upon the specific time period and will tend not to converge to stable values even with increasingly large time periods.\nFor example, a sliding window view of the realizations of a random variable over time will observe very different means across windows. Without a large realization, the mean will tend to decay, then jump up when a large value is realized. Sample means change with sample size in fat-tailed distributions\n\nThis jumpiness precludes ergodicity — the mean will be unstable through time, which eliminates the timelessness of ergodic processes.\n\nTime is what prevents everything from happening at once. To simply assume that economic processes are ergodic and concentrate on ensemble averages – and a fortiori in any relevant sense timeless – is not a sensible way for dealing with the kind of genuine uncertainty that permeates open systems such as economies. – Randomness, Fat Tails and Ergodicity – A Keynesian Perspective on Knightian Uncertainty\n\nFat tails should be expected because Interaction generates non-normality and interactions are quite common in the real world.\nIf Fat Tails should be expected and fat tails preclude ergodicity, then it follows that ergodicity should not be expected. In the real world, ergodicity is the exception, not the rule.\n\nTails should not be unexpected, for they are the rule. As the world becomes increasingly integrated – financially, economically, socially – interactions among the moving parts may make for potentially fatter tails. Catastrophe risk may be on the rise. – Randomness, Fat Tails and Ergodicity – A Keynesian Perspective on Knightian Uncertainty\n\nDespite the empirical rarity of true Ergodicity, much of financial and economic theory implicitly imply Ergodicity in its assumptions and conclusions.\n\nReferences\neonline"},"First-impression-are-high-variance":{"title":"First impression are high variance","links":["Information-is-surprisal","Volatility-is-information","tags/e","tags/online"],"tags":["e","online"],"content":"First impression are high variance\nThe idea that first impression matter can be interpreted in information theoretic terms. If Information is surprisal, variance and deviation from expectations are informative. Volatility is information, thus information sources with high variance are thus more informative than those with low variance, assuming the variance represents true signal rather than noise.\nFor first impressions to matter as much as people claim, they must be high variance. Otherwise, they wouldn’t be informative. So the statement that “first impressions matter” is simply a statement that first impressions are high variance.\nThis variance represents risk. If you want to control what people think of you, relying on making a great first impression is a very risky strategy, as it’s inherently a high variance information channel. This is partly why I write so much – I’ve found first impressions to be too unreliable and risky, and I’d rather tamp down the variance and exert more control over the impression I make on others.\n\nReferences\neonline"},"Focus-on-the-residuals":{"title":"Focus on the residuals","links":["Information-is-surprisal","Highly-likely-events-do-not-yield-much-information","residuals","Autoregressive-models","RBC","tags/e","tags/online"],"tags":["e","online"],"content":"Focus on the residuals\nFocus on what your models of the world get wrong. This is where the signal lies, and this signal can be used to update your mental models. Information is surprisal, so do not pay much attention to information sources whose output you could predict a priori. They do not yield much information (Highly likely events do not yield much information), nor are they useful in updating your models.\nFurther, many models can only be applied to residualized data, representing deviations from some expectation or trend (e.g. Autoregressive models, Real business cycles, etc).\n\nReferences\neonline"},"For-the-effectual-reasoner,-residuals-are-inputs-rather-than-outputs":{"title":"For the effectual reasoner, residuals are inputs rather than outputs","links":["Nassim-Taleb","Effectual-reasoning","Effectual-reasoning-starts-immediately-with-action-based-on-the-means-available-without-significant-planning-or-forethought","Planning-is-prediction","Why-Greatness-Cannot-Be-Planned","Errors-light-the-path","What-makes-entrepreneurs-entrepreneurial","Antifragile","The-Greatest-White-Pill-of-All","tags/e","tags/online"],"tags":["e","online"],"content":"For the effectual reasoner, residuals are inputs rather than outputs\nPer Nassim Taleb, in standard prediction contexts, errors are are exactly that: errors, problems, not good stuff, etc.\nThe effectual reasoner treats errors in the opposite way, as inputs rather than outputs, as valuable resources rather than wasteful exhaust.  Effectual reasoning starts immediately with action based on the means available without significant planning or forethought, thus they avoid prediction, which is effectively a form of planning (Planning is prediction) and just let the errors be what they are. It’s almost as if they are collecting errors (the stepping stones of Why Greatness Cannot Be Planned) along the path. The errors themselves light the path (Errors light the path).\n\nReferences\nWhat makes entrepreneurs entrepreneurial\n\nTransclude of What-makes-entrepreneurs-entrepreneurial#d5d76c\n\nAntifragile\n\nTransclude of Antifragile#7d4a7c\n\nThe Greatest White Pill of All\neonline"},"Get-leverage-on-the-fixed-cost-of-pain":{"title":"Get leverage on the fixed cost of pain","links":["Operating-leverage","tags/e","tags/online"],"tags":["e","online"],"content":"Get leverage on the fixed cost of pain\nTreat the difficulties you run into as fixed costs over which you can gain Operating leverage. If you are going to pay the cost, why not get the most out of the situation?\nA complementary idea is that you shouldn’t embark on endeavors with significant upfront costs unless you plan to exploit them to the maximum possible extent. Otherwise you miss out on the operating leverage opportunity. For so many things, the biggest gains are in the “out years” so to speak, often due to compounding.\n\nReferences\nOperating leverage\neonline"},"Goals-should-be-binomial":{"title":"Goals should be binomial","links":["variance","Discarding-details-opens-up-possibilities","tags/e","tags/online"],"tags":["e","online"],"content":"Goals should be binomial\nPrefer binary, discrete goals that don’t depend on the opinions of others.\nBinary: Two possibilities are better than many, so aim for binomial goals. Heads or tails, that’s it. This limits the variance of outcomes. Binomials are nice because they have bounded variance. Variance creates risks and uncertainty, which humans don’t do well with and can only drive manic stress. You will live happier and more stress-free if you limit your goals to things that can be distilled into binary efforts.\nDiscrete: Better to have goals with strict success criteria – you either do the thing or you don’t do the thing. Something happens or it doesn’t. Don’t attempt to achieve a particular “level” of success along some continuous spectrum. Focus on discrete objectives or tasks. “Write the essay”. “Send the email”. “Host the dinner”. This doesn’t mean quality doesn’t exist – it does – you’re just choosing not to focus on it explicitly. This doesn’t mean there aren’t intermediate steps to accomplish – there are, but they’re obvious. Discarding details opens up possibilities.\nDon’t depend on the opinions of others: Goals that depend on the opinions of others are unrealistic. Such goals are outside of your control, and create stress and worry. They require you to model the minds of others – a fraught exercise. Modeling yourself and the non-sentient world is a big enough task on its own. Limit your goals to those within your zone of control.\n\nReferences\nBinomial Distribution\n\nthe binomial distribution with parameters n and p is the discrete probability distribution of the number of successes in a sequence of n independent experiments, each asking a yes–no question, and each with its own Boolean-valued outcome: success (with probability p) or failure (with probability q = 1-p\n\neonline"},"Good-ideas-are-testable-ideas":{"title":"Good ideas are testable ideas","links":["Antifragile","Maximize-your-output-of-testable-ideas","Just-do-more","Iterative-tinkering-enhances-effective-IQ","Philosopher's-Stone","Bootstrapping-confidence","Sasha-Chapin-–-Awaken-the-Writer-Within","tags/e","tags/online"],"tags":["e","online"],"content":"Good ideas are testable ideas\nAn idea can be good in two ways. It can be good in the traditional sense, in that it comports well with reality or generates some useful output. It can also be good in the Antifragile sense - it may or may not turn out to work, but it yields some non-linear asymmetry when a certain function is applied to it. That function is testing.\nIt is in the testing of the idea that its benefits are identified. Realistically, it’s hard to know beforehand whether or not an idea is “good” in that traditional sense. Therefore, we should think of ideas as being good if they are testable, since that is the process by which their usefulness is identified. It is therefore much more robust to focus on coming up with testable ideas rather than ideas that are “good”, as that would require prediction, which is fragile.\nOnce we understand that goodness is implied by testability, it becomes obvious we should Maximize your output of testable ideas. By maximizing your output of testable ideas, you increase your odds of finding a good idea. Therefore, measure your productivity in terms of testable idea output, rather than some subjective a priori measure of idea quality. Just do more.\nTo analogize, Iterative tinkering enhances effective IQ, as does iterative idea generation and testing of those ideas. We don’t need to be “smart” in the sense of having the ability to pull good ideas out of thin air, fully-formed. We increase effective intelligence via iteration. This is the Philosopher’s Stone.\nBy focusing on testable ideas, we also Bootstrapping confidence. The goal isn’t to identify the solution to the problem in one shot. It’s to come up with enough testable hypotheses that we can have some statistical confidence in eventually finding a solution. If we trust the process and earnestly carry it out, we can reach a point of confidence much sooner in our intellectual journey.\n\nReferences\nSasha Chapin – Awaken the Writer Within\nTransclude of Sasha-Chapin-–-Awaken-the-Writer-Within#b4fff0\neonline"},"Good-representations-reduce-sample-complexity-of-downstream-tasks":{"title":"Good representations reduce sample complexity of downstream tasks","links":["sample-complexity","tags/e","tags/online"],"tags":["e","online"],"content":"Good representations reduce sample complexity of downstream tasks\n\nGood, useful representations reduce sample complexity on downstream supervised tasks. sample complexity is the number of training examples a machine learning model needs to successfully learn a target function. Therefore, good representations reduce the amount of labeled training data needed for downstream tasks to achieve a given level of performance.\nSolving a downstream supervised task with good pre-trained representations only requires samples on the order of the dimensionality of output. So binary classification tasks will only require ~2 examples during fine-tuning to “solve” the task.\n\n\nReferences\neonline"},"Granular-IV":{"title":"Granular IV","links":["Principal-component-analysis","The-Dark-Matter-of-Software-Valuations","Olley-Pakes-decomposition","@gabaixGranularInstrumentalVariables2020","@melitzDynamicOlleyPakesProductivity2015","Granular-IV","@gabaixSearchOriginsFinancial","tags/e","tags/online"],"tags":["e","online"],"content":"Granular IV\nHow to calculate granular instrumental variables\n\nRun regression of endogenous variable on exogenous variable(s) while controlling for time and unit fixed effects (alternatively, just unit effects).\nRun Principal component analysis on the residuals from the regression in (1) to extract common factors (time x components) and unit-specific loadings (components x unit), using time as the rows (samples) and the units as the columns (features) of the matrix. Can use any number of components, though two is usually good enough.\nSubtract from the residuals the dot product of factor and loadings to get the idiosyncratic shocks\nCalculate weighted average of idiosyncratic shocks, optionally excluding (weighted) shocks below certain threshold\n\nThe reason to run PCA is that you almost certainly have some omitted variables issues in the prior regressions and PCA lets you do a slightly better job in isolating the truly idiosyncratic component. I do a similar procedure in The Dark Matter of Software Valuations.\nConnection to Olley-Pakes decomposition\nGranular IV approach of Granular Instrumental Variables is quite related to decompositions of @melitzDynamicOlleyPakesProductivity2015.\nIn granular IV, size-weighted shocks are subtracted from equal weighted shocks:\nZ_t = u_{St} - u_{Et}\nIn Olley-Pakes decomposition, where overall weighted average is decomposed into equal-weighted average and covariance between the weights and things being averaged:\n\\Phi_t = \\bar{\\phi_t} + cov(s_{it}, \\phi_{it})\nThis can be re-arranged:\n\\Phi_t - \\bar{\\phi_t} = cov(s_{it}, \\phi_{it})\nThus the difference in shocks from granular IV is effectively the covariance of the shocks and the size weights. This is why there needs to be some skew in the size distribution in order for the granular IV to work — if not, the covariance between size and shocks will be zero, as there would be no variation in size weights.\nOne minor difference — when dealing with changes over time, Granular IV uses the average change while Olley-Pakes decomposition uses the change in the average.\nReferences\nGranular Instrumental Variables\nIn Search of the Origins of Financial Fluctuations: The Inelastic Markets Hypothesis\nGranular IV code per Romain Lafarguette\nScikit-learn PCA\nStatsmodels PCA\neonline"},"Granularity":{"title":"Granularity","links":["@carvalhoLargeFirmDynamics2019","Statistical-Consequences-of-Fat-Tails","Enterprise-Software-Monetization-is-Fat-Tailed","Sample-means-change-with-sample-size-in-fat-tailed-distributions","Mean-reversion","@gaubertGranularComparativeAdvantage2021","@koijenDemandSystemApproach","variance","@GranularOriginsAggregate2011","@koijenWhichInvestorsMatter2021","@FirmsDestinationsAggregate2014","@carvalhoGreatDiversificationIts2013","@gabaixGranularInstrumentalVariables2020","@gabaixSearchOriginsFinancial","@grassiWhyRiskySectors","tags/e","tags/online"],"tags":["e","online"],"content":"Granularity\nWhat it means to be granular\nGranularity can mean multiple things:\n\nHigh concentration or skewness ⇒ individual units matter\nFinite or small number of units ⇒ we live in world of pre-asymptotics\nHigh idiosyncratic volatility ⇒ units do their own thing\n\nPhenomena associated with high granularity\n\nFaster growth\n\nFat-tailedness is associated with faster growth because positive idiosyncratic shocks to large firms make the size distribution more skewed and lead to faster growth\nSee Large Firm Dynamics and the Business Cycle\n\n\nIncreasing returns to scale\n\nIn granular, fat-tailed variables, aggregating over larger quantities of underlying units increases the sample mean. Thus, a doubling of units leads to a more than doubling of the variable of interest\nSee Statistical Consequences of Fat Tails, Enterprise Software Monetization is Fat-Tailed, Sample means change with sample size in fat-tailed distributions\n\n\nMean reversion\n\nGranular/idiosyncratic shocks tend to mean revert much more quickly and forcefully\nSee Granular Comparative Advantage, A Demand System Approach to Asset Pricing\n\n\nCross-sectional variance\n\nGranularity drives larger variance across the cross-section\nSee Granular Comparative Advantage\n\n\nVolatility over time\n\nIdiosyncratic dynamics account for large share of the evolution of a variable associated with a unit over time\nSee Granular Comparative Advantage, Large Firm Dynamics and the Business Cycle\n\n\nPredictability\n\nLagged idiosyncratic shocks can be used to forecast economic variables\nSee The Granular Origins of Aggregate Fluctuations, A Demand System Approach to Asset Pricing\n\n\nConcentrated, attributable growth\n\nIf the variable in question is concentrated, growth or changes in that variable will be attributable to particular subcomponents\nSee Which Investors Matter for Equity Valuations and Expected Returns?, The Granular Origins of Aggregate Fluctuations\n\n\n\n\nReferences\nLarge Firm Dynamics and the Business Cycle\nTransclude of @carvalhoLargeFirmDynamics2019#b2f874\nFirms, Destinations, and Aggregate Fluctuations\nThe Granular Origins of Aggregate Fluctuations\nThe Great Diversification and its Undoing\nGranular Instrumental Variables\nIn Search of the Origins of Financial Fluctuations: The Inelastic Markets Hypothesis\nGranular Comparative Advantage\nWhy Risky Sectors Grow Fast\nTransclude of @gaubertGranularComparativeAdvantage2021#d2e50b\neonline"},"Hamilton-filter":{"title":"Hamilton filter","links":["@quastReliableRealTimeOutput2022","@canovaFAQHowExtract","tags/e","tags/online"],"tags":["e","online"],"content":"Hamilton filter\nProblems\nThe Hamilton filter suffers from a number of issues\n\nIt amplifies cycles of various frequencies unevenly\nThe extracted trend is noisy, rather than smooth\n\nThese flaws are made most clear via spectral analysis, a la @quastReliableRealTimeOutput2022 and @canovaFAQHowExtract:\nSpectral analysis clarifies a number of issues\n\nWithin the typical business cycle frequency range (6-32 quarters), cycles of various lengths are treated very differently\nVarious high frequencies (cycles of less than 6 quarters) are allowed through the filter\n\n\n\n\nReferences\n@canovaFAQHowExtract\n@quastReliableRealTimeOutput2022\neonline"},"Highly-likely-events-do-not-yield-much-information":{"title":"Highly likely events do not yield much information","links":["There-is-no-alpha-in-predicting-highly-probable-events","tags/e","tags/online"],"tags":["e","online"],"content":"Highly likely events do not yield much information\nThe realization of a high probability event does not reveal much net-new information about the world. Going from 90% probability to 100% is a pitiful gain. Little uncertainty is resolved by the realization of high probability events. As such, high probability events are not terribly valuable. There is no alpha in predicting highly probable events\n\nReferences\neonline"},"Ideas-generate-increasing-returns-because-they-are-nonrivalrous":{"title":"Ideas generate increasing returns because they are nonrivalrous","links":["@jonesGrowthIdeas2005","tags/e","tags/Economics","tags/Economics/Growth","tags/Economics/Macro","tags/online"],"tags":["e","Economics","Economics/Growth","Economics/Macro","online"],"content":"Ideas generate increasing returns because they are nonrivalrous\nIdeas are nonrivalrous, which means they can be used multiple times by multiple parties without fighting over some limited stock of them. Naturally, this means that when we double our other inputs, we do not need to also double ideas in order to generate at least double the output. If we do happen to increase our ideas as well, then we will get more than double the output and, therefore, increasing returns. Thus, the ability of ideas to generate increasing returns is linked to their nonrivalry.\nReferences\n\nGrowth and Ideas\n\nTransclude of @jonesGrowthIdeas2005#54c28e\nTransclude of @jonesGrowthIdeas2005#9320a7\n\n\n\neEconomicsGrowthMacroonline"},"If-you're-not-sure-something-is-getting-better,-it's-not":{"title":"If you're not sure something is getting better, it's not","links":["Fat-Tails","Today,-@Foundersfund-Led...","The-Guerrilla-Guide-to-Interviewing-(version-3.0)-–-Joel-on-Software","tags/e","tags/online"],"tags":["e","online"],"content":"If you’re not sure something is getting better, it’s not\nGood heuristic, especially in fat-tailed domains where small differences in performance don’t really matter. In such cases, if it’s not obvious you’ve made a step function improvement, you either haven’t improved or haven’t improved sufficiently for it to matter.\n\nReferences\nToday, @Foundersfund Led…\n\nTransclude of Today,-@Foundersfund-Led...#6fb3f7\n\nThe Guerrilla Guide to Interviewing (version 3.0) – Joel on Software\nTransclude of The-Guerrilla-Guide-to-Interviewing-(version-3.0)-–-Joel-on-Software#461c9c\neonline"},"In-the-long-run,-all-costs-are-variable":{"title":"In the long-run, all costs are variable","links":["MIT-14.01-Principles-of-Microeconomics,-Fall-2018","tags/e","tags/online"],"tags":["e","online"],"content":"In the long-run, all costs are variable\nReferences\nMIT 14.01 Principles of Microeconomics, Fall 2018\neonline"},"Information-is-relative":{"title":"Information is relative","links":["Nassim-Taleb","tags/e","tags/online"],"tags":["e","online"],"content":"Information is relative\nWhat is informative to someone else is not necessarily informative to you. The informativeness of a information source depends on your a priori beliefs about that source, i.e. your priors. Different priors, different information from realization.\nThis is related to the point Nassim Taleb says, that Black Swans are relative. The Black Swan for the turkey is not necessarily a Black Swan for the butcher.\n\nReferences\neonline"},"Information-is-surprisal":{"title":"Information is surprisal","links":["Highly-likely-events-do-not-yield-much-information","tags/e","tags/online"],"tags":["e","online"],"content":"Information is surprisal\nSurprises inform. Non-surprises do not. Predictable events do not yield much information via their realization. Highly likely events do not yield much information via their realization, as little uncertainty is resolved via their realization.\n\nReferences\neonline"},"Interaction-generates-non-normality":{"title":"Interaction generates non-normality","links":["correlation","i.i.d.","Fat-tails-preclude-ergodicity","tags/e","tags/online"],"tags":["e","online"],"content":"Interaction generates non-normality\nWhen there are multiple agents, reacting to each other’s behaviors, the overall behavior will not be normally distributed. This is true even if the behavior of each of the individual agents is normal.\n\nThe interactions which generate non-normalities in children’s games repeat themselves in real world systems – natural, social, economic, financial. Where there is interaction, there is non-normality. – Randomness, Fat Tails and Ergodicity – A Keynesian Perspective on Knightian Uncertainty\n\nAn extension — another way to conceive of interaction is correlation. Correlated random variables effectively “interact” with one another. If this doesn’t make sense, consider the extreme opposite — two perfectly uncorrelated random variables clearly do not “interact” in any way.\nSo an equivalent statement would be that correlation generates non-normality.\nIt turns out that, in the real world, variables are rarely i.i.d. That is to say, variables in a system are rarely independent of one another. They frequently interact and are hence dependent. This dependency generates fat tails.\nFat tails are therefore common, much more common that they would be if these interactions did not occur\n-   Tails should not be unexpected, for they are the rule. As the world becomes increasingly integrated – financially, economically, socially – interactions among the moving parts may make for potentially fatter tails. Catastrophe risk may be on the rise.\nThis is closely related to the idea that Fat tails preclude ergodicity\n\nReferences\neonline"},"Invest-in-companies-like-you-invest-in-your-career":{"title":"Invest in companies like you invest in your career","links":["heuristic","Tweets-From-Saar-Gur","tags/e","tags/online"],"tags":["e","online"],"content":"Invest in companies like you invest in your career\nA simple heuristic for testing your conviction in a potential investment is whether or not you’d be willing to join the company yourself. Are you excited enough about the opportunity that you’d bet your career on it? The answer will of course be no for the vast majority of companies.\n\nReferences\nTweets From Saar Gur\nTransclude of Tweets-From-Saar-Gur#c6e255\neonline"},"Iterative-tinkering-enhances-effective-IQ":{"title":"Iterative tinkering enhances effective IQ","links":["Just-do-more","Sasha-Chapin-–-Awaken-the-Writer-Within","tags/e","tags/online"],"tags":["e","online"],"content":"Iterative tinkering enhances effective IQ\nWith rational trial and error, you don’t need to be very smart. Experimenting via tinkering can yield strong solutions to problems, even in areas where you are unskilled or that are extremely opaque, impenetrable to logic or analysis.\nOne could think of machine learning as an example of this. Even a highly performant model is “dumb” in some fundamental way, yet it can do amazing things as a result of many iterations of trial and error. By throwing more training data at the problem (aka Just do more), one can improve a model substantially without any idea of how it works or what the optimal solution looks like a priori.\n\nReferences\nSasha Chapin – Awaken the Writer Within\nTransclude of Sasha-Chapin-–-Awaken-the-Writer-Within#b4fff0\neonline"},"Just-do-more":{"title":"Just do more","links":["When-you-don't-know-what-to-do,-either-do-a-lot-or-nothing-at-all","Sasha-Chapin-–-Awaken-the-Writer-Within","tags/e","tags/online"],"tags":["e","online"],"content":"Just do more\nWhen you don’t know what to do, either do a lot or nothing at all. Assuming you want to do something, don’t worry about exactly how much to do it. That is secondary and not something you can come to a robust viewpoint on.\n\nReferences\nSasha Chapin – Awaken the Writer Within\nTransclude of Sasha-Chapin-–-Awaken-the-Writer-Within#9336dd\neonline"},"Labor-relationships-have-debt-like-features":{"title":"Labor relationships have debt-like features","links":["tags/e","tags/inbox/write","tags/online"],"tags":["e","inbox/write","online"],"content":"Labor relationships have debt-like features\nLabor relationships have a debt-like feature in the sense that they are difficult to get rid of or reduce. Businesses are generally reluctant to lay off workers and even more reluctant to cut wages. They end up reducing hiring instead, which in a sense is like consumers choosing to avoid taking on further debt after reaching a critical level of indebtedness. This leads to economic fluctuations with some not-so-nice features, in the same way that debt leads to relatively disastrous bank failures. These are “hard” failures – things suddenly break, and drastic action must be taken to stay afloat. It’d be great if there were some other way of navigating these situations. In the banking it probably involves higher capital requirements, with an emphasis on funding assets with equity rather than debt. In the labor market, it would require forging more flexible labor relationships, even more so than the “at will” stuff that we’re so used to in certain industries.\n\nReferences\nwww.wsj.com/articles/the-fall-of-a-trucking-giant-why-yellow-is-on-the-verge-of-collapse-3724f662\newriteonline"},"Learned-representations-are-more-important-than-what-you-do-with-them":{"title":"Learned representations are more important than what you do with them","links":["Machine-Learning","NLP","TPUs","Peak","Augmenting-Long-term-Memory","With-the-right-representation,-judgement-is-cheap","tags/e","tags/online"],"tags":["e","online"],"content":"Learned representations are more important than what you do with them\nIt strikes me that most of the latest advancements in Machine Learning and specifically NLP seem to depend much more on pre-training than on fine-tuning. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding is just one example of this, going so far as to use the word “pre-training” in the title of the paper, highlighting its relative importance. The “PT” in GPT stand for pre-training, another famous example.\n\nPre-training BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding takes 4 days on 16 TPUs whereas fine-tuning takes 1 hour on a single TPU. This is a 4x16x24/1 = 1,536x difference in training time. \n\nThis suggests that coming up with a generic, globally useful representation of an input is a much more important part of the learning process than learning exactly how to leverage that representation for judgement in a particular context. Pre-training takes more time and a much larger dataset than fine-tuning.\nPeak corroborates this and notes:\nTransclude of Peak#94e758\nIn this analogy, the forests are lower dimensional representations of the trees, which are the unprocessed raw data.\nResearch into the idea of “chunking” has found that memorization of chunks of information within a specific domain can act as IQ enhancement in that domain. Someone with otherwise low horsepower can achieve the performance of a high horsepower person in that field with the right knowledge chunks.\nTransclude of Augmenting-Long-term-Memory#80aa23\nTransclude of Augmenting-Long-term-Memory#d1ba99\nThis suggests that, once the right representation of an input is achieved, most of the hard work is complete. Fine-tuning is cheap. With the right representation, judgement is cheap.\n\nReferences\neonline"},"Linearly-separable-problems-are-easy-to-solve":{"title":"Linearly separable problems are easy to solve","links":["With-the-right-representation,-judgement-is-cheap","tags/e","tags/online"],"tags":["e","online"],"content":"Linearly separable problems are easy to solve\nLinearly separable problems can be solved with relatively crude classifiers, trained with very few examples. This is because linearly separable spaces can be divided by a single hyperplane, and the number of points of data required to categorize regions around this hyperplane scales with the dimensionality of the hyperplane. So a binary classifier only requires two data points to train if the representation linearly separates the downstream task. This is why, With the right representation, judgement is cheap\n\nReferences\neonline"},"Local-projections-vs.-VARs":{"title":"Local projections vs. VARs","links":["Local-projections","Vector-autoregression","local-projections","bias","@plagborg-mollerLocalProjectionsVARs2021","@liLocalProjectionsVs2021","@miranda-agrippinoBayesianLocalProjections","@barnichonImpulseResponseEstimation2019","Impulse-responses","Hodrick-Prescott-Filter","@phillipsBoostingWhyYou2021","tags/e","tags/inbox/write","tags/online"],"tags":["e","inbox/write","online"],"content":"Local projections vs. VARs\nI’ve read quite a bit on the relative advantages of local projections and VARs. I thought I’d summarize how to select between the two methods based on their respective strengths and weaknesses.\nRun local projections when:\n\nYou care a lot about bias\nYou have lots of data (at this helps mitigate its variance weakness)\nYou have many different series (since a VAR would have too many parameters)\nYou want to control for many lags (same rationale as above)\nFor whatever reason a you don’t have access to a good VAR package\n\nIt should be noted upfront that Local Projections and VARs Estimate the Same Impulse Responses in population. So the object being estimated is exactly the same in either case, at least in theory. What differs is the characteristics of the estimator itself. Practically, the two can differ after the horizon length exceeds the lag length. The two can also differ in sample.\nIn either case it’s much better to run penalized or Bayesian versions of these estimation methods, as per Local Projections vs. VARs: Lessons From Thousands of DGPs. Bayesian VARs can be estimated with a few different publicly available packages. Bayesian Local Projections are trickier, as there doesn’t seem to be any widely available packages for running them. As an alternative, use Impulse Response Estimation by Smooth Local Projections, for which there is code available. Worst case scenario – run vanilla local projections, then smooth the impulse response with a simple smoother like the one the ships with ggplot or the Hodrick-Prescott Filter (rather, its boosted alternative: Boosting: Why You Can Use the Hp Filter).\n\nReferences\newriteonline"},"Local-projections":{"title":"Local projections","links":["@jordaLongerRunEconomicConsequences2022","@rameyGovernmentSpendingMultipliers","@rameyMacroeconomicShocksTheir2016","@oleaLocalProjectionInferencea","stationarity","autocorrelation","Vector-autoregression","VARs-in-levels-vs-differences","Impulse-responses","@chenMasteringPanelMetrics2019","@meiNickellBiasPanel2023","Local-projections-vs.-VARs","tags/e","tags/online"],"tags":["e","online"],"content":"Local projections\nGood example papers\n\nLonger-Run Economic Consequences of Pandemics\nGovernment Spending Multipliers in Good Times and in Bad: Evidence from U.S. Historical Data\nMacroeconomic Shocks and Their Propagation\n\nI’m generally finding that despite the work of Local Projection Inference is Simpler and More Robust Than You Think that for local projections you’re much better off simply using a shock that is already stationary rather than including lags as controls in order to get to effective stationarity. Relying on lags of the independent variables just hasn’t worked very well for me. A quick survey of papers shows that most of the time the shock is already stationary or is made stationary via first differencing rather than via extra lags. They do include lags, but they are lags of an already stationary variable, and I think the intent there is mainly to control for autocorrelation rather than force stationarity. \nOne thing I’ve grappled with is whether to use lags of the first difference of non-stationary variables as controls or lags of the variables in levels. The vast majority of the literature is either silent on the question or uses first differences. But similar to this discussion in the context of VARs (VARs in levels vs differences),  everything should go through just fine in levels as long as you include lags. In fact, there should hypothetically be even less concern in the context of local projections than in VARs because with LP you are only concerned with the coefficient on a single variable. Unlike a VAR, where the impulse response is a complicated function of multiple parameters, in an LP it’s just a single parameter. To be fair it’s the same parameter of a series of separate regressions, but it’s effectively just one parameter. Further, first differencing is inappropriate if the variables do not in fact have unit roots.\nSpurious local projections\nHave to be careful when running local projections to ensure that the outcome and impulse variable are stationary. If they aren’t you can get some very weird results. The issue is that the typical transformations people do are not necessarily foolproof in a wide array of situations. With macro data taking first differences or long differences is generally enough to make data stationary, so that’s what people do. But in some cases even the first differences data can trend, in which case this won’t be enough. In those situations you must detrend the data or equivalently include a time trend among the regressors\nPanel LP\nNickell bias is a pervasive issue in panel local projections and one often not accounted for, leading to results that shade toward zero. The bias is especially problematic when the cross-sectional dimension is large. Some papers that cover this problem and provide a solution include\n\n@chenMasteringPanelMetrics2019\n@meiNickellBiasPanel2023\n\nThe solution is an easy modification of the fixed effects estimator that involves running the same specification on two subsets of the data, averaging them, and the subtracting this average from two times the uncorrected fixed effects estimator.\n\nReferences\nLocal projections vs. VARs\neonline"},"Look-for-high-talent-and-high-agency":{"title":"Look for high talent and high agency","links":["Focus-on-people","Agency","Elon-Musk","Spikiness","Day-479-and-High-Agency-–-Julie-Fredrickson","Tweets-From-Julie-Fredrickson","Tweets-From-Roon","The-Guerrilla-Guide-to-Interviewing-(version-3.0)-–-Joel-on-Software","tags/e","tags/inbox/write","tags/online"],"tags":["e","inbox/write","online"],"content":"Look for high talent and high agency\nOne way to Focus on people is to identify people with both high talent and high agency.\nHigh talent is in fact reasonably common. There are lots of smart people out there, people who have some high level of skill in some area. Superstar developers, 10X employees, etc. Big tech is full of these people. There are lots of people out there with high agency, as defined as ability and orientation toward making things happen. Those people are not always working on high impact projects, but they exist nonetheless.\nHowever, the intersection of the two – high talent and high agency – is relatively rare. I know this because I meet people every day with loads of talent but without that special, Elon Musk style agency – a go-getter attitude, a willingness to break through walls, etc.\nOf all the ways in which someone can spike, talent and agency are probably the two most important.\n\nReferences\nDay 479 and High Agency – Julie Fredrickson\nTransclude of Day-479-and-High-Agency-–-Julie-Fredrickson#362d52\nTweets From Julie Fredrickson\nTransclude of Tweets-From-Julie-Fredrickson#8c23cf\nTweets From Roon\nTransclude of Tweets-From-Roon#d8f257\nThe Guerrilla Guide to Interviewing (version 3.0) – Joel on Software\nTransclude of The-Guerrilla-Guide-to-Interviewing-(version-3.0)-–-Joel-on-Software#d94133\newriteonline"},"Maximize-the-entropy-of-your-information-sources":{"title":"Maximize the entropy of your information sources","links":["Information-is-surprisal","Maximize-your-return-on-attention","Residualize-your-information-sources","Focus-on-the-residuals","tags/e","tags/online"],"tags":["e","online"],"content":"Maximize the entropy of your information sources\nInformation sources which are predictable are not very valuable because the maximum information gain from reading such sources is limited.\nThink about the information gain or surprise (Information is surprisal) from a certain information source as the ratio of certainty to your a priori prediction:\n\\text{information gain} = \\frac{\\text{certainty}}{\\text{prediction}}\nFor example, if I can predict what the New York Times is going to say on some particular issue with 90% accuracy, then I don’t gain much from finding out what they actually said. That news source is not surprising. I go from being 90% sure to 100% sure, a mere 11% gain.\nOne can think about the information gained from a news source as the return on your attention for that news source. If the information gain is low, then your return on attention for that source is low. You are rarely surprised by what this news source has to say. Information is surprisal, so by maximizing your surprisal, you maximize your information gain, and Maximize your return on attention.\nThis is related to the idea of Residualize your information sources. Imagine you have a model for certain news sources that can be framed as below:\ny = F(x) + \\varepsilon\nwhere x represents everything you know about this news source, F(\\cdot) is a function representing your internal mental model of that news source which converts what you know about it to your prediction of it, y is the actual information generated by the news source, and \\varepsilon is the error, or residual, of your predictions.\nIf you have a good model for a news source, you don’t need to actually take the time to find out what it actually said. You could guess ahead of time, take comfort in the accuracy of your predictions, and move on with your life.\nFocus on the sources of information for which your internal model can only generate poor predictions with large errors or residuals. Focus on the residuals\n\nIn general, information gain correlates with large residuals, as it implies you’re bad at predicting this source and would benefit significantly from the truth. Information is surprisal\n\n\nReferences\neonline"},"Maximize-your-output-of-testable-ideas":{"title":"Maximize your output of testable ideas","links":["Good-ideas-are-testable-ideas","Just-do-more","Philosopher's-Stone","Sasha-Chapin-–-Awaken-the-Writer-Within","tags/e","tags/online"],"tags":["e","online"],"content":"Maximize your output of testable ideas\nGood ideas are testable ideas. By maximizing your output of testable ideas, you increase your odds of finding a good idea.* Therefore, measure your productivity in terms of testable idea output, rather than some subjective a priori measure of idea quality. Just do more.\nThe best way to answer to answer a question is to come up with as many testable answers as possible, then go off and test them.\nBy doing this, we get maximum leverage out of the Philosopher’s Stone.\n\nReferences\nSasha Chapin – Awaken the Writer Within\nTransclude of Sasha-Chapin-–-Awaken-the-Writer-Within#b4fff0\nTransclude of Sasha-Chapin-–-Awaken-the-Writer-Within#30a817\neonline"},"Moving-average-model":{"title":"Moving average model","links":["Impulse-responses","Autoregressive-models","@stockIdentificationEstimationDynamic2018","tags/e","tags/inbox/write","tags/online"],"tags":["e","inbox/write","online"],"content":"Moving average model\nIn a moving average model, past shocks impact the current value of the variable directly, rather than having an indirect iterated effect that flows through past values of the observed variable. Thus you can read off the impulse response of a shock directly from the moving average coefficients.\nNotably, in a moving average model only the last q shocks affect the current value of the variable. This is in contrast to an autoregressive model where a shock impacts the variable forever.\nAs noted in Identification and Estimation of Dynamic Causal Effects in Macroeconomics Using External Instruments, the moving average model can be reached in a somewhat roundabout way by running a regression of the current value of the observed variable against current and past shocks:\n\nThus, a moving-average model is conceptually a linear regression of the current value of the series against current and previous (observed) white noise error terms or random shocks. (Wikipedia)\n\n\nReferences\nMoving-average model - Wikipedia\newriteonline"},"Network-egress-is-a-lock-in-tactic-for-public-cloud-providers":{"title":"Network egress is a lock-in tactic for public cloud providers","links":["An-App-Distribution-Network-in-Practice","Public-cloud-economics","Vendor-lock-in","tags/e","tags/online"],"tags":["e","online"],"content":"Network egress is a lock-in tactic for public cloud providers\nPublic cloud providers charge an extra fee for moving data out of their respective networks. This means that, once data is in, it becomes uneconomical to apply any services to that data that are not of the local cloud provider, as you would incur the egress cost as the data moves out. This also makes one-time migrations extremely expensive, to the point where the cost of data egress can overwhelm whatever near or long-term cost advantages the new platform might have.\nQuotes\nTransclude of An-App-Distribution-Network-in-Practice#1e0de9\nSource\nAn App Distribution Network in Practice\nPublic cloud economics // Vendor lock-in\neonline"},"Not-all-spurious-correlation-is-random":{"title":"Not all spurious correlation is random","links":["spurious-correlation","correlation","Bet-on-non-stationarity","tags/e","tags/online"],"tags":["e","online"],"content":"Not all spurious correlation is random\nPeople tend to talk about spurious correlation that drives from pure luck/randomness. Two unrelated timeseries end up correlated out of pure luck. The funny part is that this actually is a very low probability event for any two random timeseries that you pick, if they were both stationary. Sure the correlation might be somewhat positive or negative, but a 90%+ R^2 is actually quite unlikely for two completely random, stationary timeseries.\nThe thing that is actually more at fault a lot of the time is non-stationarity. Non-stationary timeseries have a tendency to be correlated between they are often trending in some direction, which tends to pull correlation away from zero mathematically. Non-stationarity is extremely common (Bet on non-stationarity).\n\nReferences\neonline"},"Observe-before-you-analyze":{"title":"Observe before you analyze","links":["With-the-right-representation,-judgement-is-cheap","Reading-Notes-The-Inner-Game-of-Tennis","tags/e","tags/online"],"tags":["e","online"],"content":"Observe before you analyze\nFocus on maximizing your skills of observation, not your skills of analysis.\nThere are two reasons for this:\n\nYou are already quite good at analysis and it tends to come naturally\nObservation is a more robust paradigm for achieving deep understanding\n\nOn the second point, remember that With the right representation, judgement is cheap, so it’s better to ensure you have the right representations of the situations you face before ruminating on the exact best next step or action to take. The best actions or solutions are often obvious once the situation is properly understood. Simply being able to accurately describe a situation is invaluable, and in many instances the process of doing so itself will illuminate nuggets of valuable information.\n\nReferences\nReading Notes The Inner Game of Tennis\nTransclude of Reading-Notes-The-Inner-Game-of-Tennis#15cac7\neonline"},"Perfect-competition-is-not-optimal-under-increasing-returns-to-scale":{"title":"Perfect competition is not optimal under increasing returns to scale","links":["@jonesGrowthIdeas2005","tags/e","tags/online"],"tags":["e","online"],"content":"Perfect competition is not optimal under increasing returns to scale\nIncreasing returns to scale causes classic models of equilibrium in perfect competition to break down. Normally, in perfect competition, factors of production are paid their marginal products. If there are increasing returns to scale,\nReferences\nGrowth and Ideas\n\nTransclude of @jonesGrowthIdeas2005#17fe4e\n\neonline"},"Planning-your-time-is-like-choosing-to-dance-on-beat":{"title":"Planning your time is like choosing to dance on beat","links":["Planning-your-time-is-scary","Deep-Habits-The-Importance-of-Planning-Every-Minute-of-Your-Work-Day","tags/e","tags/online"],"tags":["e","online"],"content":"Planning your time is like choosing to dance on beat\nPlanning your time means choosing to do all or some portion of the things you want to do on a specific schedule or cadence. The regular 15 minute, 30 minute, 1 hour, 1 day, etc. time increments are like the regular beats of a song. Every chunk of time that passes is another beat that drops. You’re not obligated to dance on beat, but it looks and feels a lot better when you do.\nMore flexibility, not less\nWhile it can initially feel constraining and even scary (Planning your time is scary), like it reduces degrees of freedom, what you invariably find is that there’s actually still a lot of flexibility, even conditional on being on beat:\nTransclude of Planning-your-time-is-scary#49b6ff\nIf anything, ingraining the idea that you should dance on beat, getting so good that it becomes natural and routine, and then mostly forgetting about it, expands the range of possibilities. Once you commit to being on beat, it doesn’t actually cost you much at “runtime”. Likewise, committing to doing things on a schedule doesn’t cost much once you get over the initial activation energy and skill required to create and stick to the schedule.\nWhat initially feel constraining eventually just becomes the air you breathe. You look with puzzled amazement at people who can’t dance on beat – they look strange and alien to you. Something is clearly “off”. Likewise, once you successful achieve intentional planning of your time, you’l look back on your prior self with amazement and wonder how you ever lived that way.\nGetting good\nIt does take work initially – it’s not a free lunch in the beginning. Ironically, what eventually will feel totally natural initially feels quite unnatural! Never fear – this too will pass. Just keep doing it, and eventually it will lock in place.\nAs with dancing, if you fall off beat, recognize that this has occurred, pause, find the beat again, and then get back on it. This cycle of losing the beat and finding it again is itself a form of training. Likewise with your work – falling off the plan, failing to plan at all, these are things that will happen with some regularity in the early days. Gently “get back on beat” – there’s no shame in it. If anything, noticing that you are off beat and coming to that conclusion yourself without the help of others is one of the greatest wins you can have in the early days. Not everyone can even do that.\n\nReferences\nDeep Habits The Importance of Planning Every Minute of Your Work Day\nTransclude of Deep-Habits-The-Importance-of-Planning-Every-Minute-of-Your-Work-Day#1fe3b8\nTransclude of Deep-Habits-The-Importance-of-Planning-Every-Minute-of-Your-Work-Day#653eb6\neonline"},"Pre-train-your-representations-before-learning-judgement":{"title":"Pre-train your representations before learning judgement","links":["Representation-Learning","Learned-representations-are-more-important-than-what-you-do-with-them","Ben-Franklin's-Autoencoder","With-the-right-representation,-judgement-is-cheap","Augmenting-Long-term-Memory","tags/e","tags/online"],"tags":["e","online"],"content":"Pre-train your representations before learning judgement\nLearning how to compress the world and represent its relevant information in the most useful way is an important skill (this is the essence of Representation Learning). Learned representations are more important than what you do with them, so focus most of your time on training your representation encoder. This is how Ben Franklin’s Autoencoder worked. This will pay dividends later, because With the right representation, judgement is cheap. Judgement will come much more naturally once you’ve pre-trained.\nWith regards to chunking per Augmenting Long-term Memory, this means that one should learn chunks first before trying to learn judgement. For example, don’t try to learn correct chess plays directly from raw information about the board state. First, learn how to better represent the state of the board. What to do will be much more obvious once this basic information has been correctly encoded. \n\nReferences\neonline"},"Prediction-is-compression,-compression-is-expression":{"title":"Prediction is compression, compression is expression","links":["tags/e","tags/online"],"tags":["e","online"],"content":"Prediction is compression, compression is expression\nRelating some ideas that I’ve been thinking about lately regarding the relationship between one’s mental representations, skill development, and expression.\nPrediction allows you to express a seemingly large amount of information in a smaller way via the parameters of whatever model you are using to predict.\nPerfect prediction is analogous to lossless compression, while imperfect prediction corresponds to lossy compression.\n\nSo whenever you have any sort of way of better predicting the future than chance, it corresponds to a better data compression algorithm for that type of data, where you store the same amount of information in your bits. – Prediction = Compression\n\n\nReferences\neonline"},"Principal-component-analysis":{"title":"Principal component analysis","links":["@baiPanicAttackUnit","@barigozziLargedimensionalDynamicFactor2021","@barigozziInferenceHeavyTailedNonstationary2022","@covarrubiasGoodBadConcentration2019","tags/e","tags/online"],"tags":["e","online"],"content":"PCA\nNon-stationary data\nWhen dealing with non-stationary data, you can take multiple approaches to PCA\n\nDifference the data, run PCA, then accumulate the estimated factors (@baiPanicAttackUnit)\nDifference the data, run PCA, apply the loading matrix to linearly detrended data (@barigozziLargedimensionalDynamicFactor2021)\nLinearly detrend the data, run PCA on the covariance matrix of the detrended data, apply the loading matrix to the detrended data (@barigozziInferenceHeavyTailedNonstationary2022)\n\nIn any case, you must standardize the data first before running PCA.\n\nReferences\n@covarrubiasGoodBadConcentration2019\nPrincipal Components Analysis and Factor Analysis\neonline"},"Random-walk":{"title":"Random walk","links":["Stochastic-trend","tags/e","tags/online"],"tags":["e","online"],"content":"Random walk\nHelpful to think about a random walk as the ultimate version of a stochastic trend, in the sense that any shocks to the time series are fully incorporated into the “trend” of the series. The trend is effectively the running cumulative sum of the shocks.\n\nReferences\neonline"},"Under-fat-tails,-look-for-quick-reasons-to-say-yes":{"title":"Under fat tails, look for quick reasons to say yes","links":["Searching-for-Outliers","Maybe-Your-Dating-Preferences-Are-Stupid","Fat-Tails","Bet-on-positively-fat-tailed-opportunities","Looking-for-the-Spike-in-a-Haystack","You-likely-haven't-seen-the-best-or-worst","Product-Market-Fit-is-Lindy","Craftspeople-How-to-Create-Good-Work","The-Case-for-American-Seriousness---by-Katherine-Boyle","tags/e","tags/online"],"tags":["e","online"],"content":"Under fat tails, look for quick reasons to say yes\nThis note summarizes my thoughts and learnings from a number of interesting articles:\n\nSearching for Outliers\nMaybe Your Dating Preferences Are Stupid\n\nFat Tails are weird in the sense that the flip the standard logic of optimal evaluation on its head, often necessitating the exact opposite behavior from what one might typically default to. Evaluation routines that make complete sense in a thin-tailed world are completely wrong in the realm of fat tails.\nFor example, in the realm of thin tails, it’s possible to, with a moderate amount of data, reach a fairly precise estimate of the importance of various characteristics to some outcome. With fat tails, there’s no amount of data within reason that will get you to precise parameter values.\nFurther, we must be vary careful about over-relying on these estimates, as the cost of saying no to a potentially very good thing could be quite high. One should Bet on positively fat-tailed opportunities, which necessitates suspension of disbelief for a moment. Humility is key – we don’t really know what we think we know, so don’t bother trying to use what we don’t actually know that well to eliminate potentially incredible options.\nWe should look for reasons to say yes, rather than reasons to say no. Avoid checklists. Almost any amazing opportunity will fail some number of requirements on a naive checklist. Overuse of checklist leads to only picking “well-rounded” options, which might have a decent chance of being better than average but have a poor chance of being exceptional. I see this in so many of my peers, as one example, but it applies in many places.\nIt’s fine to have dealbreakers, but they should be few in numbers. Use them sparingly, and only use the ones you have incredible confidence in. Be quick in your decisions, but don’t be fickle.\nFocus more of your efforts on finding reasons to say yes. What are the characteristics that, if present, could lead to an exceptional outcome? Look for the presence of these characteristics. The best option will spike in some particular way (Looking for the Spike in a Haystack), but may be mediocre in many other ways. Pass on opportunities because they lack these spiky characteristics, rather than because they have (or don’t have) some other, moderately bad (good) characteristics. Rule things in, rather rule things out.\nDon’t evaluate each option in isolation, from a blank page. This is likely to be error prone and exhausting at a minimum. Place each decision in the context of past decisions made by yourself and the collective experiences of others. Does this item look like the others that have done well? What are the shared, potentially exceptional characteristics?\nOnce you know what you want, be ruthless and quick in evaluating opportunities. Don’t dwell. You need to take many samples to arrive at a potential “high draw” from a positively fat-tailed distribution. Overindexing or optimizing over a single or small numbers of draws is a waste of time, as measured in terms of the opportunity cost of what you could have if you were only to draw more samples.\nRemember, You likely haven’t seen the best or worst, so if something is clearly far from the optimal outcome, you should cut bait quickly and feel no guilt in doing so. Similar to product-market fit, “fit” that isn’t found quickly is likely to never materialize (Product-Market Fit is Lindy), so don’t beat a dead horse. Get a move on!\n\nReferences\nYou likely haven’t seen the best or worst\nCraftspeople How to Create Good Work\nTransclude of Craftspeople-How-to-Create-Good-Work#23bc81\nThe Case for American Seriousness - by Katherine Boyle\nTransclude of The-Case-for-American-Seriousness---by-Katherine-Boyle#abc2eb\neonline"},"Under-fat-tails,-outliers-drive-movements-in-the-mean":{"title":"Under fat tails, outliers drive movements in the mean","links":["Fat-Tails","@GranularOriginsAggregate2011","Volatility-is-information","2022-01-10","Weighted-ACV","tags/e","tags/online"],"tags":["e","online"],"content":"Under fat tails, outliers drive movements in the mean\nThe law of large numbers fails when dealing with Fat Tails. Thus the influence of outliers is insufficiently diluted by their probably of occurrence. This means they move the mean when they occur. This is important, as many important economic variables aggregate fat tailed components (which is not always obvious, as in the case of inflation). The idiosyncratic fluctuations (The Granular Origins of Aggregate Fluctuations) of these components is often large enough that most of the period to period movement is driven by the outliers rather than the median component. The median could be flat over time, yet the weighted mean will bounce around. Thus, Volatility is information about the underlying components.\n\nReferences\n2022-01-10\nTransclude of 2022-01-10#406215\nWeighted ACV\neonline"},"Under-fat-tails,-unbiasedness-is-overrated":{"title":"Under fat tails, unbiasedness is overrated","links":["Fat-Tails","tags/e","tags/online"],"tags":["e","online"],"content":"Under fat tails, unbiasedness is overrated\nUnder fat tails, or situations with extremely high variance, being unbiased isn’t that helpful. The underlying variance will destroy you on a MSE basis. Better to be somewhat biased if it helps you significantly reduce variance.\n\nReferences\neonline"},"Use-TK-liberally":{"title":"Use TK liberally","links":["Write-furious-first-drafts","Writing-Wednesdays-The-Magic-of-TK","tags/e","tags/online"],"tags":["e","online"],"content":"Use TK liberally\nUse TK whenever possible during the drafting phase of writing, which will help you keep momentum and get out the first draft as quickly as possible (Write furious first drafts). Whenever it isn’t clear how to proceed, simply drop in a “TK” and move on with haste. This can even be used for evergreen notes, which can always be finished later and certainly do not need to be finished in one sitting.\n\nReferences\nWriting Wednesdays The Magic of TK\nTransclude of Writing-Wednesdays-The-Magic-of-TK#d717c1\neonline"},"Vector-autoregression":{"title":"Vector autoregression","links":["Dealing-with-nonstationarity","tags/e","tags/online"],"tags":["e","online"],"content":"Vector autoregression\nA structural VAR is simply one where the errors are uncorrelated with one another across the various included time series.\nDealing with nonstationarity\n\n”VAR models are usually guided by theory and are dynamic in nature, so you do not have to worry about the nonstationarity.” (Source)\n“There is an issue of whether the variables in a VAR need to be stationary. Sims (1980) and Sims, Stock and Watson (1990) recommend against differencing even if the variables contain a unit root. They argued that the goal of a VAR analysis is to determine the interrelationships among the variables, not to determine the parameter estimates. The main argument against differencing is that it “throws away” information concerning the comovements in the data (such as the possibility of cointegrating relationships). Similary, it is argued that the data need not be detrended. In a VAR, a trending variable will be well approximated by a unit root plus drift. However, majority view is that the form of variables in the VAR should mimic the true data-generating process. This is particularly true if the aim is to estimate a structural model.” (Source)\n\nOptions for Bayesian VAR\n\nPyFlux\npybvar\nPyMC\nLargeBvarPython\n\n\nReferences\nImpulse-Response Functions for VARs\nBayesian Vector Autoregression in PyMC\nBayesian vector autoregression models\nSome good notes:\n\n\nwww.eco.uc3m.es/~jgonzalo/teaching/timeseriesma/watsonvarnotes-reading.pdf\n\nVECM\n\neonline"},"Volatility-is-information":{"title":"Volatility is information","links":["Nassim-Taleb","Antifragile","Volatility-is-information","Information-is-surprisal","Surprise-as-general-notion-of-volatility","Effectual-reasoning","For-the-effectual-reasoner,-residuals-are-inputs-rather-than-outputs","tags/e","tags/online"],"tags":["e","online"],"content":"Volatility is information\nI’ve definitely seen this referred to somewhere other than my own notes.\nNassim Taleb is very likely a good reference source here. If Antifragile is things that gain from volatility, then if Volatility is information, Information is surprisal, and Surprise as general notion of volatility, then things that gain from information or surprise are Antifragile.\nEffectual reasoning is Antifragile because For the effectual reasoner, residuals are inputs rather than outputs, thus Effectual reasoning gains from residuals/surprises, as they lead to better output in the form of better goals or lightweight plans for reaching those goals.\n\nReferences\nAntifragile\n\nTransclude of Antifragile#7d4a7c\n\neonline"},"When-in-doubt,-don't-difference":{"title":"When in doubt, don't difference","links":["stationarity","Hamilton-filter","Hodrick-Prescott-Filter","@canovaFAQHowExtract","tags/e","tags/online"],"tags":["e","online"],"content":"When in doubt, don’t difference\nIn my experience, differencing data to achieve stationarity is a fraught exercise. It destroys a significant amount of information, and only makes sense when the data is a true random walk or close to it. If you have any doubt that the data is in fact a random walk, I would be extremely careful about differencing it.\nThis is especially true in the case of multivariate series in which there may be cointegration. Differencing such series destroys the long-run equilibrium / correction behavior in the data, leading to poor inference and inferences.\nI would also be careful about differencing when you either don’t have much data or you know there’s likely a lot of measurement noise in the data. When you difference such data, you only amplify whatever noise is already present, to the point where the noise can overwhelm the signal. I don’t know a good rule of thumb, but I’d be wary of differencing when you have fewer than 100 datapoints.\nWhen in doubt, use another method, like polynomial detrending or a more sophisticated method like the Hamilton filter or HP filter. Per @canovaFAQHowExtract, polynomial detrending is actually quite robust.\n\nReferences\n\nwhen cointegration may be present, simply getting rid of nonstationarity by differencing individual series so that they are all stationary throws away vast amounts of information and may distort inference. – Christopher Sims, Comments and discussion: Disentangling the channels of the 2007–2009 recession\n\neonline"},"When-unskilled,-complicate-the-game-and-add-randomness":{"title":"When unskilled, complicate the game and add randomness","links":["The-Greatest-White-Pill-of-All","Tweets-From-vitalik.eth","tags/e","tags/online"],"tags":["e","online"],"content":"When unskilled, complicate the game and add randomness\nBy complicating the game, you create situation where a strong opponent has to compete with you on multiple overlapping dimensions. While your competitors might be strong along one or a few lines of skill, they are unlikely to be extremely competent at a complex combination of many skills. Thus, the skill distribution for multivariable competition is much narrower, giving you better odds of coming out on top.\nRandomness helps you when you have a high probability of losing anyway. Since the probability of losing is bounded at 100%, additional randomness generally only helps you, as it opens you up to positive tails that can lead to a win that you wouldn’t have otherwise been exposed to.\n\nReferences\nThe Greatest White Pill of All\nTransclude of The-Greatest-White-Pill-of-All#604d0a\nTransclude of The-Greatest-White-Pill-of-All#c52699\nTweets From vitalik.eth\nTransclude of Tweets-From-vitalik.eth#85c681\neonline"},"With-the-right-representation,-judgement-is-cheap":{"title":"With the right representation, judgement is cheap","links":["Learned-representations-are-more-important-than-what-you-do-with-them","Chunks-are-low-dimensional-representations","Cal-Newport","Deep-Work","Exploring-the-Limits-of-Transfer-Learning-with-a-Unified-Text-to-Text-Transformer","Pre-train-your-representations-before-learning-judgement","Prediction-Machines","tags/e","tags/online"],"tags":["e","online"],"content":"With the right representation, judgement is cheap\nWe can think about representation learning as the pre-training phase of model development whereas “judgement learning” corresponds to the fine-tuning phase.\nIt appears that pre-training of NLP models to achieve proper representations for words and sequences is more important to the the overall training regimen than fine-tuning on specific tasks. Learned representations are more important than what you do with them.\nA tell-tale sign of mastery is increasingly complex chunks. These chunks enable higher-level thinking with a fixed amount of compute horsepower, as one only needs to reason about the higher-level chunks rather than the raw information. This reduces perceived complexity and compresses information.\n\nA beginner would see “a pawn here, a rook there”, and so on, a series of individual pieces. Masters, by contrast, saw much more elaborate “chunks”: combinations of pieces that they recognized as a unit, and were able to reason about at a higher level of abstraction than the individual pieces.\nSimon estimated chess masters learn between 25,000 and 100,000 of these chunks during their training, and that learning the chunks was a key element in becoming a first-rate chess player. Such players really see chess positions very differently from beginners.\n\nChunks are analogous to representations. Chunks are low-dimensional representations. Memorizing chunks is thus similar to learning representations.\nCal Newport considers Deep Work to be tasks that you couldn’t train a college-educated person to do in a short period of time. In this sense he relates necessary training time directly to the value of the work. If we think about training time as a “cost” then it could be said that learning good judgement (fine-tuning) is cheap relative to learning useful representations (pre-training). The most valuable work or training regimen should be that which is most costly in terms of training time. If we extend this to the machine learning context, learning representations is the “Deep Work” of machine learning, whereas learning judgement is relatively shallow work (once representations have been learned).\nTransclude of Learned-representations-are-more-important-than-what-you-do-with-them#757870\nThe notion that judgement is cheap is corroborated by the architectures of pre-trained NLP models (BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding), which oftentimes only require a single layer or shallow feed forward network to turn the input representation into some required task output. This is often overlooked but in some ways is an amazing result — with the right representation, you need almost no additional computation to successfully complete a wide range of difficult tasks.\n\nOur task-speciﬁc models are formed by incorporating BERT with one additional output layer, so a minimal number of parameters need to be learned from scratch. – BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n\nAccording to Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, fine-tuning a model from scratch (which isn’t really fine-tuning) results in much worse performance than with pre-training before fine-tuning. Fine-tuning is thus only cheap when the model has learned the right representation first. Judgement is cheap when equipped with the right representation, otherwise it’s quite expensive.\nExpanding this outside the machine learning context, this idea would imply that attempting to learn judgement directly from the data itself (yours or others’ experiences) without first pre-processing the data or “pre-training” yourself is inefficient and unlikely to work. Pre-train your representations before learning judgement. In doing so, be careful not to fool yourself into thinking something is valuable simply because it takes a long time.\nTransclude of Pre-train-your-representations-before-learning-judgement#6bef51\nThis differs markedly from the main thesis of Prediction Machines, which is that machine learning models mainly concern themselves with prediction and that judgement remains within the domain of humans. The authors also predict that judgement will rise in value given its complementary with prediction.\n\nBut if judgement is in fact cheap once you are good at generating representations, I’m not so sure that humans will remain valuable in this way.\nI could be misreading this though. Perhaps they don’t think of being able to generate Q&amp;A answers as judgement but really prediction. There still needs to be a human in the loop to know what to do with those predictions and make decisions. I’m not sure about this.\n\n\n\nReferences\neonline"},"agency":{"title":"agency","links":["Agency-is-doing-what-is-needed-to-achieve-your-goals","Nassim-Taleb","Emmett-Shear","Tweets-From-Kat-Woods","A-system-for-agency.canvas","Seven-ways-to-become-unstoppably-agentic","How-to-Develop-Your-Sense-of-Agency","tags/e","tags/inbox/write"],"tags":["e","inbox/write"],"content":"Agency\nAs I embark on this journey to develop more agency, I thought I’d write down some thoughts on what the concept means to me. At a meta level, the process of writing down my thoughts is itself part of demonstrating agency, so this exercise is productive even if I don’t refer back to these notes.\nFirst – what is agency? Per Neel Nanda, Agency is doing what is needed to achieve your goals.\nThat is helpful, but it’d be even more helpful to more fully spec out a model for what agency looks like in practice. That would give a richer sense of what agency is than that that pithy definition.\nHere’s what agency looks like in practice:\n\nSetting a hyperprior that you are capable of generating plausible plans, executing on them, correcting their flaws, and beginning again, and that this process will eventually yield successful results\nRealizing humans underestimate low probability events (per Nassim Taleb) and overestimate high probability events\n\nIn response to this bias, one upgrades their estimation of “low chance” plans and downgrades their estimation of “sure things”\nEspecially important heuristic for complex, difficult problems\nImplicitly acknowledges that life is more chaotic than it seems\n\n\nMaking plans to solving problems and achieving goals\n\nPlans need to merely be plausible (“this could work”) and executable, no more\nAsking the question – “What’s the stupidest and easiest one thing you could do to make even a little progress?” (h/t Emmett Shear)\nMaking multiple plans is good, even “bad” plans (“the dumbest plans that could possibly work” per Emmett Shear)\nWhen unexpected problems pop up, making deterministic plans to address them rather than fretting over the fact of the problem or idly ruminating on them or their associated difficulties, required effort, sources, etc\nPlans should include a detailed problem statement,  incorporate your goals, and anticipate likely obstacles\nAvoiding performative activity without any associated theory or story for why these behaviors will lead to success (full connection between current actions, plans, goals, and states of the world)\n\n\nSensitivity to the things one values or truly enjoys, rather than what one is “supposed” to like or seek\nTaking action without regard to the probability of success\n\nBeing overly sensitive to success probabilities opens up the risk of “psyching yourself out” and reduces action\n\n\nWhen it’s cheap, taking quick action to rectify issues or optimize situations to your liking (per Tweets From Kat Woods)\n\nRelated: avoiding status quo bias, or not letting the way things currently are be a lazy justification for accepting current conditions and allowing them to persist\n\n\nDynamically correcting plans and actions in response to real world feedback\n\nThis should be seen as an inevitable, even joyous part of the process rather than a failure or burden\n\n\nBeing willing to fail\n\nBeing too tied to success both curtails action and cuts the odds of success\n\n\nOnce plans are made, asking for help\n\nAssistance from others can help supercharge one’s plans, giving you a leg up right from the star\nMaking lists of people who can either help you directly, or can refer you to people who can help\nBeing shameless in asking for help and making it clear you want help\nAsking for help without regard to the costs you could be imposing on the help-giver\n\n\n\nHaving decided what agency is, it’s helpful to turn this into a schematic for what A system for agency could look like:\nTransclude of A-system-for-agency.canvas\nNext, having defined agency both in words and schematically, one needs a system for operationalizing agency in daily life. An easy place to start is an “agency tracker” – a system for tracking agentic behavior on a daily basis. The goal is to fill up the tracker as much as possible each day, taking every opportunity to exhibit agentic behavior and nothing such instances, similarly to how I track “Wins” in Obsidian.\n\nReferences\nSeven ways to become unstoppably agentic\ntwitter.com/eshear/status/1740059627788915050\nHow to Develop Your Sense of Agency\nTransclude of How-to-Develop-Your-Sense-of-Agency#45100d\nHow to increase your agency: a flowchart\nAgency: accepting the world, noticing paths to your goals, noticing what your goals are\n\nYou have a problem: are you working on it?\n\nNo: Are you in a victim mindset?\n\nYes: Can you answer these questions?\n\nWhat if it were possible?\nWhat’s the stupidest, easiest thing you could do to make even a little bit of progress?\nWhy are you so sure you won’t succeed?\n\n\nNo: Can you answer these questions?\n\n(now) What are you doing right now?\n(goal) What is a detailed description of the world after you’ve succeeded? What does success look like? What are you actually trying to do? In 12 months, what would you like to be celebrating with a friend?\n(why) Why do you want to do that?\n(problem) What is the roadblock? What is the problem in detail? What might make you procrastinate?\n(solutions) What are some ideas that could possibly work?\n(plan) Given the best, easiest, or most liked idea, what’s your rough draft of a plan?\n(next step) What’s the immediate next step?\n(help) Who or what could help fill in the gaps?\n(environment) How could you set up your social context and environment to\n\nbolster your motivation, and\navoid frustrations and temptations?\n\n\n\n\n\n\nYes: Can you answer these questions?\n\nWhat is a detailed description of the world after you’ve succeeded?\nWhat’s the connection between what you’re doing now, your plan, and the goal?\n\n\n\n\n\nRemember:\npeople tend to\n\nunderestimate the likelihood of success for “bad” plans that could work\noverestimate the likelihood of success for sure-thing plans (day-to-day is usually more chaotic than we expect)\n\nFree yourself from the requirement that your ideas must be good. All that matters is that they’re possible. It could work.\newrite"},"index":{"title":"Nnamdi's Notes","links":[],"tags":[],"content":"Home\nThese are my personal notes on various topics that don’t meet the bar for a proper essay. Most are not even in essay form — they are scratchings and inklings of ideas or notes to myself about things I’ve been researching or learning about.\nPeople often ask for my sources of inspiration or about my “writing process”. These notes give some sense of how I end up with these ideas."},"residuals":{"title":"residuals","links":["tags/e"],"tags":["e"],"content":"residuals\n\nReferences\ne"},"self-supervised-learning":{"title":"self-supervised learning","links":["self-supervised-learning","unsupervised-learning","Representation-Learning","Transfer-learning","tags/e","tags/online"],"tags":["e","online"],"content":"self-supervised learning\nself-supervised learning is a form of unsupervised learning where the data itself provides the supervision without access to handmade training labels.\nIn general, withhold some part of the data and task the model with predicting the masked portion. The model generates a useful representation (Representation Learning) of the input data via its parameters which can then be used in some downstream task. This is an example of Transfer learning.\nThe loss of the self-supervised learning task serves as a proxy for the downstream task that the model will be fine-tuned on.\n\nReferences\neonline"},"variance":{"title":"variance","links":["tags/e","tags/inbox/write"],"tags":["e","inbox/write"],"content":"variance\n\nReferences\newrite"}}