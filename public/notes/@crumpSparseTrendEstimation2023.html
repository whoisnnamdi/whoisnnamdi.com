<!DOCTYPE html>
<html lang="en"><head><title>Sparse Trend Estimation</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=IBM Plex Mono&amp;family=Schibsted Grotesk:wght@400;700&amp;family=Source Sans Pro:ital,wght@0,400;0,600;1,400;1,600&amp;display=swap"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:title" content="Sparse Trend Estimation"/><meta property="og:description" content="Sparse Trend Estimation Richard K. Crump, Nikolay Gospodinov, Hunter Wieman – 2023 Abstract The low-frequency movements of many economic variables play a prominent role in policy analysis and decision-making."/><meta property="og:image" content="https://whoisnnamdi.com/notes/static/og-image.png"/><meta property="og:width" content="1200"/><meta property="og:height" content="675"/><link rel="icon" href="./static/icon.png"/><meta name="description" content="Sparse Trend Estimation Richard K. Crump, Nikolay Gospodinov, Hunter Wieman – 2023 Abstract The low-frequency movements of many economic variables play a prominent role in policy analysis and decision-making."/><meta name="generator" content="Quartz"/><link href="./index.css" rel="stylesheet" type="text/css" spa-preserve/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css" rel="stylesheet" type="text/css" spa-preserve/><script src="./prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch("./static/contentIndex.json").then(data => data.json())</script></head><body data-slug="@crumpSparseTrendEstimation2023"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h1 class="page-title"><a href=".">Nnamdi's Notes</a></h1><div class="spacer mobile-only"></div><div class="search"><div id="search-icon"><p>Search</p><div></div><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search</title><desc id="desc">Search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></div><div id="search-container"><div id="search-space"><input autocomplete="off" id="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div id="search-layout" data-preview="true"></div></div></div></div><div class="darkmode"><input class="toggle" id="darkmode-toggle" type="checkbox" tabindex="-1"/><label id="toggle-label-light" for="darkmode-toggle" tabindex="-1"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg></label><label id="toggle-label-dark" for="darkmode-toggle" tabindex="-1"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></label></div><div class="explorer desktop-only"><button type="button" id="explorer" data-behavior="collapse" data-collapsed="collapsed" data-savestate="true" data-tree="[]"><h1>All Notes</h1><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="5 8 14 8" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><div id="explorer-content"><ul class="overflow" id="explorer-ul"><li><div class="folder-outer open"><ul style="padding-left:0;" class="content" data-folderul><li><div class="folder-outer "><ul style="padding-left:0;" class="content" data-folderul></ul></div></li><li><a href="./@dubeLocalProjectionsApproach" data-for="@dubeLocalProjectionsApproach">A Local Projections Approach to Difference-in-Differences Event Studies</a></li><li><a href="./@gouletcoulombeNeuralPhillipsCurve2022" data-for="@gouletcoulombeNeuralPhillipsCurve2022">A Neural Phillips Curve and a Deep Output Gap</a></li><li><a href="./agency" data-for="agency">agency</a></li><li><a href="./@hodrickExplorationTrendCycleDecomposition2020" data-for="@hodrickExplorationTrendCycleDecomposition2020">An Exploration of Trend-Cycle Decomposition Methodologies in Simulated Data</a></li><li><a href="./Autoregressive-models" data-for="Autoregressive-models">Autoregressive models</a></li><li><a href="./Ben-Franklin's-Autoencoder" data-for="Ben-Franklin's-Autoencoder">Ben Franklin's Autoencoder</a></li><li><a href="./Beveridge-Nelson-Decomposition" data-for="Beveridge-Nelson-Decomposition">Beveridge-Nelson Decomposition</a></li><li><a href="./@phillipsBoostingWhyYou2021" data-for="@phillipsBoostingWhyYou2021">Boosting: Why You Can Use the Hp Filter</a></li><li><a href="./Career-success-and-career-capital-are-cointegrated" data-for="Career-success-and-career-capital-are-cointegrated">Career success and career capital are cointegrated</a></li><li><a href="./Careers-are-a-random-walk" data-for="Careers-are-a-random-walk">Careers are a random walk</a></li><li><a href="./Change-leads-to-insight-far-more-often-than-insight-leads-to-change" data-for="Change-leads-to-insight-far-more-often-than-insight-leads-to-change">Change leads to insight far more often than insight leads to change</a></li><li><a href="./Correlation-between-prices-and-quantities-reveals-main-market-driver" data-for="Correlation-between-prices-and-quantities-reveals-main-market-driver">Correlation between prices and quantities reveals main market driver</a></li><li><a href="./Diffusion-models" data-for="Diffusion-models">Diffusion models</a></li><li><a href="./Don't-detrend-random-walks-with-linear-trends" data-for="Don't-detrend-random-walks-with-linear-trends">Don't detrend random walks with linear trends</a></li><li><a href="./Don't-rely-on-inspiration-to-write" data-for="Don't-rely-on-inspiration-to-write">Don't rely on inspiration to write</a></li><li><a href="./@vivianoDynamicCovariateBalancing2023" data-for="@vivianoDynamicCovariateBalancing2023">Dynamic covariate balancing: Estimating treatment effects over time with potential local projections</a></li><li><a href="./Dynamic-factor-model" data-for="Dynamic-factor-model">Dynamic factor model</a></li><li><a href="./@stockDynamicFactorModels2016" data-for="@stockDynamicFactorModels2016">Dynamic Factor Models, Factor-Augmented Vector Autoregressions, and Structural Vector Autoregressions in Macroeconomics</a></li><li><a href="./@plagborg-mollerEssaysMacroeconometrics" data-for="@plagborg-mollerEssaysMacroeconometrics">Essays in Macroeconometrics</a></li><li><a href="./Exponentials-drive-asymmetry" data-for="Exponentials-drive-asymmetry">Exponentials drive asymmetry</a></li><li><a href="./@canovaFAQHowExtract" data-for="@canovaFAQHowExtract">FAQ: How do I extract the output gap?</a></li><li><a href="./Fat-tails-preclude-ergodicity" data-for="Fat-tails-preclude-ergodicity">Fat tails preclude ergodicity</a></li><li><a href="./First-impression-are-high-variance" data-for="First-impression-are-high-variance">First impression are high variance</a></li><li><a href="./Focus-on-the-residuals" data-for="Focus-on-the-residuals">Focus on the residuals</a></li><li><a href="./For-the-effectual-reasoner,-residuals-are-inputs-rather-than-outputs" data-for="For-the-effectual-reasoner,-residuals-are-inputs-rather-than-outputs">For the effectual reasoner, residuals are inputs rather than outputs</a></li><li><a href="./@gorodnichenkoForecastErrorVariance2020" data-for="@gorodnichenkoForecastErrorVariance2020">Forecast Error Variance Decompositions with Local Projections</a></li><li><a href="./@bardsenForecastingLevelsLog2011" data-for="@bardsenForecastingLevelsLog2011">Forecasting levels of log variables in vector autoregressions</a></li><li><a href="./@atheyGeneralizedRandomForests2018" data-for="@atheyGeneralizedRandomForests2018">Generalized Random Forests</a></li><li><a href="./Get-leverage-on-the-fixed-cost-of-pain" data-for="Get-leverage-on-the-fixed-cost-of-pain">Get leverage on the fixed cost of pain</a></li><li><a href="./Goals-should-be-binomial" data-for="Goals-should-be-binomial">Goals should be binomial</a></li><li><a href="./Good-ideas-are-testable-ideas" data-for="Good-ideas-are-testable-ideas">Good ideas are testable ideas</a></li><li><a href="./Good-representations-reduce-sample-complexity-of-downstream-tasks" data-for="Good-representations-reduce-sample-complexity-of-downstream-tasks">Good representations reduce sample complexity of downstream tasks</a></li><li><a href="./Granular-IV" data-for="Granular-IV">Granular IV</a></li><li><a href="./Granularity" data-for="Granularity">Granularity</a></li><li><a href="./@jonesGrowthIdeas2005" data-for="@jonesGrowthIdeas2005">Growth and Ideas</a></li><li><a href="./Hamilton-filter" data-for="Hamilton-filter">Hamilton filter</a></li><li><a href="./Highly-likely-events-do-not-yield-much-information" data-for="Highly-likely-events-do-not-yield-much-information">Highly likely events do not yield much information</a></li><li><a href="./@cochraneHowBigRandom1988" data-for="@cochraneHowBigRandom1988">How Big Is the Random Walk in GNP?</a></li><li><a href="./@haddadHowCompetitiveStock2021" data-for="@haddadHowCompetitiveStock2021">How Competitive is the Stock Market? Theory, Evidence from Portfolios, and Implications for the Rise of Passive Investing</a></li><li><a href="./@greenwaldHowWealthWas" data-for="@greenwaldHowWealthWas">How the Wealth Was Won: Factor Shares as Market Fundamentals</a></li><li><a href="./Ideas-generate-increasing-returns-because-they-are-nonrivalrous" data-for="Ideas-generate-increasing-returns-because-they-are-nonrivalrous">Ideas generate increasing returns because they are nonrivalrous</a></li><li><a href="./@stockIdentificationEstimationDynamic2018" data-for="@stockIdentificationEstimationDynamic2018">Identification and Estimation of Dynamic Causal Effects in Macroeconomics Using External Instruments</a></li><li><a href="./If-you're-not-sure-something-is-getting-better,-it's-not" data-for="If-you're-not-sure-something-is-getting-better,-it's-not">If you're not sure something is getting better, it's not</a></li><li><a href="./@barnichonImpulseResponseEstimation2019" data-for="@barnichonImpulseResponseEstimation2019">Impulse Response Estimation by Smooth Local Projections</a></li><li><a href="./In-the-long-run,-all-costs-are-variable" data-for="In-the-long-run,-all-costs-are-variable">In the long-run, all costs are variable</a></li><li><a href="./@barigozziInferenceHeavyTailedNonstationary2022" data-for="@barigozziInferenceHeavyTailedNonstationary2022">Inference in Heavy-Tailed Nonstationary Multivariate Time Series</a></li><li><a href="./Information-is-relative" data-for="Information-is-relative">Information is relative</a></li><li><a href="./Information-is-surprisal" data-for="Information-is-surprisal">Information is surprisal</a></li><li><a href="./Interaction-generates-non-normality" data-for="Interaction-generates-non-normality">Interaction generates non-normality</a></li><li><a href="./Invest-in-companies-like-you-invest-in-your-career" data-for="Invest-in-companies-like-you-invest-in-your-career">Invest in companies like you invest in your career</a></li><li><a href="./Iterative-tinkering-enhances-effective-IQ" data-for="Iterative-tinkering-enhances-effective-IQ">Iterative tinkering enhances effective IQ</a></li><li><a href="./Just-do-more" data-for="Just-do-more">Just do more</a></li><li><a href="./Labor-relationships-have-debt-like-features" data-for="Labor-relationships-have-debt-like-features">Labor relationships have debt-like features</a></li><li><a href="./@barigozziLargedimensionalDynamicFactor2021" data-for="@barigozziLargedimensionalDynamicFactor2021">Large-dimensional Dynamic Factor Models: Estimation of Impulse–Response Functions with I ( 1 ) cointegrated factors</a></li><li><a href="./@adamekLassoInferenceHighDimensional2022" data-for="@adamekLassoInferenceHighDimensional2022">Lasso Inference for High-Dimensional Time Series</a></li><li><a href="./Learned-representations-are-more-important-than-what-you-do-with-them" data-for="Learned-representations-are-more-important-than-what-you-do-with-them">Learned representations are more important than what you do with them</a></li><li><a href="./@axlerLinearAlgebraDone" data-for="@axlerLinearAlgebraDone">Linear Algebra Done Right</a></li><li><a href="./Linearly-separable-problems-are-easy-to-solve" data-for="Linearly-separable-problems-are-easy-to-solve">Linearly separable problems are easy to solve</a></li><li><a href="./Local-projections" data-for="Local-projections">Local projections</a></li><li><a href="./Local-projections-vs.-VARs" data-for="Local-projections-vs.-VARs">Local projections vs. VARs</a></li><li><a href="./@liLocalProjectionsVs2023" data-for="@liLocalProjectionsVs2023">Local Projections vs. VARs: Lessons From Thousands of DGPs</a></li><li><a href="./Look-for-high-talent-and-high-agency" data-for="Look-for-high-talent-and-high-agency">Look for high talent and high agency</a></li><li><a href="./@nowakMathematicalFoundationsMachine" data-for="@nowakMathematicalFoundationsMachine">Mathematical Foundations of Machine Learning</a></li><li><a href="./Maximize-the-entropy-of-your-information-sources" data-for="Maximize-the-entropy-of-your-information-sources">Maximize the entropy of your information sources</a></li><li><a href="./Maximize-your-output-of-testable-ideas" data-for="Maximize-your-output-of-testable-ideas">Maximize your output of testable ideas</a></li><li><a href="./@karapanagiotiModelSelectionLocal" data-for="@karapanagiotiModelSelectionLocal">Model Selection for Local Projections Instrumental Variable Methods - Empirical Application to Government Spending Multipliers.</a></li><li><a href="./@jordaModelFreeImpulseResponses2003" data-for="@jordaModelFreeImpulseResponses2003">Model-Free Impulse Responses</a></li><li><a href="./Moving-average-model" data-for="Moving-average-model">Moving average model</a></li><li><a href="./Network-egress-is-a-lock-in-tactic-for-public-cloud-providers" data-for="Network-egress-is-a-lock-in-tactic-for-public-cloud-providers">Network egress is a lock-in tactic for public cloud providers</a></li><li><a href="./Not-all-spurious-correlation-is-random" data-for="Not-all-spurious-correlation-is-random">Not all spurious correlation is random</a></li><li><a href="./Observe-before-you-analyze" data-for="Observe-before-you-analyze">Observe before you analyze</a></li><li><a href="./@greenwaldOriginsStockMarket2014" data-for="@greenwaldOriginsStockMarket2014">Origins of Stock Market Fluctuations</a></li><li><a href="./Perfect-competition-is-not-optimal-under-increasing-returns-to-scale" data-for="Perfect-competition-is-not-optimal-under-increasing-returns-to-scale">Perfect competition is not optimal under increasing returns to scale</a></li><li><a href="./Planning-your-time-is-like-choosing-to-dance-on-beat" data-for="Planning-your-time-is-like-choosing-to-dance-on-beat">Planning your time is like choosing to dance on beat</a></li><li><a href="./Pre-train-your-representations-before-learning-judgement" data-for="Pre-train-your-representations-before-learning-judgement">Pre-train your representations before learning judgement</a></li><li><a href="./Prediction-is-compression,-compression-is-expression" data-for="Prediction-is-compression,-compression-is-expression">Prediction is compression, compression is expression</a></li><li><a href="./Principal-component-analysis" data-for="Principal-component-analysis">Principal component analysis</a></li><li><a href="./@hamiltonPrincipalComponentAnalysis" data-for="@hamiltonPrincipalComponentAnalysis">Principal Component Analysis for Nonstationary Series</a></li><li><a href="./@baiPrincipalComponentsEstimation2013" data-for="@baiPrincipalComponentsEstimation2013">Principal components estimation and identification of static factors</a></li><li><a href="./Random-walk" data-for="Random-walk">Random walk</a></li><li><a href="./residuals" data-for="residuals">residuals</a></li><li><a href="./self-supervised-learning" data-for="self-supervised-learning">self-supervised learning</a></li><li><a href="./@budaShortVariableLags2023" data-for="@budaShortVariableLags2023">Short and Variable Lags</a></li><li><a href="./@crumpSparseTrendEstimation2023" data-for="@crumpSparseTrendEstimation2023">Sparse Trend Estimation</a></li><li><a href="./@hsuSubsetSelectionVector2008" data-for="@hsuSubsetSelectionVector2008">Subset selection for vector autoregressive processes using Lasso</a></li><li><a href="./@taylorSystematicShiftsScaling2021" data-for="@taylorSystematicShiftsScaling2021">Systematic shifts in scaling behavior based on organizational strategy in universities</a></li><li><a href="./@hoyosTariffsGrowthHeterogeneous" data-for="@hoyosTariffsGrowthHeterogeneous">Tariffs and Growth: Heterogeneous Eﬀects by Economic Structure</a></li><li><a href="./@zouAdaptiveLassoIts2006" data-for="@zouAdaptiveLassoIts2006">The Adaptive Lasso and Its Oracle Properties</a></li><li><a href="./@parkBayesianLasso2008" data-for="@parkBayesianLasso2008">The Bayesian Lasso</a></li><li><a href="./@fernaldDisappointingRecoveryOutput2017" data-for="@fernaldDisappointingRecoveryOutput2017">The Disappointing Recovery of Output after 2009</a></li><li><a href="./@bayerLiquidityChannelFiscal2020" data-for="@bayerLiquidityChannelFiscal2020">The Liquidity Channel of Fiscal Policy</a></li><li><a href="./Under-fat-tails,-look-for-quick-reasons-to-say-yes" data-for="Under-fat-tails,-look-for-quick-reasons-to-say-yes">Under fat tails, look for quick reasons to say yes</a></li><li><a href="./Under-fat-tails,-outliers-drive-movements-in-the-mean" data-for="Under-fat-tails,-outliers-drive-movements-in-the-mean">Under fat tails, outliers drive movements in the mean</a></li><li><a href="./Under-fat-tails,-unbiasedness-is-overrated" data-for="Under-fat-tails,-unbiasedness-is-overrated">Under fat tails, unbiasedness is overrated</a></li><li><a href="./Use-TK-liberally" data-for="Use-TK-liberally">Use TK liberally</a></li><li><a href="./variance" data-for="variance">variance</a></li><li><a href="./Vector-autoregression" data-for="Vector-autoregression">Vector autoregression</a></li><li><a href="./@bardesVICRegVarianceInvarianceCovarianceRegularization2021" data-for="@bardesVICRegVarianceInvarianceCovarianceRegularization2021">VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning</a></li><li><a href="./Volatility-is-information" data-for="Volatility-is-information">Volatility is information</a></li><li><a href="./When-in-doubt,-don't-difference" data-for="When-in-doubt,-don't-difference">When in doubt, don't difference</a></li><li><a href="./When-unskilled,-complicate-the-game-and-add-randomness" data-for="When-unskilled,-complicate-the-game-and-add-randomness">When unskilled, complicate the game and add randomness</a></li><li><a href="./With-the-right-representation,-judgement-is-cheap" data-for="With-the-right-representation,-judgement-is-cheap">With the right representation, judgement is cheap</a></li></ul></div></li><li id="explorer-end"></li></ul></div></div></div><div class="center"><div class="page-header"><div class="popover-hint"></div></div><article class="popover-hint"><h1 id="sparse-trend-estimation">Sparse Trend Estimation<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#sparse-trend-estimation" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<p><strong>Richard K. Crump, Nikolay Gospodinov, Hunter Wieman – 2023</strong></p>
<blockquote class="callout abstract" data-callout="abstract">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Abstract </p></div>
                  
                </div>
<div class="callout-content">
<p>The low-frequency movements of many economic variables play a prominent role in policy analysis and decision-making. We develop a robust estimation approach for these slow-moving trend processes which is guided by a judicious choice of priors and is characterized by sparsity. We present some novel stylized facts from longer-run survey expectations that inform the structure of the estimation procedure. The general version of the proposed Bayesian estimator with a slab-and-spike prior accounts explicitly for cyclical dynamics. The practical implementation of the method is discussed in detail and we show that it performs well in simulations against some relevant benchmarks. We report empirical estimates of trend growth for U.S. output (and its components), productivity and annual mean temperature. These estimates allow policy makers to assess shortfalls and overshoots in these variables from their economic and ecological targets.</p>
</div>
</blockquote>
<p>The authors examine the evolution of long-horizon forecasts of the US economy and note that these forecast change quite slowly over time. Thus the first differences of these forecasts are usually zero, with rarer deviations in either direction when the forecast does change. The same is true of the second differences of these forecasts. These distributions don’t appear to be consistent with continuous distributions like the normal distribution. They are more consistent with mixture distributions.</p>
<p>This feels like a relatively “doable” paper to replicate</p>
<p>The March 2024 revision of the paper is definitely cleaner and easier to understand, though I’m biased from having read the first version of the paper.</p>
<p>One thing to notice is that though they omit any multivariate designation for the normals that define the distributions of the observed data and trend, you can tell they are multivariate by the fact that they are parameterized with vectors for the means and matrices for the variance.</p>
<h2 id="replication">Replication<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#replication" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>Some interesting reflections after attempting to replicate the paper multiple times, with the most success this was recent attempt:</p>
<ul>
<li>When people say that a Laplace distribution is a scale mixture of normals with exponential mixing density, what they literally mean is that the variances are exponentially distributed, meaning they get less frequent the larger they are. How the infrequency scales with the variances depends on the <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">λ</span></span></span></span> parameter, i.e. the variance of the original Laplace. The higher the variance of the original Laplace, the lower the <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">λ</span></span></span></span>, the higher variance the exponential distribution, the less high variance normals are effectively “penalized”.</li>
<li>You are effectively sampling various normals from a single exponential distribution. It’s not that there are multiple different exponentials – there is only one, characterized by <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">λ</span></span></span></span>, from which you sample all the variances, i.e. all the normals. It just so happens that when you sample a bunch of numbers from an exponential, call those things “variances” and use them to parameterize a bunch of normals centered around the same mean, you get a Laplace. That just happens by definition, that’s just the math. The rarity of higher variances comes for free via the exponential distribution. When combined with those relative frequencies, you get a Laplace.</li>
<li>The normals are integrated over to get the final Laplace distribution. The exact way this is done is fairly complicated in its full form, but it’s basically integrating over the normals conditional on their variances, then integrating over the variances themselves. So if the normals are <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mord">∣</span><span class="mord mathnormal">s</span><span class="mclose">)</span></span></span></span>  and the exponentials are <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mord">∣</span><span class="mord mathnormal">a</span><span class="mclose">)</span></span></span></span> where <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">s</span></span></span></span> is the variance of the normals and <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">a</span></span></span></span> is the scale parameter of the exponentials, then the integral for the density is <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2151em;vertical-align:-0.3558em;"></span><span class="mop"><span class="mop op-symbol small-op" style="margin-right:0.19445em;position:relative;top:-0.0006em;">∫</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8593em;"><span style="top:-2.3442em;margin-left:-0.1945em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span><span style="top:-3.2579em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">∞</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3558em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mord">∣</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mord">∣</span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mord mathnormal">d</span><span class="mord mathnormal">s</span></span></span></span> which evaluates out to the Laplace density <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.233em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6954em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">a</span><span class="mord mtight">∣</span><span class="mord mathnormal mtight">x</span><span class="mord mtight">∣</span></span></span></span></span></span></span></span></span></span></span></span></li>
<li>You need to use a mixture to represent the spike and slab density, since the whole point is that a single parameterized Laplace distribution can’t accommodate the stylized facts we see in the data – lots of zeros and a small number of larger magnitude changes. This was originally confusing because I thought there was only one mixture, the one generating the Laplace if you use the hierarchical setup. But no, you need to setup a mixture, because although there are “2 Laplaces” you are only pulling a single sample for each time period. It’s not a separate sample for each Laplace for each time period. The only way to set that up is via a mixture distribution, which gives you one effective distribution. Now, because you use a Bernoulli to pick between them, any given time period is only effectively using one Laplace or the other.</li>
<li>I’m still not sure if its critical to use the mixture of normals approach. My sense is that they use that so that they can use a Gibbs sampler, but I don’t know enough about Gibbs sampling to know if I’m right on that. For my purposes it’s much simpler to just use Laplace distributions directly, though now with my new understanding maybe it wouldn’t be so bad to rewrite using normals and exponentials… I need to make sure that the “scale mixture of normals” can be represented in PyMC using the <code>NormalMixture</code> function or if there’s some subtle distinction there. I think the weights in that function are different than what comes out of an exponential distribution. I think the weights for the <code>NormalMixture</code> should actually be the relative frequency of the variances you are sampling from the exponential, not the variances themselves which is what I was previously doing.</li>
<li>Something seems to just completely blow up when you expand the sample size. The time to take to run the routine grows dramatically if you add just 5 more years of data, something like 50x. Strange.</li>
</ul>
<hr/>
<h2 id="references">References<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#references" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p><a href="./@phillipsBoostingWhyYou2021" class="internal" data-slug="@phillipsBoostingWhyYou2021">@phillipsBoostingWhyYou2021</a></p>
<p><a href="https://statmodeling.stat.columbia.edu/2017/11/02/king-must-die/" class="external">The king must die<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
<ul>
<li>Really good discussion of why <a href="./@parkBayesianLasso2008" class="internal alias" data-slug="@parkBayesianLasso2008">The Bayesian Lasso</a> is not very good in practice</li>
<li>Similar to why this paper even exists, the Bayesian Lasso can’t properly balance between big and small values, so it ends up trying to compromise which shrinks the larger values too much and doesn’t shrink the small values enough</li>
<li>The core reason for this is that a single Laplace prior distribution doesn’t place enough probability mass near the spike at zero relative to the mass in the tails to ensure that you get a reasonable level of sparsity</li>
</ul>
<p><a href="./tags/literature/paper" class="tag-link internal alias" data-slug="tags/literature/paper">paper</a><a href="./tags/inbox/read" class="tag-link internal alias" data-slug="tags/inbox/read">read</a><a href="./tags/online" class="tag-link internal alias" data-slug="tags/online">online</a></p></article></div><div class="right sidebar"><div class="backlinks"><h3>Backlinks</h3><ul class="overflow"><li>No backlinks found</li></ul></div><div class="explorer mobile-only"><button type="button" id="explorer" data-behavior="collapse" data-collapsed="collapsed" data-savestate="true" data-tree="[]"><h1>All Notes</h1><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="5 8 14 8" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><div id="explorer-content"><ul class="overflow" id="explorer-ul"><li><div class="folder-outer open"><ul style="padding-left:0;" class="content" data-folderul><li><div class="folder-outer "><ul style="padding-left:0;" class="content" data-folderul></ul></div></li><li><a href="./@dubeLocalProjectionsApproach" data-for="@dubeLocalProjectionsApproach">A Local Projections Approach to Difference-in-Differences Event Studies</a></li><li><a href="./@gouletcoulombeNeuralPhillipsCurve2022" data-for="@gouletcoulombeNeuralPhillipsCurve2022">A Neural Phillips Curve and a Deep Output Gap</a></li><li><a href="./agency" data-for="agency">agency</a></li><li><a href="./@hodrickExplorationTrendCycleDecomposition2020" data-for="@hodrickExplorationTrendCycleDecomposition2020">An Exploration of Trend-Cycle Decomposition Methodologies in Simulated Data</a></li><li><a href="./Autoregressive-models" data-for="Autoregressive-models">Autoregressive models</a></li><li><a href="./Ben-Franklin's-Autoencoder" data-for="Ben-Franklin's-Autoencoder">Ben Franklin's Autoencoder</a></li><li><a href="./Beveridge-Nelson-Decomposition" data-for="Beveridge-Nelson-Decomposition">Beveridge-Nelson Decomposition</a></li><li><a href="./@phillipsBoostingWhyYou2021" data-for="@phillipsBoostingWhyYou2021">Boosting: Why You Can Use the Hp Filter</a></li><li><a href="./Career-success-and-career-capital-are-cointegrated" data-for="Career-success-and-career-capital-are-cointegrated">Career success and career capital are cointegrated</a></li><li><a href="./Careers-are-a-random-walk" data-for="Careers-are-a-random-walk">Careers are a random walk</a></li><li><a href="./Change-leads-to-insight-far-more-often-than-insight-leads-to-change" data-for="Change-leads-to-insight-far-more-often-than-insight-leads-to-change">Change leads to insight far more often than insight leads to change</a></li><li><a href="./Correlation-between-prices-and-quantities-reveals-main-market-driver" data-for="Correlation-between-prices-and-quantities-reveals-main-market-driver">Correlation between prices and quantities reveals main market driver</a></li><li><a href="./Diffusion-models" data-for="Diffusion-models">Diffusion models</a></li><li><a href="./Don't-detrend-random-walks-with-linear-trends" data-for="Don't-detrend-random-walks-with-linear-trends">Don't detrend random walks with linear trends</a></li><li><a href="./Don't-rely-on-inspiration-to-write" data-for="Don't-rely-on-inspiration-to-write">Don't rely on inspiration to write</a></li><li><a href="./@vivianoDynamicCovariateBalancing2023" data-for="@vivianoDynamicCovariateBalancing2023">Dynamic covariate balancing: Estimating treatment effects over time with potential local projections</a></li><li><a href="./Dynamic-factor-model" data-for="Dynamic-factor-model">Dynamic factor model</a></li><li><a href="./@stockDynamicFactorModels2016" data-for="@stockDynamicFactorModels2016">Dynamic Factor Models, Factor-Augmented Vector Autoregressions, and Structural Vector Autoregressions in Macroeconomics</a></li><li><a href="./@plagborg-mollerEssaysMacroeconometrics" data-for="@plagborg-mollerEssaysMacroeconometrics">Essays in Macroeconometrics</a></li><li><a href="./Exponentials-drive-asymmetry" data-for="Exponentials-drive-asymmetry">Exponentials drive asymmetry</a></li><li><a href="./@canovaFAQHowExtract" data-for="@canovaFAQHowExtract">FAQ: How do I extract the output gap?</a></li><li><a href="./Fat-tails-preclude-ergodicity" data-for="Fat-tails-preclude-ergodicity">Fat tails preclude ergodicity</a></li><li><a href="./First-impression-are-high-variance" data-for="First-impression-are-high-variance">First impression are high variance</a></li><li><a href="./Focus-on-the-residuals" data-for="Focus-on-the-residuals">Focus on the residuals</a></li><li><a href="./For-the-effectual-reasoner,-residuals-are-inputs-rather-than-outputs" data-for="For-the-effectual-reasoner,-residuals-are-inputs-rather-than-outputs">For the effectual reasoner, residuals are inputs rather than outputs</a></li><li><a href="./@gorodnichenkoForecastErrorVariance2020" data-for="@gorodnichenkoForecastErrorVariance2020">Forecast Error Variance Decompositions with Local Projections</a></li><li><a href="./@bardsenForecastingLevelsLog2011" data-for="@bardsenForecastingLevelsLog2011">Forecasting levels of log variables in vector autoregressions</a></li><li><a href="./@atheyGeneralizedRandomForests2018" data-for="@atheyGeneralizedRandomForests2018">Generalized Random Forests</a></li><li><a href="./Get-leverage-on-the-fixed-cost-of-pain" data-for="Get-leverage-on-the-fixed-cost-of-pain">Get leverage on the fixed cost of pain</a></li><li><a href="./Goals-should-be-binomial" data-for="Goals-should-be-binomial">Goals should be binomial</a></li><li><a href="./Good-ideas-are-testable-ideas" data-for="Good-ideas-are-testable-ideas">Good ideas are testable ideas</a></li><li><a href="./Good-representations-reduce-sample-complexity-of-downstream-tasks" data-for="Good-representations-reduce-sample-complexity-of-downstream-tasks">Good representations reduce sample complexity of downstream tasks</a></li><li><a href="./Granular-IV" data-for="Granular-IV">Granular IV</a></li><li><a href="./Granularity" data-for="Granularity">Granularity</a></li><li><a href="./@jonesGrowthIdeas2005" data-for="@jonesGrowthIdeas2005">Growth and Ideas</a></li><li><a href="./Hamilton-filter" data-for="Hamilton-filter">Hamilton filter</a></li><li><a href="./Highly-likely-events-do-not-yield-much-information" data-for="Highly-likely-events-do-not-yield-much-information">Highly likely events do not yield much information</a></li><li><a href="./@cochraneHowBigRandom1988" data-for="@cochraneHowBigRandom1988">How Big Is the Random Walk in GNP?</a></li><li><a href="./@haddadHowCompetitiveStock2021" data-for="@haddadHowCompetitiveStock2021">How Competitive is the Stock Market? Theory, Evidence from Portfolios, and Implications for the Rise of Passive Investing</a></li><li><a href="./@greenwaldHowWealthWas" data-for="@greenwaldHowWealthWas">How the Wealth Was Won: Factor Shares as Market Fundamentals</a></li><li><a href="./Ideas-generate-increasing-returns-because-they-are-nonrivalrous" data-for="Ideas-generate-increasing-returns-because-they-are-nonrivalrous">Ideas generate increasing returns because they are nonrivalrous</a></li><li><a href="./@stockIdentificationEstimationDynamic2018" data-for="@stockIdentificationEstimationDynamic2018">Identification and Estimation of Dynamic Causal Effects in Macroeconomics Using External Instruments</a></li><li><a href="./If-you're-not-sure-something-is-getting-better,-it's-not" data-for="If-you're-not-sure-something-is-getting-better,-it's-not">If you're not sure something is getting better, it's not</a></li><li><a href="./@barnichonImpulseResponseEstimation2019" data-for="@barnichonImpulseResponseEstimation2019">Impulse Response Estimation by Smooth Local Projections</a></li><li><a href="./In-the-long-run,-all-costs-are-variable" data-for="In-the-long-run,-all-costs-are-variable">In the long-run, all costs are variable</a></li><li><a href="./@barigozziInferenceHeavyTailedNonstationary2022" data-for="@barigozziInferenceHeavyTailedNonstationary2022">Inference in Heavy-Tailed Nonstationary Multivariate Time Series</a></li><li><a href="./Information-is-relative" data-for="Information-is-relative">Information is relative</a></li><li><a href="./Information-is-surprisal" data-for="Information-is-surprisal">Information is surprisal</a></li><li><a href="./Interaction-generates-non-normality" data-for="Interaction-generates-non-normality">Interaction generates non-normality</a></li><li><a href="./Invest-in-companies-like-you-invest-in-your-career" data-for="Invest-in-companies-like-you-invest-in-your-career">Invest in companies like you invest in your career</a></li><li><a href="./Iterative-tinkering-enhances-effective-IQ" data-for="Iterative-tinkering-enhances-effective-IQ">Iterative tinkering enhances effective IQ</a></li><li><a href="./Just-do-more" data-for="Just-do-more">Just do more</a></li><li><a href="./Labor-relationships-have-debt-like-features" data-for="Labor-relationships-have-debt-like-features">Labor relationships have debt-like features</a></li><li><a href="./@barigozziLargedimensionalDynamicFactor2021" data-for="@barigozziLargedimensionalDynamicFactor2021">Large-dimensional Dynamic Factor Models: Estimation of Impulse–Response Functions with I ( 1 ) cointegrated factors</a></li><li><a href="./@adamekLassoInferenceHighDimensional2022" data-for="@adamekLassoInferenceHighDimensional2022">Lasso Inference for High-Dimensional Time Series</a></li><li><a href="./Learned-representations-are-more-important-than-what-you-do-with-them" data-for="Learned-representations-are-more-important-than-what-you-do-with-them">Learned representations are more important than what you do with them</a></li><li><a href="./@axlerLinearAlgebraDone" data-for="@axlerLinearAlgebraDone">Linear Algebra Done Right</a></li><li><a href="./Linearly-separable-problems-are-easy-to-solve" data-for="Linearly-separable-problems-are-easy-to-solve">Linearly separable problems are easy to solve</a></li><li><a href="./Local-projections" data-for="Local-projections">Local projections</a></li><li><a href="./Local-projections-vs.-VARs" data-for="Local-projections-vs.-VARs">Local projections vs. VARs</a></li><li><a href="./@liLocalProjectionsVs2023" data-for="@liLocalProjectionsVs2023">Local Projections vs. VARs: Lessons From Thousands of DGPs</a></li><li><a href="./Look-for-high-talent-and-high-agency" data-for="Look-for-high-talent-and-high-agency">Look for high talent and high agency</a></li><li><a href="./@nowakMathematicalFoundationsMachine" data-for="@nowakMathematicalFoundationsMachine">Mathematical Foundations of Machine Learning</a></li><li><a href="./Maximize-the-entropy-of-your-information-sources" data-for="Maximize-the-entropy-of-your-information-sources">Maximize the entropy of your information sources</a></li><li><a href="./Maximize-your-output-of-testable-ideas" data-for="Maximize-your-output-of-testable-ideas">Maximize your output of testable ideas</a></li><li><a href="./@karapanagiotiModelSelectionLocal" data-for="@karapanagiotiModelSelectionLocal">Model Selection for Local Projections Instrumental Variable Methods - Empirical Application to Government Spending Multipliers.</a></li><li><a href="./@jordaModelFreeImpulseResponses2003" data-for="@jordaModelFreeImpulseResponses2003">Model-Free Impulse Responses</a></li><li><a href="./Moving-average-model" data-for="Moving-average-model">Moving average model</a></li><li><a href="./Network-egress-is-a-lock-in-tactic-for-public-cloud-providers" data-for="Network-egress-is-a-lock-in-tactic-for-public-cloud-providers">Network egress is a lock-in tactic for public cloud providers</a></li><li><a href="./Not-all-spurious-correlation-is-random" data-for="Not-all-spurious-correlation-is-random">Not all spurious correlation is random</a></li><li><a href="./Observe-before-you-analyze" data-for="Observe-before-you-analyze">Observe before you analyze</a></li><li><a href="./@greenwaldOriginsStockMarket2014" data-for="@greenwaldOriginsStockMarket2014">Origins of Stock Market Fluctuations</a></li><li><a href="./Perfect-competition-is-not-optimal-under-increasing-returns-to-scale" data-for="Perfect-competition-is-not-optimal-under-increasing-returns-to-scale">Perfect competition is not optimal under increasing returns to scale</a></li><li><a href="./Planning-your-time-is-like-choosing-to-dance-on-beat" data-for="Planning-your-time-is-like-choosing-to-dance-on-beat">Planning your time is like choosing to dance on beat</a></li><li><a href="./Pre-train-your-representations-before-learning-judgement" data-for="Pre-train-your-representations-before-learning-judgement">Pre-train your representations before learning judgement</a></li><li><a href="./Prediction-is-compression,-compression-is-expression" data-for="Prediction-is-compression,-compression-is-expression">Prediction is compression, compression is expression</a></li><li><a href="./Principal-component-analysis" data-for="Principal-component-analysis">Principal component analysis</a></li><li><a href="./@hamiltonPrincipalComponentAnalysis" data-for="@hamiltonPrincipalComponentAnalysis">Principal Component Analysis for Nonstationary Series</a></li><li><a href="./@baiPrincipalComponentsEstimation2013" data-for="@baiPrincipalComponentsEstimation2013">Principal components estimation and identification of static factors</a></li><li><a href="./Random-walk" data-for="Random-walk">Random walk</a></li><li><a href="./residuals" data-for="residuals">residuals</a></li><li><a href="./self-supervised-learning" data-for="self-supervised-learning">self-supervised learning</a></li><li><a href="./@budaShortVariableLags2023" data-for="@budaShortVariableLags2023">Short and Variable Lags</a></li><li><a href="./@crumpSparseTrendEstimation2023" data-for="@crumpSparseTrendEstimation2023">Sparse Trend Estimation</a></li><li><a href="./@hsuSubsetSelectionVector2008" data-for="@hsuSubsetSelectionVector2008">Subset selection for vector autoregressive processes using Lasso</a></li><li><a href="./@taylorSystematicShiftsScaling2021" data-for="@taylorSystematicShiftsScaling2021">Systematic shifts in scaling behavior based on organizational strategy in universities</a></li><li><a href="./@hoyosTariffsGrowthHeterogeneous" data-for="@hoyosTariffsGrowthHeterogeneous">Tariffs and Growth: Heterogeneous Eﬀects by Economic Structure</a></li><li><a href="./@zouAdaptiveLassoIts2006" data-for="@zouAdaptiveLassoIts2006">The Adaptive Lasso and Its Oracle Properties</a></li><li><a href="./@parkBayesianLasso2008" data-for="@parkBayesianLasso2008">The Bayesian Lasso</a></li><li><a href="./@fernaldDisappointingRecoveryOutput2017" data-for="@fernaldDisappointingRecoveryOutput2017">The Disappointing Recovery of Output after 2009</a></li><li><a href="./@bayerLiquidityChannelFiscal2020" data-for="@bayerLiquidityChannelFiscal2020">The Liquidity Channel of Fiscal Policy</a></li><li><a href="./Under-fat-tails,-look-for-quick-reasons-to-say-yes" data-for="Under-fat-tails,-look-for-quick-reasons-to-say-yes">Under fat tails, look for quick reasons to say yes</a></li><li><a href="./Under-fat-tails,-outliers-drive-movements-in-the-mean" data-for="Under-fat-tails,-outliers-drive-movements-in-the-mean">Under fat tails, outliers drive movements in the mean</a></li><li><a href="./Under-fat-tails,-unbiasedness-is-overrated" data-for="Under-fat-tails,-unbiasedness-is-overrated">Under fat tails, unbiasedness is overrated</a></li><li><a href="./Use-TK-liberally" data-for="Use-TK-liberally">Use TK liberally</a></li><li><a href="./variance" data-for="variance">variance</a></li><li><a href="./Vector-autoregression" data-for="Vector-autoregression">Vector autoregression</a></li><li><a href="./@bardesVICRegVarianceInvarianceCovarianceRegularization2021" data-for="@bardesVICRegVarianceInvarianceCovarianceRegularization2021">VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning</a></li><li><a href="./Volatility-is-information" data-for="Volatility-is-information">Volatility is information</a></li><li><a href="./When-in-doubt,-don't-difference" data-for="When-in-doubt,-don't-difference">When in doubt, don't difference</a></li><li><a href="./When-unskilled,-complicate-the-game-and-add-randomness" data-for="When-unskilled,-complicate-the-game-and-add-randomness">When unskilled, complicate the game and add randomness</a></li><li><a href="./With-the-right-representation,-judgement-is-cheap" data-for="With-the-right-representation,-judgement-is-cheap">With the right representation, judgement is cheap</a></li></ul></div></li><li id="explorer-end"></li></ul></div></div></div></div><footer class><hr/><ul><li><a href="https://whoisnnamdi.com">whoisnnamdi.com</a></li><li><a href="https://x.com/whoisnnamdi">X</a></li><li><a href="https://www.linkedin.com/in/nnamdiiregbulem">LinkedIn</a></li><li><a href="https://github.com/whoisnnamdi">GitHub</a></li></ul></footer></div></body><script type="application/javascript">function c(){let t=this.parentElement;t.classList.toggle("is-collapsed");let l=t.classList.contains("is-collapsed")?this.scrollHeight:t.scrollHeight;t.style.maxHeight=l+"px";let o=t,e=t.parentElement;for(;e;){if(!e.classList.contains("callout"))return;let n=e.classList.contains("is-collapsed")?e.scrollHeight:e.scrollHeight+o.scrollHeight;e.style.maxHeight=n+"px",o=e,e=e.parentElement}}function i(){let t=document.getElementsByClassName("callout is-collapsible");for(let s of t){let l=s.firstElementChild;if(l){l.addEventListener("click",c),window.addCleanup(()=>l.removeEventListener("click",c));let e=s.classList.contains("is-collapsed")?l.scrollHeight:s.scrollHeight;s.style.maxHeight=e+"px"}}}document.addEventListener("nav",i);window.addEventListener("resize",i);
</script><script type="module">
          let mermaidImport = undefined
          document.addEventListener('nav', async () => {
            if (document.querySelector("code.mermaid")) {
              mermaidImport ||= await import('https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs')
              const mermaid = mermaidImport.default
              const darkMode = document.documentElement.getAttribute('saved-theme') === 'dark'
              mermaid.initialize({
                startOnLoad: false,
                securityLevel: 'loose',
                theme: darkMode ? 'dark' : 'default'
              })

              await mermaid.run({
                querySelector: '.mermaid'
              })
            }
          });
          </script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/copy-tex.min.js" type="application/javascript"></script><script type="application/javascript">
            const socket = new WebSocket('ws://localhost:3001')
            // reload(true) ensures resources like images and scripts are fetched again in firefox
            socket.addEventListener('message', () => document.location.reload(true))
          </script><script src="./postscript.js" type="module"></script></html>